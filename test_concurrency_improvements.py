#!/usr/bin/env python3
"""
æµ‹è¯•å¹¶å‘å¤„ç†èƒ½åŠ›æå‡æ•ˆæœ
"""

import asyncio
import random
import time

from src.utils.concurrency_manager import get_concurrency_manager
from src.utils.load_balancer import get_load_balancer
from src.utils.performance_monitor import TaskMonitoringContext, get_performance_monitor


def simulate_llm_call(task_id: str) -> str:
    """æ¨¡æ‹ŸLLMè°ƒç”¨"""
    # æ¨¡æ‹Ÿä¸åŒçš„å¤„ç†æ—¶é—´
    processing_time = random.uniform(0.5, 3.0)
    time.sleep(processing_time)

    # æ¨¡æ‹Ÿå¶å°”çš„å¤±è´¥
    if random.random() < 0.1:  # 10% å¤±è´¥ç‡
        raise Exception(f"æ¨¡æ‹ŸLLMè°ƒç”¨å¤±è´¥: {task_id}")

    return f"LLMå“åº” for {task_id}"


def simulate_file_processing(file_path: str) -> str:
    """æ¨¡æ‹Ÿæ–‡ä»¶å¤„ç†"""
    processing_time = random.uniform(0.1, 1.0)
    time.sleep(processing_time)

    if random.random() < 0.05:  # 5% å¤±è´¥ç‡
        raise Exception(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {file_path}")

    return f"å¤„ç†å®Œæˆ: {file_path}"


async def test_concurrency_manager():
    """æµ‹è¯•å¹¶å‘ç®¡ç†å™¨"""
    print("\nğŸ”§ æµ‹è¯•å¹¶å‘ç®¡ç†å™¨...")

    # è·å–å¹¶å‘ç®¡ç†å™¨
    config = {
        "max_concurrent_llm_calls": 8,
        "batch_size": 15,
        "adaptive_batch_size": True,
        "performance_monitoring": True,
        "circuit_breaker_threshold": 5,
    }

    manager = get_concurrency_manager(config)

    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
    tasks = [f"task_{i}" for i in range(50)]

    print(f"ğŸ“Š å¼€å§‹å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡...")
    start_time = time.time()

    # ä½¿ç”¨å¹¶å‘ç®¡ç†å™¨å¤„ç†ä»»åŠ¡
    results = await manager.process_batch_async(items=tasks, process_func=simulate_llm_call, max_concurrency=8)

    end_time = time.time()
    execution_time = end_time - start_time

    # ç»Ÿè®¡ç»“æœ
    successful_results = [r for r in results if r is not None]
    success_rate = len(successful_results) / len(tasks)
    throughput = len(successful_results) / execution_time

    print("âœ… å¹¶å‘ç®¡ç†å™¨æµ‹è¯•å®Œæˆ:")
    print(f"   - æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
    print(f"   - æˆåŠŸç‡: {success_rate:.2%}")
    print(f"   - ååé‡: {throughput:.2f} ä»»åŠ¡/ç§’")

    # è·å–æ€§èƒ½æŒ‡æ ‡
    metrics = manager.get_metrics()
    print(f"   - å¹³å‡å“åº”æ—¶é—´: {metrics.avg_execution_time:.2f}ç§’")
    print(f"   - æ€»ååé‡: {metrics.throughput:.2f} ä»»åŠ¡/ç§’")


async def test_load_balancer():
    """æµ‹è¯•è´Ÿè½½å‡è¡¡å™¨"""
    print("\nâš–ï¸ æµ‹è¯•è´Ÿè½½å‡è¡¡å™¨...")

    # è·å–è´Ÿè½½å‡è¡¡å™¨
    balancer = get_load_balancer(max_workers=6, strategy="least_loaded")

    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
    files = [f"file_{i}.py" for i in range(30)]

    print(f"ğŸ“Š å¼€å§‹å¤„ç† {len(files)} ä¸ªæ–‡ä»¶...")
    start_time = time.time()

    # ä½¿ç”¨è´Ÿè½½å‡è¡¡å™¨å¤„ç†ä»»åŠ¡
    results = await balancer.execute_batch(task_func=simulate_file_processing, task_args=files, timeout=5.0)

    end_time = time.time()
    execution_time = end_time - start_time

    # ç»Ÿè®¡ç»“æœ
    successful_results = [r for r in results if r is not None]
    success_rate = len(successful_results) / len(files)
    throughput = len(successful_results) / execution_time

    print("âœ… è´Ÿè½½å‡è¡¡å™¨æµ‹è¯•å®Œæˆ:")
    print(f"   - æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
    print(f"   - æˆåŠŸç‡: {success_rate:.2%}")
    print(f"   - ååé‡: {throughput:.2f} ä»»åŠ¡/ç§’")

    # æ˜¾ç¤ºè´Ÿè½½å‡è¡¡ç»Ÿè®¡
    balancer.log_stats()


async def test_performance_monitoring():
    """æµ‹è¯•æ€§èƒ½ç›‘æ§"""
    print("\nğŸ“Š æµ‹è¯•æ€§èƒ½ç›‘æ§...")

    monitor = get_performance_monitor()

    # å¼€å§‹ç›‘æ§
    await monitor.start_monitoring()

    # æ¨¡æ‹Ÿä¸€äº›ä»»åŠ¡
    async def simulate_tasks():
        for i in range(20):
            task_id = f"monitor_task_{i}"
            with TaskMonitoringContext(task_id):
                # æ¨¡æ‹Ÿä»»åŠ¡å¤„ç†
                await asyncio.sleep(random.uniform(0.1, 0.5))
                if random.random() < 0.1:  # 10% å¤±è´¥ç‡
                    raise Exception(f"ä»»åŠ¡å¤±è´¥: {task_id}")

    try:
        await simulate_tasks()
    except:
        pass  # å¿½ç•¥æ¨¡æ‹Ÿçš„é”™è¯¯

    # ç­‰å¾…ä¸€æ®µæ—¶é—´æ”¶é›†æ•°æ®
    await asyncio.sleep(2)

    # åœæ­¢ç›‘æ§
    await monitor.stop_monitoring()

    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    report = monitor.get_performance_report()
    print("âœ… æ€§èƒ½ç›‘æ§æµ‹è¯•å®Œæˆ:")
    print(f"   - æ€»ä»»åŠ¡æ•°: {report['totals']['total_tasks']}")
    print(f"   - å®Œæˆä»»åŠ¡: {report['totals']['completed_tasks']}")
    print(f"   - å¤±è´¥ä»»åŠ¡: {report['totals']['failed_tasks']}")
    print(f"   - å½“å‰ååé‡: {report['current']['throughput']:.2f} ä»»åŠ¡/ç§’")
    print(f"   - é”™è¯¯ç‡: {report['current']['error_rate']:.2%}")
    print(f"   - å¹³å‡å“åº”æ—¶é—´: {report['current']['avg_response_time']:.3f}ç§’")


def test_traditional_processing():
    """æµ‹è¯•ä¼ ç»Ÿä¸²è¡Œå¤„ç†ï¼ˆå¯¹æ¯”åŸºå‡†ï¼‰"""
    print("\nğŸŒ æµ‹è¯•ä¼ ç»Ÿä¸²è¡Œå¤„ç†ï¼ˆåŸºå‡†å¯¹æ¯”ï¼‰...")

    tasks = [f"baseline_task_{i}" for i in range(20)]

    print(f"ğŸ“Š å¼€å§‹ä¸²è¡Œå¤„ç† {len(tasks)} ä¸ªä»»åŠ¡...")
    start_time = time.time()

    results = []
    for task in tasks:
        try:
            result = simulate_llm_call(task)
            results.append(result)
        except Exception:
            results.append(None)

    end_time = time.time()
    execution_time = end_time - start_time

    successful_results = [r for r in results if r is not None]
    success_rate = len(successful_results) / len(tasks)
    throughput = len(successful_results) / execution_time

    print("âœ… ä¸²è¡Œå¤„ç†æµ‹è¯•å®Œæˆ:")
    print(f"   - æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
    print(f"   - æˆåŠŸç‡: {success_rate:.2%}")
    print(f"   - ååé‡: {throughput:.2f} ä»»åŠ¡/ç§’")

    return execution_time, throughput


async def test_optimized_processing():
    """æµ‹è¯•ä¼˜åŒ–åçš„å¹¶å‘å¤„ç†"""
    print("\nğŸš€ æµ‹è¯•ä¼˜åŒ–åçš„å¹¶å‘å¤„ç†...")

    config = {
        "max_concurrent_llm_calls": 8,
        "batch_size": 15,
        "adaptive_batch_size": True,
        "performance_monitoring": True,
    }

    manager = get_concurrency_manager(config)
    tasks = [f"optimized_task_{i}" for i in range(20)]

    print(f"ğŸ“Š å¼€å§‹å¹¶å‘å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡...")
    start_time = time.time()

    results = await manager.process_batch_async(items=tasks, process_func=simulate_llm_call, max_concurrency=8)

    end_time = time.time()
    execution_time = end_time - start_time

    successful_results = [r for r in results if r is not None]
    success_rate = len(successful_results) / len(tasks)
    throughput = len(successful_results) / execution_time

    print("âœ… å¹¶å‘å¤„ç†æµ‹è¯•å®Œæˆ:")
    print(f"   - æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
    print(f"   - æˆåŠŸç‡: {success_rate:.2%}")
    print(f"   - ååé‡: {throughput:.2f} ä»»åŠ¡/ç§’")

    return execution_time, throughput


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ å¹¶å‘å¤„ç†èƒ½åŠ›æå‡æµ‹è¯•")
    print("=" * 50)

    # æµ‹è¯•å„ä¸ªç»„ä»¶
    await test_concurrency_manager()
    await test_load_balancer()
    await test_performance_monitoring()

    print("\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 30)

    # å¯¹æ¯”æµ‹è¯•
    baseline_time, baseline_throughput = test_traditional_processing()
    optimized_time, optimized_throughput = await test_optimized_processing()

    # è®¡ç®—æ”¹è¿›
    time_improvement = (baseline_time - optimized_time) / baseline_time * 100
    throughput_improvement = (optimized_throughput - baseline_throughput) / baseline_throughput * 100

    print("\nğŸ‰ æ€§èƒ½æå‡æ€»ç»“:")
    print(f"   - æ‰§è¡Œæ—¶é—´æ”¹è¿›: {time_improvement:.1f}%")
    print(f"   - ååé‡æå‡: {throughput_improvement:.1f}%")

    if time_improvement > 0:
        print(f"   âœ… å¹¶å‘ä¼˜åŒ–æˆåŠŸï¼å¤„ç†é€Ÿåº¦æå‡äº† {time_improvement:.1f}%")
    else:
        print(f"   âš ï¸ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå½“å‰æ”¹è¿›: {time_improvement:.1f}%")


if __name__ == "__main__":
    asyncio.run(main())
