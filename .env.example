# 环境变量配置示例
# 只需要配置必要的环境变量，其他配置可以在 config/default.yml 中设置

# LLM API 密钥（必需）
LLM_API_KEY=your_api_key_here

# LLM 模型（必需）
# 格式: "provider/model"，例如: "openai/gpt-4", "anthropic/claude-3-opus-20240229"
# 对于 OpenRouter，格式必须为: "openrouter/provider/model"，例如: "openrouter/anthropic/claude-3-opus-20240229"
# 对于阿里云DashScope，直接使用模型名称，例如: "qwen-turbo", "qwen-plus", "qwen-max"
# 注意：OpenRouter模型必须包含"openrouter/"前缀，否则会被识别为直连提供商
LLM_MODEL=your_provider/your_model_name

# 阿里云DashScope配置示例（使用OpenAI兼容模式）
# LLM_API_KEY=sk-your-dashscope-api-key
# LLM_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
# LLM_MODEL=qwen-turbo

# 通用LLM Base URL配置（推荐）
# LLM_BASE_URL=your_custom_base_url  # 统一配置，支持所有提供商

# 提供商特定的Base URL配置（兼容性支持）
# OPENAI_BASE_URL=https://api.openai.com/v1
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
# ALIBABA_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1

# 节点特定的模型配置（可选）
# 如果需要为特定节点设置不同的模型，可以使用以下格式的环境变量
# 格式: "provider/model"，例如: "openai/gpt-4", "anthropic/claude-3-opus-20240229"
# 对于 OpenRouter，格式应为: "openrouter/provider/model"，例如: "openrouter/anthropic/claude-3-opus-20240229"
# LLM_MODEL_GENERATE_OVERALL_ARCHITECTURE=openai/gpt-4
# LLM_MODEL_GENERATE_API_DOCS=openai/gpt-3.5-turbo
# LLM_MODEL_GENERATE_TIMELINE=openai/gpt-3.5-turbo
# LLM_MODEL_GENERATE_DEPENDENCY=openai/gpt-3.5-turbo
# LLM_MODEL_GENERATE_GLOSSARY=openai/gpt-3.5-turbo
# LLM_MODEL_GENERATE_QUICK_LOOK=openai/gpt-3.5-turbo
# LLM_MODEL_CONTENT_QUALITY_CHECK=openai/gpt-3.5-turbo
# LLM_MODEL_GENERATE_MODULE_DETAILS=openai/gpt-4
# LLM_MODEL_MODULE_QUALITY_CHECK=anthropic/claude-3-haiku-20240307
# LLM_MODEL_AI_UNDERSTAND_CORE_MODULES=openai/gpt-4
# LLM_MODEL_COMBINE_TRANSLATE=anthropic/claude-3-sonnet-20240229
# LLM_MODEL_INTERACTIVE_QA=anthropic/claude-3-opus-20240229
# 使用 OpenRouter 的示例:
# LLM_MODEL_COMBINE_TRANSLATE=openrouter/anthropic/claude-3-sonnet-20240229
# LLM_MODEL_INTERACTIVE_QA=openrouter/anthropic/claude-3-opus-20240229

# LLM Token 限制配置（建议同时设置两个参数）
# 控制LLM输出的最大token数
# LLM_MAX_TOKENS=4000
# 控制输入到LLM的最大token数
# LLM_MAX_INPUT_TOKENS=25000
# 注意: 输入token数 + 输出token数 的总和不应超过模型的最大上下文长度
# 例如: OpenRouter的最大上下文长度为40960，则 LLM_MAX_INPUT_TOKENS + LLM_MAX_TOKENS 应小于40960

# LLM_TEMPERATURE=0.7

# LLM 缓存配置
# LLM_CACHE_ENABLED=true
# LLM_CACHE_TTL=86400 # 缓存有效期，单位：秒（86400秒 = 24小时）
# LLM_CACHE_DIR=.cache/llm # 缓存目录

# 提供商特定配置（可选，默认值在配置文件中设置）
# 如果使用 OpenRouter，可能需要设置以下环境变量
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
# APP_URL=http://localhost:3000
# APP_NAME=Codebase Knowledge Builder

# Git 仓库缓存配置
# GIT_CACHE_TTL=86400 # 仓库缓存有效期，单位：秒（86400秒 = 24小时）
# GIT_FORCE_CLONE=false # 是否强制克隆，不使用缓存

# Langfuse 配置（如果启用）
# LANGFUSE_ENABLED=true
# LANGFUSE_PUBLIC_KEY=your_public_key_here
# LANGFUSE_SECRET_KEY=your_secret_key_here
# LANGFUSE_HOST=https://cloud.langfuse.com
# LANGFUSE_PROJECT=codebase-knowledge-builder
