# 环境变量配置示例
# 只需要配置必要的环境变量，其他配置可以在 config/default.yml 中设置

# LLM API 密钥（必需）
LLM_API_KEY=your_api_key_here

# LLM 模型（可选，默认值在配置文件中设置）
# 如果需要覆盖配置文件中的值，取消注释并修改
# 格式: "provider/model"，例如: "openai/gpt-4", "openrouter/qwen/qwen3-30b-a3b:free"
# LLM_MODEL=openai/gpt-4

# 节点特定的模型配置（可选）
# 如果需要为特定节点设置不同的模型，可以使用以下格式的环境变量
# 格式: "provider/model"，例如: "openai/gpt-4", "openrouter/qwen/qwen3-30b-a3b:free"
# LLM_MODEL_GENERATE_OVERALL_ARCHITECTURE=openai/gpt-4
# LLM_MODEL_GENERATE_API_DOCS=openai/gpt-3.5-turbo
# LLM_MODEL_GENERATE_TIMELINE=openai/gpt-3.5-turbo
# LLM_MODEL_GENERATE_DEPENDENCY=openai/gpt-3.5-turbo
# LLM_MODEL_GENERATE_GLOSSARY=openai/gpt-3.5-turbo
# LLM_MODEL_GENERATE_QUICK_LOOK=openai/gpt-3.5-turbo
# LLM_MODEL_CONTENT_QUALITY_CHECK=openai/gpt-3.5-turbo
# LLM_MODEL_GENERATE_MODULE_DETAILS=openai/gpt-4
# LLM_MODEL_MODULE_QUALITY_CHECK=openrouter/qwen/qwen3-30b-a3b:free
# LLM_MODEL_AI_UNDERSTAND_CORE_MODULES=openai/gpt-4
# LLM_MODEL_COMBINE_TRANSLATE=openrouter/qwen/qwen3-30b-a3b:free
# LLM_MODEL_INTERACTIVE_QA=openrouter/qwen/qwen3-30b-a3b:free

# LLM Token 限制配置（建议同时设置两个参数）
# 控制LLM输出的最大token数
# LLM_MAX_TOKENS=4000
# 控制输入到LLM的最大token数
# LLM_MAX_INPUT_TOKENS=25000
# 注意: 输入token数 + 输出token数 的总和不应超过模型的最大上下文长度
# 例如: OpenRouter的最大上下文长度为40960，则 LLM_MAX_INPUT_TOKENS + LLM_MAX_TOKENS 应小于40960

# LLM_TEMPERATURE=0.7

# LLM 缓存配置
# LLM_CACHE_ENABLED=true
# LLM_CACHE_TTL=86400 # 缓存有效期，单位：秒（86400秒 = 24小时）
# LLM_CACHE_DIR=.cache/llm # 缓存目录

# 提供商特定配置（可选，默认值在配置文件中设置）
# 如果使用 OpenRouter，可能需要设置以下环境变量
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
# APP_URL=http://localhost:3000
# APP_NAME=Codebase Knowledge Builder

# Git 仓库缓存配置
# GIT_CACHE_TTL=86400 # 仓库缓存有效期，单位：秒（86400秒 = 24小时）
# GIT_FORCE_CLONE=false # 是否强制克隆，不使用缓存

# Langfuse 配置（如果启用）
# LANGFUSE_ENABLED=true
LANGFUSE_PUBLIC_KEY=your_public_key_here
LANGFUSE_SECRET_KEY=your_secret_key_here
# LANGFUSE_HOST=https://cloud.langfuse.com
# LANGFUSE_PROJECT=codebase-knowledge-builder
