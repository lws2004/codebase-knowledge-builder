# 环境变量配置示例
# 只需要配置必要的环境变量，其他配置可以在 config/default.yml 中设置

# LLM API 密钥（必需）
LLM_API_KEY=your_api_key_here

# LLM 提供商（可选，默认值在配置文件中设置）
# 如果需要覆盖配置文件中的值，取消注释并修改
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4

# 节点特定的模型配置（可选）
# 如果需要为特定节点设置不同的模型，可以使用以下格式的环境变量
# LLM_MODEL_GENERATE_OVERALL_ARCHITECTURE=gpt-4
# LLM_MODEL_GENERATE_API_DOCS=gpt-3.5-turbo
# LLM_MODEL_GENERATE_TIMELINE=gpt-3.5-turbo
# LLM_MODEL_GENERATE_DEPENDENCY=gpt-3.5-turbo
# LLM_MODEL_GENERATE_GLOSSARY=gpt-3.5-turbo
# LLM_MODEL_GENERATE_QUICK_LOOK=gpt-3.5-turbo
# LLM_MODEL_CONTENT_QUALITY_CHECK=gpt-3.5-turbo
# LLM_MODEL_GENERATE_MODULE_DETAILS=gpt-4
# LLM_MODEL_MODULE_QUALITY_CHECK=gpt-3.5-turbo
# LLM_MODEL_AI_UNDERSTAND_CORE_MODULES=gpt-4
# LLM_MODEL_COMBINE_TRANSLATE=gpt-3.5-turbo
# LLM_MODEL_INTERACTIVE_QA=gpt-4

# LLM Token 限制配置（建议同时设置两个参数）
# 控制LLM输出的最大token数
# LLM_MAX_TOKENS=4000
# 控制输入到LLM的最大token数
# LLM_MAX_INPUT_TOKENS=25000
# 注意: 输入token数 + 输出token数 的总和不应超过模型的最大上下文长度
# 例如: OpenRouter的最大上下文长度为40960，则 LLM_MAX_INPUT_TOKENS + LLM_MAX_TOKENS 应小于40960

# LLM_TEMPERATURE=0.7

# 提供商特定配置（可选，默认值在配置文件中设置）
# 如果使用 OpenRouter，可能需要设置以下环境变量
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
# APP_URL=http://localhost:3000
# APP_NAME=Codebase Knowledge Builder

# Langfuse 配置（如果启用）
# LANGFUSE_ENABLED=true
LANGFUSE_PUBLIC_KEY=your_public_key_here
LANGFUSE_SECRET_KEY=your_secret_key_here
# LANGFUSE_HOST=https://cloud.langfuse.com
# LANGFUSE_PROJECT=codebase-knowledge-builder
