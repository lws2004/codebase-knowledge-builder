#!/usr/bin/env python3
"""éªŒè¯LLMé…ç½®çš„è„šæœ¬"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def validate_config():
    """éªŒè¯LLMé…ç½®"""
    print("ğŸ”§ éªŒè¯LLMé…ç½®\n")

    try:
        from src.utils.env_manager import get_llm_config, load_env_vars
        from src.utils.llm_client import LLMClient

        # åŠ è½½ç¯å¢ƒå˜é‡
        load_env_vars()

        # è·å–é…ç½®
        config = get_llm_config()

        print("ğŸ“‹ å½“å‰é…ç½®:")
        print(f"  - æä¾›å•†: {config.get('provider')}")
        print(f"  - æ¨¡å‹: {config.get('model')}")
        print(f"  - APIå¯†é’¥: {'å·²è®¾ç½®' if config.get('api_key') else 'æœªè®¾ç½®'}")
        print(f"  - Base URL: {config.get('base_url', 'é»˜è®¤')}")
        print(f"  - æœ€å¤§Tokenæ•°: {config.get('max_tokens')}")
        print(f"  - æ¸©åº¦: {config.get('temperature')}")

        # æ£€æŸ¥é…ç½®æ¥æº
        print("\nğŸ” é…ç½®æ¥æºåˆ†æ:")
        llm_base_url = os.getenv("LLM_BASE_URL")
        openai_base_url = os.getenv("OPENAI_BASE_URL")

        if llm_base_url:
            print(f"  - ä½¿ç”¨ LLM_BASE_URL: {llm_base_url}")
        elif openai_base_url:
            print(f"  - ä½¿ç”¨ OPENAI_BASE_URL: {openai_base_url}")
        else:
            print("  - ä½¿ç”¨é»˜è®¤é…ç½®æˆ–é…ç½®æ–‡ä»¶")

        # æµ‹è¯•LLMå®¢æˆ·ç«¯
        print("\nğŸ¤– æµ‹è¯•LLMå®¢æˆ·ç«¯:")
        client = LLMClient(config)
        print("  âœ… LLMå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")

        # æµ‹è¯•tokenè®¡ç®—
        test_text = "Hello, this is a test."
        token_count = client.count_tokens(test_text)
        print(f"  âœ… Tokenè®¡ç®—æˆåŠŸ: {token_count}")

        # æµ‹è¯•æ–‡æœ¬ç”Ÿæˆï¼ˆå¦‚æœAPIå¯†é’¥æœ‰æ•ˆï¼‰
        if config.get("api_key") and config.get("api_key") not in ["your_api_key_here", "test-key"]:
            print("  ğŸ”„ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
            try:
                response = client.generate_text("è¯·è¯´ä¸€å¥è¯", max_tokens=10)
                print(f"  âœ… æ–‡æœ¬ç”ŸæˆæˆåŠŸ: {response[:50]}...")
            except Exception as e:
                print(f"  âŒ æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {str(e)}")
                return False
        else:
            print("  âš ï¸  è·³è¿‡æ–‡æœ¬ç”Ÿæˆæµ‹è¯•ï¼ˆæœªè®¾ç½®æœ‰æ•ˆAPIå¯†é’¥ï¼‰")

        print("\nğŸ‰ é…ç½®éªŒè¯å®Œæˆï¼")
        return True

    except Exception as e:
        print(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {str(e)}")
        return False


def show_config_examples():
    """æ˜¾ç¤ºé…ç½®ç¤ºä¾‹"""
    print("\nğŸ“š é…ç½®ç¤ºä¾‹:")

    print("\nğŸ”¹ é˜¿é‡Œäº‘DashScope (æ¨è):")
    print("LLM_API_KEY=sk-your-dashscope-api-key")
    print("LLM_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1")
    print("LLM_MODEL=qwen-turbo")

    print("\nğŸ”¹ OpenAI:")
    print("LLM_API_KEY=sk-your-openai-api-key")
    print("LLM_MODEL=gpt-4")
    print("# LLM_BASE_URL=https://api.openai.com/v1  # å¯é€‰")

    print("\nğŸ”¹ OpenRouter:")
    print("LLM_API_KEY=sk-your-openrouter-api-key")
    print("LLM_MODEL=openrouter/anthropic/claude-3-opus-20240229  # å¿…é¡»åŒ…å«openrouter/å‰ç¼€")
    print("LLM_BASE_URL=https://openrouter.ai/api/v1")
    print("# æ³¨æ„ï¼šæ¨¡å‹æ ¼å¼å¿…é¡»ä¸º openrouter/provider/model-name")

    print("\nğŸ”¹ è‡ªå®šä¹‰ç«¯ç‚¹:")
    print("LLM_API_KEY=your-custom-api-key")
    print("LLM_MODEL=your-custom-model")
    print("LLM_BASE_URL=https://your-custom-endpoint.com/v1")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LLMé…ç½®éªŒè¯å·¥å…·\n")

    # æ£€æŸ¥.envæ–‡ä»¶
    env_file = Path(".env")
    if not env_file.exists():
        print("âŒ .envæ–‡ä»¶ä¸å­˜åœ¨")
        print("ğŸ’¡ è¯·å¤åˆ¶.env.exampleä¸º.envå¹¶é…ç½®APIå¯†é’¥")
        show_config_examples()
        return False

    # éªŒè¯é…ç½®
    success = validate_config()

    if not success:
        print("\nğŸ’¡ é…ç½®å»ºè®®:")
        show_config_examples()
        print("\nğŸ“– æ›´å¤šå¸®åŠ©è¯·å‚è€ƒ: docs/llm_config_troubleshooting.md")

    return success


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
