# Design Doc: ä»£ç åº“æ•™ç¨‹ç”Ÿæˆ Agent ğŸ“„ğŸ¤–

> è¯·å‹¿ç§»é™¤ AI æç¤º

## ğŸ“ æ‘˜è¦

æœ¬è®¾è®¡æ–‡æ¡£æè¿°äº†ä¸€ä¸ªåˆ©ç”¨ AI æŠ€æœ¯è‡ªåŠ¨åˆ†æä»£ç åº“å¹¶ç”Ÿæˆæ•™ç¨‹çš„æ™ºèƒ½ Agent ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿä»¥å¤§å‹è¯­è¨€æ¨¡å‹(LLM)ä¸ºæ ¸å¿ƒï¼Œç»“åˆä»£ç åˆ†æå·¥å…·å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯ï¼Œèƒ½å¤Ÿç”Ÿæˆç»“æ„åŒ–ã€å¤šè¯­è¨€ã€å¯Œæœ‰æ´å¯ŸåŠ›çš„ä»£ç åº“æ•™ç¨‹ï¼Œå¹¶æ”¯æŒäº¤äº’å¼é—®ç­”å’Œå¤šç§å‘å¸ƒæ–¹å¼ã€‚ç³»ç»Ÿè®¾è®¡éµå¾ªæ¨¡å—åŒ–åŸåˆ™ï¼Œå¼ºè°ƒé”™è¯¯å¤„ç†ã€å¯æ‰©å±•æ€§å’Œç”¨æˆ·åé¦ˆæœºåˆ¶ã€‚

**æ ¸å¿ƒç†å¿µ**: æœ¬è®¾è®¡å¼ºè°ƒåˆ©ç”¨ AI (å¤§å‹è¯­è¨€æ¨¡å‹, LLM) ä½œä¸ºæ ¸å¿ƒå¼•æ“æ¥ **ç†è§£** ä»£ç åº“å¹¶ **ç”Ÿæˆ** å¯Œæœ‰æ´å¯ŸåŠ›çš„æ–‡æ¡£å†…å®¹ï¼Œè€Œéä»…ä»…æå–å’Œæ ¼å¼åŒ–ç°æœ‰ä¿¡æ¯ã€‚ä»£ç åˆ†æå·¥å…·ä¸»è¦ä¸º AI æä¾›ä¸Šä¸‹æ–‡ï¼ŒRAG ç”¨äºç¡®ä¿ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å¹¶æ”¯æŒäº¤äº’é—®ç­”ã€‚ **å®ç°ä»£ç åº”è¿½æ±‚ç®€æ´ã€é«˜æ•ˆã€‚** âœ¨

## ğŸŒŸ éœ€æ±‚ (Requirements)

> AI æç¤º: ä¿æŒç®€æ´æ˜äº†ã€‚
> å¦‚æœéœ€æ±‚æŠ½è±¡ï¼Œè¯·ç¼–å†™å…·ä½“çš„ç”¨æˆ·æ•…äº‹ï¼Œå¹¶ç¡®ä¿éœ€æ±‚ä¸åç»­è®¾è®¡ï¼ˆå¦‚ AI æ ¸å¿ƒã€å±‚çº§ç»“æ„ã€å¤šè¯­è¨€ï¼‰ä¿æŒä¸€è‡´ã€‚

è¯¥ Agent æ—¨åœ¨åˆ©ç”¨ AI æ·±å…¥ç†è§£ä»£ç åº“ï¼Œå¹¶ä¸ºå¤šç§ç”¨æˆ·è§’è‰²è‡ªåŠ¨ç”Ÿæˆå’Œå®šåˆ¶å¯Œæœ‰æ´å¯ŸåŠ›çš„æ•™ç¨‹ã€‚éœ€æ±‚æŒ‰ä¼˜å…ˆçº§åˆ†ä¸ºæ ¸å¿ƒéœ€æ±‚ã€å¢å¼ºéœ€æ±‚å’Œå¯é€‰éœ€æ±‚ï¼š

### æ ¸å¿ƒéœ€æ±‚ï¼ˆP0ï¼‰

1.  **å¼€æºé¡¹ç›®ç»´æŠ¤è€…**: **AI è‡ªåŠ¨åˆ†æå¹¶ç†è§£é¡¹ç›®æ¶æ„**ï¼Œç”Ÿæˆ**ç»“æ„æ¸…æ™°ã€ä»æ•´ä½“åˆ°æ¨¡å—ç»†èŠ‚é€å±‚å±•å¼€**çš„æ•™ç¨‹ï¼Œå¸®åŠ©æ–°è´¡çŒ®è€…å¿«é€Ÿä¸Šæ‰‹ã€‚
    - _éªŒæ”¶æ ‡å‡†_: èƒ½å¤Ÿè¯†åˆ«å¹¶è§£é‡Šé¡¹ç›®çš„æ ¸å¿ƒæ¨¡å—ã€æ¶æ„å’Œå…³é”®ç»„ä»¶ï¼Œç”Ÿæˆå±‚æ¬¡åˆ†æ˜çš„æ–‡æ¡£ç»“æ„ã€‚

2.  **æ–°æ‰‹å¼€å‘è€…**: é€šè¿‡äº¤äº’å¼é—®ç­”ï¼ˆ**ç”± AI Agent åŸºäº RAG ç†è§£å¹¶å›ç­”**ï¼‰å®šåˆ¶ä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„ï¼Œå¿«é€Ÿå®šä½å…³é”®æ¨¡å—ä¸æ ¸å¿ƒåŠŸèƒ½çš„å®ç°ã€‚
    - _éªŒæ”¶æ ‡å‡†_: èƒ½å¤Ÿå‡†ç¡®å›ç­”å…³äºä»£ç åº“ç»“æ„ã€åŠŸèƒ½å’Œå®ç°ç»†èŠ‚çš„é—®é¢˜ï¼Œæä¾›ç›¸å…³ä»£ç å¼•ç”¨ã€‚

3.  **ç»Ÿä¸€è¾“å…¥æº**: ä¸ºæ–¹ä¾¿ä½¿ç”¨ï¼ŒAgent åº”èƒ½æ¥å—**ç»Ÿä¸€çš„ä»£ç åº“æ¥æº**ï¼ˆæœ¬åœ°æ–‡ä»¶è·¯å¾„æˆ–è¿œç¨‹ Git URLï¼‰ã€‚
    - _éªŒæ”¶æ ‡å‡†_: ç³»ç»Ÿèƒ½å¤Ÿå¤„ç†æœ¬åœ°è·¯å¾„å’Œè¿œç¨‹ Git URLï¼Œè‡ªåŠ¨è¿›è¡Œå¿…è¦çš„å…‹éš†å’ŒéªŒè¯ã€‚

4.  **ä¾¿æ·åˆ†å‘ä¸å¯Œæ–‡æœ¬å‘ˆç°**: æ”¯æŒå¤šæ–‡ä»¶ Markdown (**é€‚é… GitHub/GitLab æ–‡æ¡£è§„èŒƒï¼ŒåŒ…å«æ¸…æ™°çš„å†…éƒ¨é“¾æ¥ä»¥ä¾¿äº GitHub Pages ç­‰ç½‘é¡µæµè§ˆ**) å’Œ PDF å¯¼å‡ºï¼Œæ–‡ä»¶é—´å…·æœ‰ä¾¿æ·å¯¼èˆªï¼Œä»£ç å¼•ç”¨å¯ç›´æ¥é“¾æ¥åˆ°æºç ã€‚
    - _éªŒæ”¶æ ‡å‡†_:
      - ç”Ÿæˆçš„æ–‡æ¡£è¢«æ‹†åˆ†ä¸ºå¤šä¸ª Markdown æ–‡ä»¶ï¼ŒåŒ…æ‹¬ä¸»æ–‡æ¡£å’Œå„æ¨¡å—/åŠŸèƒ½çš„å­æ–‡æ¡£
      - æ–‡ä»¶é—´å…·æœ‰æ¸…æ™°çš„å¯¼èˆªé“¾æ¥ï¼ˆä¸Šä¸€é¡µ/ä¸‹ä¸€é¡µ/ç›®å½•ï¼‰
      - ä»£ç å¼•ç”¨éƒ¨åˆ†åŒ…å«åˆ°æºä»£ç æ–‡ä»¶çš„ç›´æ¥é“¾æ¥
      - æ‰€æœ‰æ–‡æ¡£ç¬¦åˆ Markdown è§„èŒƒï¼Œå¯ä»¥å¯¼å‡ºä¸º PDF

### å¢å¼ºéœ€æ±‚ï¼ˆP1ï¼‰

5.  **æŠ€æœ¯æ•™è‚²è€…**: **AI æ ¹æ®ä»£ç é€»è¾‘ç”Ÿæˆ**åŒ…å«ä»£ç ç‰‡æ®µã€æµç¨‹å›¾ç¤ºï¼ˆåŠå…¶è§£é‡Šï¼‰å’Œç”¨æ³•ç¤ºä¾‹çš„æ•™ç¨‹ï¼Œè¾…åŠ©å­¦ç”Ÿå®è·µæŒæ¡å¼€å‘æ€ç»´ã€‚
    - _éªŒæ”¶æ ‡å‡†_: ç”Ÿæˆçš„æ•™ç¨‹åŒ…å«æœ‰æ„ä¹‰çš„ä»£ç ç¤ºä¾‹å’Œå¯è§†åŒ–å›¾è¡¨ï¼Œèƒ½å¤Ÿæ¸…æ™°è§£é‡Šä»£ç é€»è¾‘ã€‚

6.  **æŠ€æœ¯å›¢é˜Ÿè´Ÿè´£äºº**: **AI ç»“åˆä»£ç åˆ†æä¸ç‰ˆæœ¬å†å²ç†è§£**ï¼Œç”Ÿæˆèƒ½è‡ªåŠ¨å…³è”æœ€æ–°ä»£ç è®¾è®¡æ€è·¯çš„æ•™ç¨‹ï¼Œç¡®ä¿æ–°æˆå‘˜é«˜æ•ˆåŒæ­¥ã€‚
    - _éªŒæ”¶æ ‡å‡†_: æ•™ç¨‹èƒ½å¤Ÿåæ˜ ä»£ç åº“çš„æœ€æ–°çŠ¶æ€ï¼Œå¹¶è§£é‡Šå…³é”®è®¾è®¡å†³ç­–ã€‚

7.  **å›½é™…åŒ–ç¤¾åŒº**: æ”¯æŒ**æ ¹æ®ç”¨æˆ·æŒ‡å®šè¯­è¨€ï¼ˆå¦‚ä¸­ã€è‹±æ–‡ï¼‰ç›´æ¥ç”Ÿæˆ**æ•™ç¨‹å†…å®¹ï¼ŒåŒæ—¶ä¿æŒä»£ç å’ŒæŠ€æœ¯æœ¯è¯­çš„å‡†ç¡®æ€§ã€‚
    - _éªŒæ”¶æ ‡å‡†_: èƒ½å¤Ÿç”ŸæˆæŒ‡å®šè¯­è¨€çš„æ•™ç¨‹ï¼ŒæŠ€æœ¯æœ¯è¯­ä¿æŒä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚

8.  **å¼€å‘è€…ä½“éªŒ**: **AI åŸºäºä»£ç ç†è§£ç”Ÿæˆ** API åŠŸèƒ½è¯´æ˜ï¼Œå¹¶ç»“åˆä»£ç ä¸Šä¸‹æ–‡ç”Ÿæˆæˆ–æå–å¯è¿è¡Œçš„æµ‹è¯•ç”¨ä¾‹ä½œä¸ºåŠŸèƒ½æ¼”ç¤ºã€‚
    - _éªŒæ”¶æ ‡å‡†_: ç”Ÿæˆçš„ API æ–‡æ¡£å‡†ç¡®åæ˜ ä»£ç åŠŸèƒ½ï¼Œæä¾›çš„æµ‹è¯•ç”¨ä¾‹å¯ä»¥è¿è¡Œå¹¶æ¼”ç¤ºåŠŸèƒ½ã€‚

9.  **æ¶æ„æ¼”å˜ç†è§£**: **AI è§£è¯» Commit å†å²**ï¼Œç”ŸæˆåŠ¨æ€çš„ä»£ç æ¼”å˜æ—¶é—´è½´æˆ–å…³é”®èŠ‚ç‚¹å™è¿°ï¼Œå±•ç¤ºæ¶æ„è¿­ä»£å†³ç­–è¿‡ç¨‹ã€‚
    - _éªŒæ”¶æ ‡å‡†_: èƒ½å¤Ÿä» Git å†å²ä¸­æå–å¹¶è§£é‡Šå…³é”®çš„æ¶æ„å˜æ›´ï¼Œå½¢æˆæœ‰æ„ä¹‰çš„æ¼”å˜å™è¿°ã€‚

### å¯é€‰éœ€æ±‚ï¼ˆP2ï¼‰

10. **éæŠ€æœ¯èƒŒæ™¯è´¡çŒ®è€…**: **AI ç”Ÿæˆ**å¯è§†åŒ–çš„ä¾èµ–å…³ç³»å›¾ï¼ˆåŠå…¶è§£é‡Šï¼‰å’Œé€šä¿—æ˜“æ‡‚çš„æœ¯è¯­è¡¨ï¼Œé™ä½ç†è§£ä»£ç é€»è¾‘çš„é—¨æ§›ã€‚
    - _éªŒæ”¶æ ‡å‡†_: ç”Ÿæˆçš„ä¾èµ–å›¾å’Œæœ¯è¯­è¡¨æ˜“äºç†è§£ï¼Œé€‚åˆéæŠ€æœ¯èƒŒæ™¯äººå‘˜é˜…è¯»ã€‚

11. **å¿«é€Ÿæ¦‚è§ˆ**: **AI æç‚¼æ ¸å¿ƒè®¾è®¡æ€æƒ³**ï¼Œç”ŸæˆåŒ…å«é«˜äº®æ³¨é‡Šå’Œæ¶æ„å›¾è§£é‡Šçš„"äº”åˆ†é’Ÿé€Ÿè§ˆ"æ¨¡å¼ã€‚
    - _éªŒæ”¶æ ‡å‡†_: èƒ½å¤Ÿç”Ÿæˆç®€æ´çš„é¡¹ç›®æ¦‚è§ˆï¼Œçªå‡ºæ ¸å¿ƒè®¾è®¡ç†å¿µå’Œå…³é”®ç»„ä»¶ã€‚

12. **GitHub Pages å‘å¸ƒ**: æ”¯æŒå°†ç”Ÿæˆçš„ Markdown æ•™ç¨‹ç›´æ¥æ„å»ºæˆ–é…ç½®ä»¥ä¾¿å‘å¸ƒä¸º GitHub Pages ç«™ç‚¹ã€‚
    - _éªŒæ”¶æ ‡å‡†_: èƒ½å¤Ÿè‡ªåŠ¨é…ç½®å¹¶å‘å¸ƒæ–‡æ¡£åˆ° GitHub Pagesï¼Œç”Ÿæˆå¯è®¿é—®çš„ç½‘ç«™ã€‚

### éåŠŸèƒ½æ€§éœ€æ±‚

1. **æ€§èƒ½è¦æ±‚**: ç³»ç»Ÿåº”èƒ½å¤„ç†ä¸­å¤§å‹ä»£ç åº“ï¼ˆ10ä¸‡è¡Œä»£ç ä»¥ä¸Šï¼‰ï¼Œç”Ÿæˆè¿‡ç¨‹çš„å“åº”æ—¶é—´åº”åœ¨å¯æ¥å—èŒƒå›´å†…ï¼ˆåˆå§‹åˆ†æå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œä½†äº¤äº’å¼é—®ç­”åº”åœ¨ç§’çº§å“åº”ï¼‰ã€‚

2. **å®‰å…¨æ€§**: ç³»ç»Ÿåº”æä¾›å®‰å…¨æœºåˆ¶å¤„ç†ç§æœ‰ä»£ç åº“ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºè®¤è¯ã€æˆæƒå’Œæ•æ„Ÿä¿¡æ¯ä¿æŠ¤ã€‚

3. **å¯é æ€§**: ç³»ç»Ÿåº”å…·å¤‡é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶ï¼Œåœ¨å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜æ—¶èƒ½å¤Ÿæä¾›æœ‰æ„ä¹‰çš„é”™è¯¯ä¿¡æ¯å¹¶å°½å¯èƒ½æ¢å¤ã€‚

## ğŸŒŠ æµç¨‹è®¾è®¡ (Flow Design)

> AI æç¤º:
>
> 1. è€ƒè™‘ Agentã€MapReduceã€RAGã€Workflow ç­‰è®¾è®¡æ¨¡å¼ï¼Œå¹¶åœ¨é€‚ç”¨çš„åœ°æ–¹åº”ç”¨ã€‚
> 2. å‘ˆç°ç®€æ´ã€é«˜å±‚æ¬¡çš„å·¥ä½œæµæè¿°ï¼Œå¼ºè°ƒ AI åœ¨åˆ†æå’Œç”Ÿæˆä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚

### é€‚ç”¨çš„è®¾è®¡æ¨¡å¼:

1.  **Workflow**: æ•´ä¸ªæ•™ç¨‹ç”Ÿæˆè¿‡ç¨‹æ˜¯ä¸€ä¸ªå¤šé˜¶æ®µçš„æµæ°´çº¿ï¼Œç”± AI ç†è§£å’Œç”Ÿæˆé©±åŠ¨ã€‚
2.  **MapReduce**:
    - _Map_: å¹¶è¡Œåˆ†æä»£ç åº“ä¸­çš„æ¯ä¸ªæ–‡ä»¶ï¼ˆæå–åŸºç¡€ç»“æ„ä¾› AI ç†è§£ï¼‰æˆ–æ¯ä¸ª Commitï¼ˆåˆ†æå˜æ›´ï¼‰ã€‚
    - _Reduce_: æ±‡æ€»åˆ†æç»“æœï¼Œå¯èƒ½åˆ©ç”¨ AI ç”Ÿæˆç»Ÿä¸€çš„æ¦‚è§ˆæˆ–æ€»ç»“ã€‚
3.  **Agent/RAG**: Agent åˆ©ç”¨ LLM ç†è§£ç”¨æˆ·é—®é¢˜ï¼Œå¹¶é€šè¿‡ RAG ä»ä»£ç åº“åŠ AI åˆæ­¥åˆ†æç»“æœä¸­æ£€ç´¢ä¿¡æ¯ï¼ŒåŠ¨æ€ç”Ÿæˆç­”æ¡ˆæˆ–å®šåˆ¶æ•™ç¨‹ã€‚
4.  **é”™è¯¯å¤„ç†ä¸æ¢å¤**: åœ¨å„ä¸ªé˜¶æ®µå®ç°é”™è¯¯æ£€æµ‹ã€æ—¥å¿—è®°å½•å’Œæ¢å¤æœºåˆ¶ï¼Œç¡®ä¿ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚
5.  **ç”¨æˆ·åé¦ˆå¾ªç¯**: æ”¶é›†ç”¨æˆ·å¯¹ç”Ÿæˆå†…å®¹çš„åé¦ˆï¼Œç”¨äºæ”¹è¿›åç»­ç”Ÿæˆè¿‡ç¨‹å’Œç»“æœè´¨é‡ã€‚

### æµç¨‹é«˜å±‚è®¾è®¡:

```mermaid
flowchart TD
    subgraph Stage1 ["ğŸ·ï¸ 1: è¾“å…¥ä¸å‡†å¤‡"]
        direction TB
        Input["è¾“å…¥"] --> Prep("å‡†å¤‡ä»£ç åº“");
        Prep --> ErrorCheck1{"é”™è¯¯æ£€æŸ¥"};
        ErrorCheck1 -- å¤±è´¥ --> HandleError1["é”™è¯¯å¤„ç†"];
        HandleError1 --> Retry1["é‡è¯•/é€€å‡º"];
    end

    subgraph Stage2 ["ğŸ·ï¸ 2: AI ç†è§£"]
        direction TB
        Understand("AI ç†è§£ä»£ç åº“");
        Understand --> ErrorCheck2{"é”™è¯¯æ£€æŸ¥"};
        ErrorCheck2 -- å¤±è´¥ --> HandleError2["é”™è¯¯å¤„ç†"];
        HandleError2 --> Retry2["é‡è¯•/é™çº§å¤„ç†"];
    end

    subgraph Stage3 ["ğŸ·ï¸ 3: AI ç”Ÿæˆ"]
        direction TB
        Generate("AI ç”Ÿæˆå†…å®¹");
        Generate --> ErrorCheck3{"é”™è¯¯æ£€æŸ¥"};
        ErrorCheck3 -- å¤±è´¥ --> HandleError3["é”™è¯¯å¤„ç†"];
        HandleError3 --> Retry3["é‡è¯•/é™çº§ç”Ÿæˆ"];
    end

    subgraph Stage4 ["ğŸ·ï¸ 4: ç»„åˆä¸æ ¼å¼åŒ–"]
        direction TB
        Combine("ç»„åˆ/ç¿»è¯‘") --> Format("æ ¼å¼åŒ–è¾“å‡º");
        Format --> ErrorCheck4{"é”™è¯¯æ£€æŸ¥"};
        ErrorCheck4 -- å¤±è´¥ --> HandleError4["é”™è¯¯å¤„ç†"];
        HandleError4 --> Retry4["é‡è¯•/é™çº§è¾“å‡º"];
    end

    subgraph Stage5 ["ğŸ·ï¸ 5: äº¤äº’é—®ç­” (å¯é€‰)"]
        direction TB
        OptionalQA{"äº¤äº’é—®ç­”?"};
        QA("å¤„ç†é—®ç­”");
        OptionalQA -- æ˜¯ --> QA;
        QA --> UserFeedback["æ”¶é›†ç”¨æˆ·åé¦ˆ"];
        UserFeedback --> FeedbackLoop["åé¦ˆå¾ªç¯"];
        FeedbackLoop -.-> Understand;
    end

    subgraph Stage6 ["ğŸ·ï¸ 6: å‘å¸ƒ (å¯é€‰)"]
        direction TB
        CheckPublish{"å‘å¸ƒ?"};
        DoPublish("å‘å¸ƒ");
        CheckPublish -- æ˜¯ --> DoPublish;
        DoPublish --> PublishCheck{"å‘å¸ƒæ£€æŸ¥"};
        PublishCheck -- å¤±è´¥ --> HandlePublishError["å¤„ç†å‘å¸ƒé”™è¯¯"];
        HandlePublishError --> RetryPublish["é‡è¯•/æ‰‹åŠ¨æŒ‡å¯¼"];
    end

    Done(å®Œæˆ);

    %% è¿æ¥å„ä¸ªé˜¶æ®µ
    ErrorCheck1 -- æˆåŠŸ --> Understand;
    ErrorCheck2 -- æˆåŠŸ --> Generate;
    ErrorCheck3 -- æˆåŠŸ --> Combine;
    ErrorCheck4 -- æˆåŠŸ --> OptionalQA;
    OptionalQA -- å¦ --> CheckPublish;
    QA --> CheckPublish;
    CheckPublish -- å¦ --> Done;
    PublishCheck -- æˆåŠŸ --> Done;
    RetryPublish --> Done;
    Retry1 -.-> Done;
    Retry2 -.-> Done;
    Retry3 -.-> Done;
    Retry4 -.-> Done;
```

**åˆ†é˜¶æ®µè¯¦è§£**

#### `ğŸ·ï¸ 1`: è¾“å…¥ä¸ä»£ç åº“å‡†å¤‡

```mermaid
flowchart TD
    A["ğŸ“¥ è¾“å…¥: repo_source (URL/è·¯å¾„), ç”¨æˆ·é—®é¢˜(å¯é€‰), è¯­è¨€, æ ¼å¼"] --> PrepareRepo{"âš™ï¸ å‡†å¤‡æœ¬åœ°ä»£ç åº“"};
    PrepareRepo -- å…‹éš† --> CloneRepo["â˜ï¸ Git Clone"];
    PrepareRepo -- éªŒè¯ --> ValidateLocalPath["âœ”ï¸ éªŒè¯æœ¬åœ°è·¯å¾„"];
    CloneRepo --> SizeCheck{"ğŸ“ ä»£ç åº“å¤§å°æ£€æŸ¥"};
    SizeCheck -- è¿‡å¤§ --> SplitRepo["ğŸ”ª åˆ†å‰²å¤„ç†ç­–ç•¥"];
    SizeCheck -- é€‚ä¸­ --> Stage2_1_Input;
    ValidateLocalPath --> PermissionCheck{"ğŸ”’ æƒé™æ£€æŸ¥"};
    PermissionCheck -- å¤±è´¥ --> RequestPermission["ğŸ”‘ è¯·æ±‚æƒé™"];
    PermissionCheck -- æˆåŠŸ --> Stage2_1_Input("è¿›å…¥ ğŸ·ï¸ 2.1: ä»£ç è§£æ");
    SplitRepo --> Stage2_1_Input;
    RequestPermission --> Stage2_1_Input;
```

> _æ­¤é˜¶æ®µç¡®ä¿æˆ‘ä»¬æœ‰ä¸€ä¸ªæœ‰æ•ˆçš„æœ¬åœ°ä»£ç åº“è·¯å¾„ (`local_repo_path`) ä¾›åç»­ä½¿ç”¨ï¼Œå¹¶å¤„ç†å¤§å‹ä»£ç åº“å’Œæƒé™é—®é¢˜ã€‚_

#### `ğŸ·ï¸ 2`: ä»£ç åº“ AI ç†è§£

##### `ğŸ·ï¸ 2.1`: ä»£ç è§£æ (å¹¶è¡Œ)

```mermaid
flowchart TD
    Start["æ¥è‡ª ğŸ·ï¸ 1: éªŒè¯åçš„æœ¬åœ°ä»£ç åº“è·¯å¾„"]
    Start --> LanguageDetect["ğŸ” è¯­è¨€æ£€æµ‹"];
    LanguageDetect --> Parse(è§£æä»»åŠ¡);
    Parse --> D["ğŸ› ï¸ åŸºç¡€ç»“æ„/æ³¨é‡Š/API"];
    Parse --> E["â³ Commitå†å²"];
    Parse --> F["ğŸ”— ä¾èµ–å…³ç³»"];
    D --> ErrorCheckD{"âœ“ è§£ææ£€æŸ¥"};
    ErrorCheckD -- å¤±è´¥ --> RetryD["â™»ï¸ é‡è¯•/é™çº§"];
    ErrorCheckD -- æˆåŠŸ --> MergeResults;
    E --> ErrorCheckE{"âœ“ å†å²æ£€æŸ¥"};
    ErrorCheckE -- å¤±è´¥ --> RetryE["â™»ï¸ é‡è¯•/é™çº§"];
    ErrorCheckE -- æˆåŠŸ --> MergeResults;
    F --> ErrorCheckF{"âœ“ ä¾èµ–æ£€æŸ¥"};
    ErrorCheckF -- å¤±è´¥ --> RetryF["â™»ï¸ é‡è¯•/é™çº§"];
    ErrorCheckF -- æˆåŠŸ --> MergeResults;
    RetryD & RetryE & RetryF --> MergeResults["ğŸ”„ åˆå¹¶å¯ç”¨ç»“æœ"];
    MergeResults --> Stage2_2_Input("è¿›å…¥ ğŸ·ï¸ 2.2: AIæ ¸å¿ƒç†è§£");
```

> _å¹¶è¡Œè§£æä»£ç åº“ï¼Œæå–åŸºç¡€ä¿¡æ¯ï¼ŒåŒ…å«é”™è¯¯å¤„ç†å’Œè¯­è¨€æ£€æµ‹ã€‚_

##### `ğŸ·ï¸ 2.2`: AI æ ¸å¿ƒç†è§£

```mermaid
flowchart TD
    Input["æ¥è‡ª ğŸ·ï¸ 2.1: ä»£ç ç»“æ„ä¸ä¾èµ–å…³ç³»"]
    Input --> UnderstandCore{"ğŸ¤– AI: ç†è§£æ ¸å¿ƒæ¨¡å—/æ¶æ„"};
    UnderstandCore --> QualityCheck{"âœ“ è´¨é‡æ£€æŸ¥"};
    QualityCheck -- ä¸æ»¡è¶³ --> Refine["ğŸ”„ ç»†åŒ–ç†è§£"];
    QualityCheck -- æ»¡è¶³ --> Stage2_3_Input("è¿›å…¥ ğŸ·ï¸ 2.3: RAGæ•°æ®å‡†å¤‡");
    Refine --> UnderstandCore;
```

> _AI åŸºäºè§£æç»“æœè¿›è¡Œæ ¸å¿ƒç†è§£ï¼ŒåŒ…å«è´¨é‡æ£€æŸ¥å’Œè¿­ä»£ç»†åŒ–ã€‚_

##### `ğŸ·ï¸ 2.3`: RAG æ•°æ®å‡†å¤‡

```mermaid
flowchart TD
    Input["æ¥è‡ª ğŸ·ï¸ 2.1: ä»£ç ç»“æ„, Commitå†å² & ğŸ·ï¸ 2.2: AIæ ¸å¿ƒç†è§£"]
    Input --> Chunking["ğŸ“„ æ–‡æœ¬åˆ†å—"];
    Chunking --> Embedding["ğŸ§  ç”ŸæˆåµŒå…¥å‘é‡"];
    Embedding --> IndexBuild["ğŸ“š æ„å»ºå‘é‡ç´¢å¼•"];
    IndexBuild --> CacheStrategy["ğŸ’¾ ç¼“å­˜ç­–ç•¥"];
    CacheStrategy --> Stage3_Start("è¿›å…¥ ğŸ·ï¸ 3: å†…å®¹ç”Ÿæˆ");
```

> _ç»“åˆä»£ç è§£æã€å†å²ä¿¡æ¯å’Œ AI ç†è§£ç»“æœå‡†å¤‡ RAG æ•°æ®ï¼ŒåŒ…å«åˆ†å—ç­–ç•¥å’Œç¼“å­˜æœºåˆ¶ã€‚_

#### `ğŸ·ï¸ 3`: AI å†…å®¹ç”Ÿæˆ

##### `ğŸ·ï¸ 3.1`: ç”Ÿæˆæ•´ä½“å†…å®¹ (å¹¶è¡Œ)

```mermaid
flowchart TD
    Start["æ¥è‡ª ğŸ·ï¸ 2: åˆ†æ/ç†è§£/RAGæ•°æ®"]
    Start --> G["ğŸ“ˆ æ¶æ„å›¾/è§£é‡Š"];
    Start --> H["ğŸ’¬ API åŠŸèƒ½è¯´æ˜ (æ¦‚è§ˆ)"];
    Start --> J["ğŸ•°ï¸ æ¼”å˜æ—¶é—´è½´è§£è¯»"];
    Start --> K["ğŸŒ ä¾èµ–å›¾/è§£é‡Š"];
    Start --> L["ğŸ“– æœ¯è¯­è§£é‡Š"];
    Start --> QuickLook["ğŸ’¡ é€Ÿè§ˆæ‘˜è¦"];
    G & H & J & K & L & QuickLook --> ContentQuality["âœ“ å†…å®¹è´¨é‡è¯„ä¼°"];
    ContentQuality -- ä¸æ»¡è¶³ --> RegenerateContent["ğŸ”„ é‡æ–°ç”Ÿæˆ"];
    ContentQuality -- æ»¡è¶³ --> Stage4_Start1("è¿›å…¥ ğŸ·ï¸ 4.1: å†…å®¹ç»„åˆ");
    RegenerateContent --> ContentQuality;
```

> _å¹¶è¡Œç”Ÿæˆæ•™ç¨‹çš„æ•´ä½“æ€§å†…å®¹ï¼ŒåŒ…å«è´¨é‡è¯„ä¼°å’Œé‡æ–°ç”Ÿæˆæœºåˆ¶ã€‚_

##### `ğŸ·ï¸ 3.2`: ç”Ÿæˆæ¨¡å—ç»†èŠ‚ (Batch/Loop)

```mermaid
flowchart TD
    Start["æ¥è‡ª ğŸ·ï¸ 2: åˆ†æ/ç†è§£/RAGæ•°æ®"]
    Start --> ModuleBatch("ForEach Module");
    ModuleBatch --> ModuleDetail["ğŸ“„ æ¨¡å—æè¿°/API/ç¤ºä¾‹/..."];
    ModuleDetail --> ModuleQuality["âœ“ æ¨¡å—å†…å®¹è´¨é‡æ£€æŸ¥"];
    ModuleQuality -- ä¸æ»¡è¶³ --> RegenerateModule["ğŸ”„ é‡æ–°ç”Ÿæˆ"];
    ModuleQuality -- æ»¡è¶³ --> Stage4_Start2("è¿›å…¥ ğŸ·ï¸ 4.1: å†…å®¹ç»„åˆ");
    RegenerateModule --> ModuleQuality;
```

> _éå†æ ¸å¿ƒæ¨¡å—ï¼Œä¸ºæ¯ä¸ªæ¨¡å—ç”Ÿæˆè¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…å«è´¨é‡æ£€æŸ¥å’Œé‡æ–°ç”Ÿæˆæœºåˆ¶ã€‚_

#### `ğŸ·ï¸ 4`: ç»„åˆã€æ ¼å¼åŒ–ä¸è¾“å‡º

##### `ğŸ·ï¸ 4.1`: å†…å®¹ç»„åˆ

```mermaid
flowchart TD
    Input["æ¥è‡ª ğŸ·ï¸ 3: ç”Ÿæˆçš„æ•´ä½“å’Œæ¨¡å—å†…å®¹ç‰‡æ®µ"]
    Input --> Combine["ğŸ§© ç»„åˆå†…å®¹ (æŒ‰å±‚çº§)"];
    Combine --> GenerateNavLinks["ğŸ”— ç”Ÿæˆå†…éƒ¨å¯¼èˆªé“¾æ¥"];
    GenerateNavLinks --> ConsistencyCheck["âœ“ ä¸€è‡´æ€§æ£€æŸ¥"];
    ConsistencyCheck -- ä¸ä¸€è‡´ --> FixConsistency["ğŸ”§ ä¿®å¤ä¸€è‡´æ€§é—®é¢˜"];
    ConsistencyCheck -- ä¸€è‡´ --> Stage4_2_1_Input("è¿›å…¥ ğŸ·ï¸ 4.2.1: ç¿»è¯‘æ£€æŸ¥");
    FixConsistency --> ConsistencyCheck;
```

> _æŒ‰å±‚çº§é¡ºåºç»„åˆæ‰€æœ‰ç”Ÿæˆçš„å†…å®¹ï¼Œç”Ÿæˆå†…éƒ¨å¯¼èˆªé“¾æ¥ï¼Œå¹¶è¿›è¡Œä¸€è‡´æ€§æ£€æŸ¥ã€‚_

##### `ğŸ·ï¸ 4.2.1`: ç¿»è¯‘æ£€æŸ¥ (å¯é€‰)

```mermaid
flowchart TD
    Input["æ¥è‡ª ğŸ·ï¸ 4.1: ç»„åˆåçš„å†…å®¹"]
    Input --> CheckTranslate{ğŸŒ éœ€è¦ç¿»è¯‘/æ£€æŸ¥?};
    CheckTranslate -- æ˜¯ --> DetectMixedLanguage["ğŸ” æ£€æµ‹æ··åˆè¯­è¨€"];
    DetectMixedLanguage --> PreserveTerms["ğŸ“ æ ‡è®°ä¿ç•™æœ¯è¯­"];
    PreserveTerms --> Translate["ğŸŒ AI: ç¿»è¯‘/æ£€æŸ¥"];
    CheckTranslate -- å¦ --> SkipTranslate((è·³è¿‡));
    Translate --> TranslationQuality["âœ“ ç¿»è¯‘è´¨é‡æ£€æŸ¥"];
    TranslationQuality -- ä¸æ»¡è¶³ --> RefineTranslation["ğŸ”„ ç»†åŒ–ç¿»è¯‘"];
    TranslationQuality -- æ»¡è¶³ --> OutputTextForFormatting(å‡†å¤‡æ ¼å¼åŒ–çš„æ–‡æœ¬);
    SkipTranslate --> OutputTextForFormatting;
    RefineTranslation --> TranslationQuality;
    OutputTextForFormatting --> Stage4_2_2_Start("è¿›å…¥ ğŸ·ï¸ 4.2.2: æ ¼å¼åŒ–è¾“å‡º");
```

> _æ­¤å­é˜¶æ®µå¤„ç†å¯é€‰çš„æœ€ç»ˆç¿»è¯‘æ£€æŸ¥ï¼ŒåŒ…å«æ··åˆè¯­è¨€æ£€æµ‹ã€æœ¯è¯­ä¿ç•™å’Œç¿»è¯‘è´¨é‡è¯„ä¼°ã€‚_

##### `ğŸ·ï¸ 4.2.2`: æ ¼å¼åŒ–è¾“å‡º

```mermaid
flowchart TD
    Input["æ¥è‡ª ğŸ·ï¸ 4.2.1: å‡†å¤‡æ ¼å¼åŒ–çš„æ–‡æœ¬"]
    Input --> Format["ğŸ¨ æ ¼å¼åŒ–è¾“å‡º"];
    Format --> ChooseFormat{"ğŸ“‘ è¾“å‡ºæ ¼å¼?"};
    ChooseFormat -- Markdown --> MarkdownFile["ğŸ“„ ç”ŸæˆMarkdownæ–‡ä»¶"];
    ChooseFormat -- PDF --> PDFFile["ğŸ“• ç”ŸæˆPDFæ–‡ä»¶"];
    MarkdownFile --> ValidateMarkdown["âœ“ éªŒè¯Markdown"];
    PDFFile --> ValidatePDF["âœ“ éªŒè¯PDF"];
    ValidateMarkdown -- å¤±è´¥ --> FixMarkdown["ğŸ”§ ä¿®å¤Markdown"];
    ValidatePDF -- å¤±è´¥ --> FixPDF["ğŸ”§ ä¿®å¤PDF"];
    ValidateMarkdown -- æˆåŠŸ --> Stage5_Input;
    ValidatePDF -- æˆåŠŸ --> Stage5_Input("è¿›å…¥ ğŸ·ï¸ 5: äº¤äº’é—®ç­”");
    FixMarkdown --> ValidateMarkdown;
    FixPDF --> ValidatePDF;
```

> _æ­¤å­é˜¶æ®µæ ¹æ®ç”¨æˆ·é€‰æ‹©çš„æ ¼å¼ç”Ÿæˆæœ€ç»ˆçš„è¾“å‡ºæ–‡ä»¶ï¼Œå¹¶éªŒè¯è¾“å‡ºè´¨é‡ã€‚_

#### `ğŸ·ï¸ 5`: å¤„ç†äº¤äº’å¼é—®ç­” (å¯é€‰)

```mermaid
flowchart TD
    Input["æ¥è‡ª ğŸ·ï¸ 4.2.2: å·²ç”Ÿæˆçš„æ•™ç¨‹æ–‡ä»¶"]
    Input2["æ¥è‡ª ğŸ·ï¸ 2: RAG æ•°æ®"]

    Input --> CheckQA{â“ æœ‰åˆå§‹æé—®æˆ–è¿›å…¥äº¤äº’æ¨¡å¼?};
    CheckQA -- æ˜¯ --> GetQuery(è·å–ç”¨æˆ·é—®é¢˜);
    CheckQA -- å¦ --> SkipQA(è·³è¿‡é—®ç­”);

    GetQuery --> QueryAnalysis["ğŸ” åˆ†æé—®é¢˜ç±»å‹/æ„å›¾"];
    QueryAnalysis & Input2 --> QAProcess["ğŸ¤ Agent/RAG+AI å›ç­”"];
    QAProcess --> AnswerQuality["âœ“ å›ç­”è´¨é‡æ£€æŸ¥"];
    AnswerQuality -- ä¸æ»¡è¶³ --> RefineAnswer["ğŸ”„ ç»†åŒ–å›ç­”"];
    AnswerQuality -- æ»¡è¶³ --> OutputAnswer["è¾“å‡ºå›ç­”ç»™ç”¨æˆ·"];
    RefineAnswer --> AnswerQuality;

    OutputAnswer --> CollectFeedback["ğŸ“Š æ”¶é›†ç”¨æˆ·åé¦ˆ"];
    CollectFeedback --> StoreFeedback["ğŸ’¾ å­˜å‚¨åé¦ˆç”¨äºæ”¹è¿›"];
    StoreFeedback --> MaybeMoreQA{è¿˜æœ‰é—®é¢˜?};
    MaybeMoreQA -- æ˜¯ --> GetQuery;
    MaybeMoreQA -- å¦ --> QADone(é—®ç­”ç»“æŸ);

    SkipQA --> Stage6_Start("è¿›å…¥ ğŸ·ï¸ 6: å‘å¸ƒ");
    QADone --> Stage6_Start;
```

> _åœ¨ç”Ÿæˆä¸»æ•™ç¨‹åï¼Œæ­¤å¯é€‰é˜¶æ®µå¤„ç†ç”¨æˆ·çš„äº¤äº’å¼æé—®ï¼ŒåŒ…å«é—®é¢˜åˆ†æã€è´¨é‡æ£€æŸ¥å’Œç”¨æˆ·åé¦ˆæ”¶é›†ã€‚_

#### `ğŸ·ï¸ 6`: å‘å¸ƒ (å¯é€‰)

```mermaid
flowchart TD
    Input["æ¥è‡ª ğŸ·ï¸ 5: é—®ç­”å¤„ç†å®Œæˆ"]
    Input2["æ¥è‡ª ğŸ·ï¸ 4.2.2: æœ€ç»ˆæ•™ç¨‹æ–‡ä»¶ (é€šå¸¸ä¸º Markdown)"]

    Input --> CheckPublish{"ğŸ·ï¸ 6 å‘å¸ƒ?"};
    CheckPublish -- æ˜¯ --> ValidateAuth["ğŸ”‘ éªŒè¯è®¤è¯ä¿¡æ¯"];
    ValidateAuth -- å¤±è´¥ --> RequestAuth["ğŸ”’ è¯·æ±‚è®¤è¯"];
    ValidateAuth -- æˆåŠŸ --> PrepareGitHubPages["ğŸ“˜ å‡†å¤‡GitHub Pagesé…ç½®"];
    PrepareGitHubPages --> Publish["ğŸš€ å‘å¸ƒ"];
    RequestAuth --> ValidateAuth;

    Publish --> VerifyPublish["âœ“ éªŒè¯å‘å¸ƒç»“æœ"];
    VerifyPublish -- å¤±è´¥ --> HandlePublishError["ğŸ”§ å¤„ç†å‘å¸ƒé”™è¯¯"];
    VerifyPublish -- æˆåŠŸ --> GenerateAccessInfo["ğŸ“‹ ç”Ÿæˆè®¿é—®ä¿¡æ¯"];
    HandlePublishError --> RetryPublish["ğŸ”„ é‡è¯•å‘å¸ƒ"];
    RetryPublish --> VerifyPublish;
    GenerateAccessInfo --> Done["ğŸ å®Œæˆ"];

    CheckPublish -- å¦ --> Done;
```

> _å¦‚æœç”¨æˆ·æŒ‡å®šï¼Œå°†ç»“æœå‘å¸ƒåˆ°å¹³å°ï¼Œç‰¹åˆ«æ”¯æŒ GitHub Pages é…ç½®ï¼ŒåŒ…å«è®¤è¯éªŒè¯ã€å‘å¸ƒéªŒè¯å’Œé”™è¯¯å¤„ç†ã€‚_

## ğŸ› ï¸ å·¥å…·å‡½æ•° (Utility Functions)

> AI æç¤º:
>
> 1. ä»”ç»†å›é¡¾æ–‡æ¡£ï¼Œç†è§£å·¥å…·å‡½æ•°çš„å®šä¹‰ã€‚
> 2. ä»…åŒ…å«æµç¨‹ä¸­èŠ‚ç‚¹æ‰€å¿…éœ€çš„å·¥å…·å‡½æ•°ï¼Œå¼ºè°ƒ `call_llm` çš„æ ¸å¿ƒåœ°ä½ã€‚

### æ ¸å¿ƒ AI å‡½æ•°

1.  **`call_llm(prompt, context=None, task_type=None, target_language='en', retry_count=3, config=None)`** (`utils/llm_wrapper.py`) - **æ ¸å¿ƒ ğŸ§ **
    - _è¾“å…¥_: ä¸»è¦æç¤º (str), ä¸Šä¸‹æ–‡ä¿¡æ¯ (ä»£ç ç‰‡æ®µ, ç»“æ„, å†å²ç­‰) (str), å¯é€‰çš„ä»»åŠ¡ç±»å‹æ ‡è¯† (str, å¦‚ 'summarize', 'explain_code', 'translate'), **ç›®æ ‡è¯­è¨€ (str, é»˜è®¤ 'en')**, é‡è¯•æ¬¡æ•° (int, é»˜è®¤ 3), é…ç½® (dict, é»˜è®¤ä»ç¯å¢ƒå˜é‡åŠ è½½)
    - _è¾“å‡º_: LLM ç”Ÿæˆçš„æ–‡æœ¬ (str), æˆåŠŸ/å¤±è´¥çŠ¶æ€ (bool), å…ƒæ•°æ® (dict, åŒ…å«æ¨¡å‹ä¿¡æ¯ã€å»¶è¿Ÿã€token ä½¿ç”¨ç­‰)
    - _å¿…è¦æ€§_: **é©±åŠ¨å‡ ä¹æ‰€æœ‰çš„å†…å®¹ç†è§£å’Œç”Ÿæˆä»»åŠ¡**ã€‚éœ€è¦åœ¨ prompt ä¸­ç»“åˆ target_language æŒ‡ç¤º AI è¾“å‡ºè¯­è¨€ï¼Œå¹¶å¼ºè°ƒä¸ç¿»è¯‘ä»£ç /æŠ€æœ¯æœ¯è¯­ã€‚
    - _é”™è¯¯å¤„ç†_: å®ç°æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶ï¼Œå¤„ç† API è¶…æ—¶ã€é™æµç­‰é—®é¢˜ã€‚è®°å½•è¯¦ç»†é”™è¯¯æ—¥å¿—ã€‚
    - _ç¼“å­˜æœºåˆ¶_: å¯¹ç›¸åŒæˆ–ç›¸ä¼¼çš„æç¤ºå®ç°æœ¬åœ°ç¼“å­˜ï¼Œé¿å…é‡å¤è°ƒç”¨ï¼Œæé«˜æ•ˆç‡å’Œé™ä½æˆæœ¬ã€‚
    - _æ¨èå®ç°_: **ä½¿ç”¨ `litellm` åº“** ç»Ÿä¸€è°ƒç”¨ä¸åŒçš„ LLM APIã€‚
    - _é»˜è®¤æ”¯æŒçš„ LLM æä¾›å•†_:
      - **OpenRouter**: æä¾›å¤šç§æ¨¡å‹è®¿é—®ï¼ŒåŒ…æ‹¬ Claudeã€Llama ç­‰
      - **é˜¿é‡Œç™¾ç‚¼ (Alibaba Tongyi)**: æ”¯æŒé€šä¹‰åƒé—®ç­‰æ¨¡å‹
      - **ç«å±±å¼•æ“ (Volcengine)**: æ”¯æŒç«å±±æ–¹èˆŸç­‰æ¨¡å‹
      - **ç¡…åŸºæµåŠ¨ (Moonshot AI)**: æ”¯æŒ Moonshot ç³»åˆ—æ¨¡å‹
      - ä»¥åŠ OpenAIã€Anthropic Claude ç­‰å¸¸è§æ¨¡å‹
    - _é…ç½®åŠ è½½_: é»˜è®¤ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®ï¼Œæ”¯æŒè¿è¡Œæ—¶è¦†ç›–ã€‚
    - _æ™ºèƒ½æ¨¡å‹é€‰æ‹©_: æ ¹æ®ä»»åŠ¡ç±»å‹å’Œå¤æ‚åº¦è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹ã€‚
    - _æ¨¡å‹å›é€€é“¾_: å®šä¹‰æ¨¡å‹å›é€€é¡ºåºï¼Œå½“é¦–é€‰æ¨¡å‹å¤±è´¥æ—¶è‡ªåŠ¨å°è¯•å¤‡é€‰æ¨¡å‹ã€‚

    ```python
    # å¢å¼ºçš„ LLM è°ƒç”¨å®ç°ç¤ºä¾‹
    def call_llm(prompt, context=None, task_type=None, target_language='en',
                retry_count=3, config=None):
        """å¢å¼ºçš„ LLM è°ƒç”¨å‡½æ•°ï¼Œæ”¯æŒæ™ºèƒ½æ¨¡å‹é€‰æ‹©å’Œå›é€€æœºåˆ¶"""
        # åŠ è½½é…ç½®
        llm_config = config or get_llm_config()

        # æ„å»ºå®Œæ•´æç¤º
        full_prompt = _build_prompt(prompt, context, task_type, target_language)

        # æ£€æŸ¥ç¼“å­˜
        cache_key = _generate_cache_key(full_prompt)
        cached_result = get_from_cache(cache_key)
        if cached_result:
            return cached_result["response"], True, {"from_cache": True, **cached_result["metadata"]}

        # æ™ºèƒ½æ¨¡å‹é€‰æ‹©
        selected_model, provider = _select_model_for_task(task_type, llm_config)

        # å®šä¹‰æ¨¡å‹å›é€€é“¾
        fallback_chain = _get_fallback_chain(selected_model, provider, task_type, llm_config)

        # å°è¯•ä¸»æ¨¡å‹å’Œå›é€€æ¨¡å‹
        for model_info in [{"model": selected_model, "provider": provider}] + fallback_chain:
            current_model = model_info["model"]
            current_provider = model_info["provider"]

            # å‡†å¤‡ API è°ƒç”¨å‚æ•°
            params = _prepare_api_params(current_model, current_provider, full_prompt, llm_config)

            # é‡è¯•æœºåˆ¶
            for attempt in range(retry_count):
                try:
                    # è®°å½•å¼€å§‹æ—¶é—´
                    start_time = time.time()

                    # è°ƒç”¨ API
                    response = completion(**params)

                    # è®¡ç®—å»¶è¿Ÿ
                    latency = time.time() - start_time

                    # æå–ç»“æœ
                    result = response.choices[0].message.content

                    # æ”¶é›†å…ƒæ•°æ®
                    metadata = {
                        "model": current_model,
                        "provider": current_provider,
                        "latency": latency,
                        "tokens": {
                            "prompt": response.usage.prompt_tokens,
                            "completion": response.usage.completion_tokens,
                            "total": response.usage.total_tokens
                        },
                        "attempt": attempt + 1,
                        "timestamp": datetime.now().isoformat()
                    }

                    # ç¼“å­˜ç»“æœ
                    save_to_cache(cache_key, {
                        "response": result,
                        "metadata": metadata
                    })

                    # è®°å½•æ¨¡å‹æ€§èƒ½
                    _record_model_performance(current_model, task_type, latency,
                                            response.usage.total_tokens, True)

                    return result, True, metadata

                except Exception as e:
                    # è®°å½•é”™è¯¯
                    log_and_notify(
                        f"æ¨¡å‹ {current_model} è°ƒç”¨å¤±è´¥ (å°è¯• {attempt+1}/{retry_count}): {str(e)}",
                        "warning"
                    )

                    # è®°å½•æ¨¡å‹æ€§èƒ½ (å¤±è´¥)
                    _record_model_performance(current_model, task_type,
                                            time.time() - start_time, 0, False)

                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
                    if attempt == retry_count - 1:
                        break

                    # æŒ‡æ•°é€€é¿
                    backoff_time = 2 ** attempt
                    time.sleep(backoff_time)

        # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥
        return f"æ‰€æœ‰æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•", False, {"error": "all_models_failed"}

    def _select_model_for_task(task_type, config):
        """æ ¹æ®ä»»åŠ¡ç±»å‹æ™ºèƒ½é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹"""
        # ä»»åŠ¡ç±»å‹åˆ°æ¨¡å‹èƒ½åŠ›çš„æ˜ å°„
        task_model_mapping = {
            # ä»£ç ç†è§£ä»»åŠ¡ä¼˜å…ˆä½¿ç”¨æ“…é•¿ä»£ç çš„æ¨¡å‹
            "understand_code": ["gpt-4", "claude-3-opus", "qwen-max", "glm-4", "moonshot-v1"],
            # å†…å®¹ç”Ÿæˆä»»åŠ¡å¯ä»¥ä½¿ç”¨æ›´å¤šæ ·çš„æ¨¡å‹
            "generate_content": ["claude-3-opus", "gpt-4", "qwen-max", "glm-4", "moonshot-v1"],
            # ç¿»è¯‘ä»»åŠ¡ä¼˜å…ˆä½¿ç”¨å¤šè¯­è¨€èƒ½åŠ›å¼ºçš„æ¨¡å‹
            "translate": ["qwen-max", "gpt-4", "claude-3-opus", "glm-4"],
            # é—®ç­”ä»»åŠ¡ä¼˜å…ˆä½¿ç”¨ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›å¼ºçš„æ¨¡å‹
            "answer_question": ["claude-3-opus", "gpt-4", "qwen-max", "glm-4"],
            # é»˜è®¤ä»»åŠ¡é…ç½®
            "default": ["gpt-4", "claude-3-opus", "qwen-max", "glm-4"]
        }

        # è·å–ä»»åŠ¡å¯¹åº”çš„æ¨¡å‹ä¼˜å…ˆçº§åˆ—è¡¨
        priority_models = task_model_mapping.get(task_type, task_model_mapping["default"])

        # è·å–é…ç½®ä¸­å¯ç”¨çš„æ¨¡å‹å’Œæä¾›å•†
        available_models = _get_available_models(config)

        # æŒ‰ä¼˜å…ˆçº§é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹
        for model_name in priority_models:
            for provider, models in available_models.items():
                if model_name in models:
                    return model_name, provider

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ¨¡å‹ï¼Œä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤æ¨¡å‹
        default_model = config.get("model", "gpt-4")
        default_provider = config.get("provider", "openai")

        return default_model, default_provider

    def _get_fallback_chain(primary_model, primary_provider, task_type, config):
        """è·å–æ¨¡å‹å›é€€é“¾"""
        # æ„å»ºå›é€€é“¾ï¼Œæ’é™¤ä¸»æ¨¡å‹
        fallback_chain = []

        # ä»»åŠ¡ç±»å‹åˆ°æ¨¡å‹èƒ½åŠ›çš„æ˜ å°„
        task_model_mapping = {
            "understand_code": ["gpt-4", "claude-3-opus", "qwen-max", "glm-4", "moonshot-v1"],
            "generate_content": ["claude-3-opus", "gpt-4", "qwen-max", "glm-4", "moonshot-v1"],
            "translate": ["qwen-max", "gpt-4", "claude-3-opus", "glm-4"],
            "answer_question": ["claude-3-opus", "gpt-4", "qwen-max", "glm-4"],
            "default": ["gpt-4", "claude-3-opus", "qwen-max", "glm-4"]
        }

        # è·å–ä»»åŠ¡å¯¹åº”çš„æ¨¡å‹ä¼˜å…ˆçº§åˆ—è¡¨
        priority_models = task_model_mapping.get(task_type, task_model_mapping["default"])

        # è·å–é…ç½®ä¸­å¯ç”¨çš„æ¨¡å‹å’Œæä¾›å•†
        available_models = _get_available_models(config)

        # æŒ‰ä¼˜å…ˆçº§æ„å»ºå›é€€é“¾ï¼Œæ’é™¤ä¸»æ¨¡å‹
        for model_name in priority_models:
            if model_name == primary_model:
                continue

            for provider, models in available_models.items():
                if model_name in models:
                    fallback_chain.append({"model": model_name, "provider": provider})

        # é™åˆ¶å›é€€é“¾é•¿åº¦ï¼Œé¿å…å°è¯•è¿‡å¤šæ¨¡å‹
        max_fallbacks = config.get("max_fallbacks", 2)
        return fallback_chain[:max_fallbacks]

    def _get_available_models(config):
        """è·å–é…ç½®ä¸­å¯ç”¨çš„æ¨¡å‹å’Œæä¾›å•†"""
        available_models = {}

        # æ£€æŸ¥ OpenAI é…ç½®
        if config.get("openai_api_key"):
            available_models["openai"] = ["gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"]

        # æ£€æŸ¥ Anthropic é…ç½®
        if config.get("anthropic_api_key"):
            available_models["anthropic"] = ["claude-3-opus", "claude-3-sonnet", "claude-3-haiku"]

        # æ£€æŸ¥é˜¿é‡Œç™¾ç‚¼é…ç½®
        if config.get("alibaba_api_key"):
            available_models["alibaba"] = ["qwen-max", "qwen-plus", "qwen-turbo"]

        # æ£€æŸ¥æ™ºè°±é…ç½®
        if config.get("zhipu_api_key"):
            available_models["zhipu"] = ["glm-4", "glm-3-turbo"]

        # æ£€æŸ¥ Moonshot é…ç½®
        if config.get("moonshot_api_key"):
            available_models["moonshot"] = ["moonshot-v1"]

        # æ£€æŸ¥ OpenRouter é…ç½® (å¯ä»¥è®¿é—®å¤šç§æ¨¡å‹)
        if config.get("openrouter_api_key"):
            available_models["openrouter"] = [
                "gpt-4", "claude-3-opus", "claude-3-sonnet",
                "mistral-large", "llama-3-70b"
            ]

        return available_models

    def _record_model_performance(model, task_type, latency, tokens, success):
        """è®°å½•æ¨¡å‹æ€§èƒ½ï¼Œç”¨äºä¼˜åŒ–æœªæ¥çš„æ¨¡å‹é€‰æ‹©"""
        # è·å–æ€§èƒ½è®°å½•æ–‡ä»¶è·¯å¾„
        performance_file = "data/model_performance.json"

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(performance_file), exist_ok=True)

        # è¯»å–ç°æœ‰è®°å½•
        performance_data = {}
        if os.path.exists(performance_file):
            try:
                with open(performance_file, "r") as f:
                    performance_data = json.load(f)
            except:
                pass

        # åˆå§‹åŒ–æ¨¡å‹è®°å½•
        if model not in performance_data:
            performance_data[model] = {}

        # åˆå§‹åŒ–ä»»åŠ¡ç±»å‹è®°å½•
        if task_type not in performance_data[model]:
            performance_data[model][task_type] = {
                "calls": 0,
                "success": 0,
                "failures": 0,
                "total_latency": 0,
                "total_tokens": 0,
                "avg_latency": 0,
                "avg_tokens": 0,
                "success_rate": 0
            }

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        stats = performance_data[model][task_type]
        stats["calls"] += 1

        if success:
            stats["success"] += 1
            stats["total_latency"] += latency
            stats["total_tokens"] += tokens
        else:
            stats["failures"] += 1

        # è®¡ç®—å¹³å‡å€¼
        if stats["success"] > 0:
            stats["avg_latency"] = stats["total_latency"] / stats["success"]
            stats["avg_tokens"] = stats["total_tokens"] / stats["success"]

        # è®¡ç®—æˆåŠŸç‡
        stats["success_rate"] = stats["success"] / stats["calls"]

        # ä¿å­˜æ›´æ–°åçš„è®°å½•
        with open(performance_file, "w") as f:
            json.dump(performance_data, f, indent=2)
    ```

    ```python
    # å®ç°ç¤ºä¾‹
    from litellm import completion
    import time
    import hashlib
    import json
    import os
    from .env_manager import get_llm_config
    from .cache_manager import get_from_cache, save_to_cache

    def call_llm(prompt, context=None, task_type=None, target_language='en',
                retry_count=3, config=None):
        """è°ƒç”¨ LLM API ç”Ÿæˆæ–‡æœ¬"""
        # åŠ è½½é…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®ï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½
        llm_config = config or get_llm_config()

        # æ„å»ºå®Œæ•´æç¤º
        full_prompt = _build_prompt(prompt, context, task_type, target_language)

        # æ£€æŸ¥ç¼“å­˜
        cache_key = _generate_cache_key(full_prompt)
        cached_result = get_from_cache(cache_key)
        if cached_result:
            return cached_result, True

        # è·å– LLM æä¾›å•†
        provider = llm_config.get("provider", "openai").lower()

        # å‡†å¤‡ API è°ƒç”¨å‚æ•°
        params = {
            "model": llm_config["model"],
            "messages": [{"role": "user", "content": full_prompt}],
            "max_tokens": llm_config["max_tokens"],
            "temperature": 0.2 if task_type in ["explain_code", "summarize"] else 0.7,
            "api_key": llm_config["api_key"]
        }

        # æ ¹æ®ä¸åŒæä¾›å•†é…ç½®ç‰¹å®šå‚æ•°
        if provider == "openrouter":
            params["api_base"] = llm_config.get("base_url", "https://openrouter.ai/api/v1")
            # OpenRouter éœ€è¦æ·»åŠ  HTTP å¤´ä»¥è¯†åˆ«åº”ç”¨
            params["headers"] = {
                "HTTP-Referer": llm_config.get("app_url", "https://your-app-url.com"),
                "X-Title": llm_config.get("app_name", "Code Tutorial Generator")
            }
        elif provider == "alibaba" or provider == "tongyi":
            params["api_base"] = llm_config.get("base_url", "https://dashscope.aliyuncs.com/api/v1")
            # é˜¿é‡Œç™¾ç‚¼æ¨¡å‹æ˜ å°„
            if "qwen" in params["model"].lower() and not params["model"].startswith("qwen-"):
                params["model"] = f"qwen-{params['model']}"
        elif provider == "volcengine":
            params["api_base"] = llm_config.get("base_url", "https://api.volcengine.com/ml/api/v1/services")
            # ç«å±±å¼•æ“éœ€è¦é¢å¤–çš„è®¤è¯å‚æ•°
            params["extra_body"] = {
                "service_id": llm_config.get("service_id", "")
            }
        elif provider == "moonshot":
            params["api_base"] = llm_config.get("base_url", "https://api.moonshot.cn/v1")
        else:
            # å…¶ä»–æä¾›å•†ï¼ˆå¦‚ OpenAIã€Anthropic ç­‰ï¼‰
            if llm_config.get("base_url"):
                params["api_base"] = llm_config["base_url"]

        # é‡è¯•æœºåˆ¶
        for attempt in range(retry_count):
            try:
                response = completion(**params)
                result = response.choices[0].message.content

                # ç¼“å­˜ç»“æœ
                save_to_cache(cache_key, result)

                return result, True
            except Exception as e:
                # è®°å½•é”™è¯¯
                log_and_notify(f"LLM API è°ƒç”¨å¤±è´¥ (å°è¯• {attempt+1}/{retry_count}): {str(e)}",
                              "error")

                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè¿”å›é”™è¯¯
                if attempt == retry_count - 1:
                    return f"LLM API è°ƒç”¨å¤±è´¥: {str(e)}", False

                # æŒ‡æ•°é€€é¿
                time.sleep(2 ** attempt)

    def get_llm_config():
        """ä»ç¯å¢ƒå˜é‡åŠ è½½ LLM é…ç½®"""
        # é»˜è®¤é…ç½®
        default_config = {
            "provider": "openai",
            "model": "gpt-3.5-turbo",
            "max_tokens": 4000,
            "temperature": 0.7
        }

        # ä»ç¯å¢ƒå˜é‡åŠ è½½
        config = {
            "provider": os.getenv("LLM_PROVIDER", default_config["provider"]),
            "model": os.getenv("LLM_MODEL", default_config["model"]),
            "max_tokens": int(os.getenv("LLM_MAX_TOKENS", default_config["max_tokens"])),
            "temperature": float(os.getenv("LLM_TEMPERATURE", default_config["temperature"])),
            "api_key": os.getenv("LLM_API_KEY", "")
        }

        # åŠ è½½æä¾›å•†ç‰¹å®šé…ç½®
        if config["provider"] == "openrouter":
            config["base_url"] = os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
            config["app_url"] = os.getenv("APP_URL", "")
            config["app_name"] = os.getenv("APP_NAME", "Code Tutorial Generator")
        elif config["provider"] in ["alibaba", "tongyi"]:
            config["base_url"] = os.getenv("ALIBABA_BASE_URL", "https://dashscope.aliyuncs.com/api/v1")
        elif config["provider"] == "volcengine":
            config["base_url"] = os.getenv("VOLCENGINE_BASE_URL", "https://api.volcengine.com/ml/api/v1/services")
            config["service_id"] = os.getenv("VOLCENGINE_SERVICE_ID", "")
        elif config["provider"] == "moonshot":
            config["base_url"] = os.getenv("MOONSHOT_BASE_URL", "https://api.moonshot.cn/v1")
        else:
            # OpenAI æˆ–å…¶ä»–æä¾›å•†
            config["base_url"] = os.getenv("LLM_BASE_URL", "")

        return config
    ```

2.  **`evaluate_llm_output(output, task_type, criteria=None)`** (`utils/llm_evaluator.py`) - **è´¨é‡ä¿è¯**
    - _è¾“å…¥_: LLM è¾“å‡º (str), ä»»åŠ¡ç±»å‹ (str), å¯é€‰çš„è¯„ä¼°æ ‡å‡† (dict)
    - _è¾“å‡º_: è´¨é‡è¯„åˆ† (float), é—®é¢˜æ ‡è®° (list), æ”¹è¿›å»ºè®® (str)
    - _å¿…è¦æ€§_: è¯„ä¼° LLM ç”Ÿæˆå†…å®¹çš„è´¨é‡ï¼Œç¡®ä¿æ»¡è¶³é¢„æœŸæ ‡å‡†ã€‚
    - _å®ç°å»ºè®®_: å¯ä½¿ç”¨è§„åˆ™åŸºç¡€æ£€æŸ¥æˆ–å¦ä¸€ä¸ª LLM è°ƒç”¨è¿›è¡Œè¯„ä¼°ã€‚

### ä»£ç åˆ†æå‡½æ•°

3.  **`parse_code(code_path, language=None, max_file_size=10*1024*1024)`** (`utils/code_parser.py`) - **AI è¾“å…¥æä¾›è€…**
    - _è¾“å…¥_: ä»£ç æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„ (str), ç¼–ç¨‹è¯­è¨€ (str, å¯é€‰è‡ªåŠ¨æ£€æµ‹), æœ€å¤§æ–‡ä»¶å¤§å°é™åˆ¶ (int)
    - _è¾“å‡º_: ä»£ç åŸºç¡€ç»“æ„ (AST, å‡½æ•°/ç±»ç­¾å, åŸå§‹æ³¨é‡Š) (dict/object), ä¾èµ–å…³ç³» (dict)
    - _å¿…è¦æ€§_: ä¸º `call_llm` æä¾›å‡†ç¡®ã€ç»“æ„åŒ–çš„ä»£ç ä¸Šä¸‹æ–‡ã€‚
    - _é”™è¯¯å¤„ç†_: å¤„ç†ä¸æ”¯æŒçš„è¯­è¨€ã€è¿‡å¤§æ–‡ä»¶ã€è§£æé”™è¯¯ç­‰æƒ…å†µï¼Œæä¾›é™çº§è§£æé€‰é¡¹ã€‚
    - _è¯­è¨€æ”¯æŒ_: å®ç°å¯¹å¤šç§å¸¸è§ç¼–ç¨‹è¯­è¨€çš„æ”¯æŒï¼ŒåŒ…æ‹¬æ··åˆè¯­è¨€é¡¹ç›®çš„å¤„ç†ç­–ç•¥ã€‚

4.  **`detect_programming_language(file_path)`** (`utils/code_parser.py`)
    - _è¾“å…¥_: æ–‡ä»¶è·¯å¾„ (str)
    - _è¾“å‡º_: æ£€æµ‹åˆ°çš„ç¼–ç¨‹è¯­è¨€ (str), ç½®ä¿¡åº¦ (float)
    - _å¿…è¦æ€§_: è‡ªåŠ¨è¯†åˆ«ä»£ç æ–‡ä»¶çš„ç¼–ç¨‹è¯­è¨€ï¼Œæ”¯æŒæ··åˆè¯­è¨€é¡¹ç›®ã€‚
    - _å®ç°å»ºè®®_: ç»“åˆæ–‡ä»¶æ‰©å±•åã€shebang è¡Œå’Œå†…å®¹ç‰¹å¾è¿›è¡Œæ£€æµ‹ã€‚

5.  **`analyze_code_size(repo_path)`** (`utils/code_parser.py`)
    - _è¾“å…¥_: ä»£ç åº“è·¯å¾„ (str)
    - _è¾“å‡º_: ä»£ç åº“å¤§å°ç»Ÿè®¡ (dict)ï¼ŒåŒ…å«æ€»å¤§å°ã€æ–‡ä»¶æ•°ã€å„è¯­è¨€ä»£ç è¡Œæ•°ç­‰
    - _å¿…è¦æ€§_: è¯„ä¼°ä»£ç åº“è§„æ¨¡ï¼Œå†³å®šæ˜¯å¦éœ€è¦åˆ†å‰²å¤„ç†ã€‚
    - _å®ç°å»ºè®®_: ä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿæ“ä½œå’Œç®€å•çš„ä»£ç è¡Œè®¡æ•°ã€‚

### Git ç›¸å…³å‡½æ•°

6.  **`get_commit_history(repo_path, max_commits=1000, filter_criteria=None)`** (`utils/git_utils.py`) - **AI è¾“å…¥æä¾›è€…**
    - _è¾“å…¥_: æœ¬åœ°ä»“åº“è·¯å¾„ (str), æœ€å¤§æäº¤æ•° (int), è¿‡æ»¤æ¡ä»¶ (dict)
    - _è¾“å‡º_: Commit å†å²åˆ—è¡¨ (list of dicts)
    - _å¿…è¦æ€§_: ä¸º `call_llm` æä¾›ä»£ç æ¼”å˜å†å²ä¸Šä¸‹æ–‡ã€‚
    - _é”™è¯¯å¤„ç†_: å¤„ç† Git æ“ä½œå¤±è´¥ã€ç©ºä»“åº“ç­‰æƒ…å†µã€‚
    - _ä¼˜åŒ–_: å®ç°æ™ºèƒ½è¿‡æ»¤ï¼Œåªæå–å…³é”®çš„æ¶æ„å˜æ›´æäº¤ã€‚

7.  **`git_clone(repo_url, local_path, depth=None, branch=None, auth=None, use_cache=True, cache_ttl=86400)`** (`utils/git_utils.py`)
    - _è¾“å…¥_: ä»“åº“ URL (str), æœ¬åœ°ç›®æ ‡è·¯å¾„ (str), å…‹éš†æ·±åº¦ (int), åˆ†æ”¯ (str), è®¤è¯ä¿¡æ¯ (dict), æ˜¯å¦ä½¿ç”¨ç¼“å­˜ (bool), ç¼“å­˜æœ‰æ•ˆæœŸ (int, ç§’)
    - _è¾“å‡º_: å…‹éš†æ˜¯å¦æˆåŠŸ (bool), è¯¦ç»†ä¿¡æ¯ (dict), æ˜¯å¦ä½¿ç”¨äº†ç¼“å­˜ (bool)
    - _å¿…è¦æ€§_: ä»è¿œç¨‹ URL è·å–ä»£ç åº“ã€‚
    - _é”™è¯¯å¤„ç†_: å¤„ç†ç½‘ç»œé—®é¢˜ã€è®¤è¯å¤±è´¥ã€æƒé™é—®é¢˜ç­‰ã€‚
    - _å®‰å…¨æ€§_: å®‰å…¨å¤„ç†è®¤è¯ä¿¡æ¯ï¼Œæ”¯æŒå¤šç§è®¤è¯æ–¹å¼ã€‚
    - _ç¼“å­˜æœºåˆ¶_: é»˜è®¤å¯ç”¨24å°æ—¶ç¼“å­˜ï¼Œé¿å…é‡å¤ä¸‹è½½ç›¸åŒä»“åº“ã€‚

    ```python
    # å®ç°ç¤ºä¾‹
    import os
    import time
    import hashlib
    import shutil
    import git
    from pathlib import Path

    def git_clone(repo_url, local_path, depth=None, branch=None, auth=None,
                 use_cache=True, cache_ttl=None):
        """å…‹éš†æˆ–ä½¿ç”¨ç¼“å­˜çš„Gitä»“åº“"""
        # ä»ç¯å¢ƒå˜é‡è·å–ç¼“å­˜æœ‰æ•ˆæœŸï¼Œé»˜è®¤ä¸º24å°æ—¶ï¼ˆ86400ç§’ï¼‰
        if cache_ttl is None:
            cache_ttl = int(os.getenv("REPO_CACHE_TTL", "86400"))
        # ç”Ÿæˆä»“åº“å”¯ä¸€æ ‡è¯†
        repo_hash = hashlib.md5(repo_url.encode()).hexdigest()

        # ä»ç¯å¢ƒå˜é‡è·å–ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä¸º ~/.repo_cache
        cache_dir = os.getenv("REPO_CACHE_DIR", os.path.join(os.path.expanduser("~"), ".repo_cache"))
        os.makedirs(cache_dir, exist_ok=True)

        # ç¼“å­˜çš„ä»“åº“è·¯å¾„
        cached_repo_path = os.path.join(cache_dir, repo_hash)
        cache_meta_path = f"{cached_repo_path}.meta"

        # æ£€æŸ¥ç¼“å­˜
        if use_cache and os.path.exists(cached_repo_path) and os.path.exists(cache_meta_path):
            # è¯»å–ç¼“å­˜å…ƒæ•°æ®
            with open(cache_meta_path, "r") as f:
                meta = json.load(f)

            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
            if time.time() - meta["timestamp"] < cache_ttl:
                # ç¼“å­˜æœ‰æ•ˆï¼Œå¤åˆ¶åˆ°ç›®æ ‡è·¯å¾„
                if os.path.exists(local_path):
                    shutil.rmtree(local_path)

                shutil.copytree(cached_repo_path, local_path)

                return {
                    "success": True,
                    "path": local_path,
                    "from_cache": True,
                    "branch": meta.get("branch", "main")
                }

        # ç¼“å­˜æ— æ•ˆæˆ–ä¸ä½¿ç”¨ç¼“å­˜ï¼Œæ‰§è¡Œå…‹éš†
        try:
            # å‡†å¤‡å…‹éš†å‚æ•°
            clone_args = {
                "to_path": local_path,
                "multi_options": []
            }

            # æ·»åŠ æ·±åº¦å‚æ•°
            if depth:
                clone_args["multi_options"].append(f"--depth={depth}")

            # æ·»åŠ åˆ†æ”¯å‚æ•°
            if branch:
                clone_args["branch"] = branch

            # å¤„ç†è®¤è¯ä¿¡æ¯
            if auth:
                if "token" in auth:
                    # ä½¿ç”¨ä»¤ç‰Œè®¤è¯
                    auth_url = repo_url.replace("https://", f"https://{auth['token']}@")
                    clone_args["url"] = auth_url
                elif "username" in auth and "password" in auth:
                    # ä½¿ç”¨ç”¨æˆ·åå¯†ç è®¤è¯
                    auth_url = repo_url.replace(
                        "https://",
                        f"https://{auth['username']}:{auth['password']}@"
                    )
                    clone_args["url"] = auth_url
            else:
                clone_args["url"] = repo_url

            # æ‰§è¡Œå…‹éš†
            repo = git.Repo.clone_from(**clone_args)
            actual_branch = repo.active_branch.name

            # æ›´æ–°ç¼“å­˜
            if use_cache:
                # å¦‚æœç¼“å­˜ç›®å½•å­˜åœ¨ï¼Œå…ˆåˆ é™¤
                if os.path.exists(cached_repo_path):
                    shutil.rmtree(cached_repo_path)

                # å¤åˆ¶åˆ°ç¼“å­˜ç›®å½•
                shutil.copytree(local_path, cached_repo_path)

                # ä¿å­˜å…ƒæ•°æ®
                meta = {
                    "url": repo_url,
                    "timestamp": time.time(),
                    "branch": actual_branch
                }

                with open(cache_meta_path, "w") as f:
                    json.dump(meta, f)

            return {
                "success": True,
                "path": local_path,
                "from_cache": False,
                "branch": actual_branch
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "from_cache": False
            }
    ```

### å¯è§†åŒ–å‡½æ•°

8.  **`generate_mermaid(data, type, theme=None, config=None)`** (`utils/viz_generator.py`)
    - _è¾“å…¥_: ç»“æ„åŒ–æ•°æ® (dict/list), å›¾è¡¨ç±»å‹ ('flowchart', 'graph' ç­‰) (str), ä¸»é¢˜ (str), é…ç½® (dict)
    - _è¾“å‡º_: Mermaid è¯­æ³•çš„å­—ç¬¦ä¸² (str)
    - _å¿…è¦æ€§_: ç”Ÿæˆæ¶æ„å›¾ã€ä¾èµ–å…³ç³»å›¾ç­‰å¯è§†åŒ–å†…å®¹ã€‚
    - _é”™è¯¯å¤„ç†_: å¤„ç†å¤æ‚æˆ–æ— æ•ˆçš„è¾“å…¥æ•°æ®ã€‚
    - _å¢å¼º_: æ”¯æŒå¤šç§å›¾è¡¨ç±»å‹å’Œè‡ªå®šä¹‰æ ·å¼ã€‚

### RAG ç›¸å…³å‡½æ•°

9.  **`chunk_text(text, chunk_size=1000, overlap=200, smart_chunking=True)`** (`utils/rag_utils.py`)
    - _è¾“å…¥_: æ–‡æœ¬ (str), å—å¤§å° (int), é‡å å¤§å° (int), æ™ºèƒ½åˆ†å—æ ‡å¿— (bool)
    - _è¾“å‡º_: æ–‡æœ¬å—åˆ—è¡¨ (list of str)
    - _å¿…è¦æ€§_: å°†ä»£ç å’Œæ–‡æ¡£åˆ†å‰²æˆé€‚åˆåµŒå…¥å’Œæ£€ç´¢çš„å—ã€‚
    - _æ™ºèƒ½åˆ†å—_: å¦‚æœå¯ç”¨ï¼Œå°Šé‡ä»£ç å’Œæ–‡æ¡£çš„è‡ªç„¶è¾¹ç•Œï¼ˆå¦‚å‡½æ•°ã€ç±»ã€æ®µè½ï¼‰ã€‚

10. **`get_embedding(text, model='default', batch=False)`** (`utils/embedding.py`)
    - _è¾“å…¥_: æ–‡æœ¬ (str æˆ– list of str), æ¨¡å‹åç§° (str), æ‰¹å¤„ç†æ ‡å¿— (bool)
    - _è¾“å‡º_: æ–‡æœ¬çš„å‘é‡è¡¨ç¤º (list of float æˆ– list of list of float)
    - _å¿…è¦æ€§_: ç”¨äº RAG ä¸­çš„æ–‡æœ¬åµŒå…¥ï¼Œä»¥ä¾¿è¿›è¡Œç›¸ä¼¼åº¦æœç´¢ã€‚
    - _é”™è¯¯å¤„ç†_: å¤„ç† API é”™è¯¯ã€è¶…é•¿æ–‡æœ¬ç­‰æƒ…å†µã€‚
    - _æ‰¹å¤„ç†_: æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æœ¬ï¼Œæé«˜æ•ˆç‡ã€‚

11. **`vector_search(query_embedding, index, top_k=5, similarity_threshold=0.7)`** (`utils/vector_db.py`)
    - _è¾“å…¥_: æŸ¥è¯¢å‘é‡ (list of float), å‘é‡ç´¢å¼• (object), è¿”å›æ•°é‡ (int), ç›¸ä¼¼åº¦é˜ˆå€¼ (float)
    - _è¾“å‡º_: æœ€ç›¸ä¼¼çš„æ–‡æ¡£ç‰‡æ®µ ID å’Œç›¸ä¼¼åº¦ (list of tuples)
    - _å¿…è¦æ€§_: ç”¨äº RAG ä¸­æ ¹æ®ç”¨æˆ·é—®é¢˜æ£€ç´¢ç›¸å…³ä»£ç æˆ–æ–‡æ¡£ç‰‡æ®µã€‚
    - _è¿‡æ»¤_: æ ¹æ®ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ç»“æœï¼Œç¡®ä¿åªè¿”å›ç›¸å…³å†…å®¹ã€‚

12. **`create_vector_index(embeddings, metadata=None, index_type='flat')`** (`utils/vector_db.py`)
    - _è¾“å…¥_: åµŒå…¥å‘é‡åˆ—è¡¨ (list of list of float), å…ƒæ•°æ® (list of dict), ç´¢å¼•ç±»å‹ (str)
    - _è¾“å‡º_: å‘é‡ç´¢å¼•å¯¹è±¡ (object)
    - _å¿…è¦æ€§_: æ„å»º RAG æ‰€éœ€çš„å‘é‡æ•°æ®åº“ç´¢å¼•ã€‚
    - _ç´¢å¼•ç±»å‹_: æ”¯æŒå¤šç§ç´¢å¼•ç±»å‹ï¼Œå¦‚å¹³é¢ç´¢å¼•ã€HNSW ç­‰ï¼Œæ ¹æ®æ•°æ®è§„æ¨¡é€‰æ‹©åˆé€‚çš„ç±»å‹ã€‚
    - _æŒä¹…åŒ–_: æ”¯æŒç´¢å¼•çš„ä¿å­˜å’ŒåŠ è½½ï¼Œé¿å…é‡å¤æ„å»ºã€‚

### æ ¼å¼åŒ–ä¸å‘å¸ƒå‡½æ•°

13. **`split_content_into_files(content_dict, output_dir, file_structure=None, repo_structure=None, justdoc_compatible=True)`** (`utils/formatter.py`)
    - _è¾“å…¥_: åŒ…å«æ•™ç¨‹å„éƒ¨åˆ†å†…å®¹çš„å­—å…¸ (dict), è¾“å‡ºç›®å½• (str), æ–‡ä»¶ç»“æ„é…ç½® (dict), ä»£ç ä»“åº“ç»“æ„ (dict), æ˜¯å¦ç”Ÿæˆ JustDoc å…¼å®¹æ–‡æ¡£ (bool)
    - _è¾“å‡º_: ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ (list of str)
    - _å¿…è¦æ€§_: å°†ç”Ÿæˆçš„å†…å®¹æ‹†åˆ†ä¸ºå¤šä¸ª Markdown æ–‡ä»¶ï¼Œä¾¿äºå¯¼èˆªå’Œé˜…è¯»ã€‚
    - _æ–‡ä»¶ç»“æ„_: é‡‡ç”¨æ¦‚è§ˆ-æ¨¡å—æ–¹å¼ç»„ç»‡æ–‡ä»¶ï¼Œæ–‡ä»¶æ”¾ç½®åœ¨ä¸ä»£ç ä»“åº“ç»“æ„å¯¹åº”çš„ç›®å½•ä¸­ã€‚
    - _JustDoc å‘½åçº¦å®š_:
      - ä½¿ç”¨ `index.md` ä½œä¸ºç›®å½•ç´¢å¼•æ–‡ä»¶
      - ç›®å½•å’Œæ–‡ä»¶åä½¿ç”¨å°å†™å­—æ¯
      - å¤šå•è¯åç§°ä½¿ç”¨è¿å­—ç¬¦ï¼ˆ-ï¼‰åˆ†éš”ï¼Œè€Œéä¸‹åˆ’çº¿
      - æ¯ä¸ªæ–‡ä»¶åŒ…å« JustDoc å…¼å®¹çš„å…ƒæ•°æ®ï¼ˆtitle, category, order ç­‰ï¼‰
    - _æ–‡æ¡£æ”¾ç½®_: ç”Ÿæˆçš„æ–‡æ¡£ç»Ÿä¸€æ”¾ç½®åˆ°ä»£ç ä»“åº“å¯¹åº”çš„ç›®å½•ç»“æ„ä¸­ï¼Œä¸æºä»£ç ä¿æŒä¸€è‡´ï¼Œä¾¿äºæŸ¥æ‰¾å’Œé€šè¿‡ JustDoc è¾“å‡ºçº¿ä¸Šæ–‡æ¡£ã€‚
    - _è·¯å¾„æ˜ å°„å®ç°_:
      ```python
      def map_module_to_docs_path(module_name, repo_structure):
          """å°†æ¨¡å—åæ˜ å°„åˆ°æ–‡æ¡£è·¯å¾„ï¼Œç¬¦åˆ JustDoc å‘½åçº¦å®š"""
          # æŸ¥æ‰¾æ¨¡å—åœ¨ä»£ç ä»“åº“ä¸­çš„ä½ç½®
          module_path = repo_structure.get(module_name, {}).get("path")

          if not module_path:
              # å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”è·¯å¾„ï¼Œæ”¾åœ¨æ ¹ç›®å½•
              # å°†ä¸‹åˆ’çº¿è½¬æ¢ä¸ºè¿å­—ç¬¦ï¼Œç¬¦åˆ JustDoc å‘½åçº¦å®š
              justdoc_name = module_name.replace("_", "-").lower()
              return f"docs/{justdoc_name}.md"

          # å°†æºä»£ç è·¯å¾„è½¬æ¢ä¸ºæ–‡æ¡£è·¯å¾„
          # ä¾‹å¦‚: src/auth/service.py -> docs/auth/service.md
          # ä¾‹å¦‚: src/data_processor/main.py -> docs/data-processor/main.md
          parts = os.path.normpath(module_path).split(os.sep)

          # ç§»é™¤ src/ å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
          if parts and parts[0] == "src":
              parts = parts[1:]

          # å¤„ç†ç›®å½•åå’Œæ–‡ä»¶åï¼Œå°†ä¸‹åˆ’çº¿è½¬æ¢ä¸ºè¿å­—ç¬¦
          justdoc_parts = []
          for i, part in enumerate(parts):
              # æœ€åä¸€éƒ¨åˆ†æ˜¯æ–‡ä»¶åï¼Œç§»é™¤æ‰©å±•å
              if i == len(parts) - 1 and "." in part:
                  part = os.path.splitext(part)[0]

              # å°†ä¸‹åˆ’çº¿è½¬æ¢ä¸ºè¿å­—ç¬¦
              justdoc_part = part.replace("_", "-").lower()
              justdoc_parts.append(justdoc_part)

          # åˆ›å»ºæ–‡æ¡£ç›®å½•ç»“æ„
          doc_path = os.path.join("docs", *justdoc_parts[:-1])

          # è¿”å›å®Œæ•´çš„æ–‡æ¡£è·¯å¾„
          return os.path.join(doc_path, f"{justdoc_parts[-1]}.md")

      def create_index_files(module_dirs, module_info_dict=None, repo_structure=None):
          """ä¸ºæ¯ä¸ªæ¨¡å—ç›®å½•åˆ›å»ºæ›´åŠ ç»„ç»‡åŒ–çš„ index.md æ–‡ä»¶

          Args:
              module_dirs: æ¨¡å—ç›®å½•åˆ—è¡¨
              module_info_dict: æ¨¡å—ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«æ¨¡å—æè¿°å’ŒåŠŸèƒ½
              repo_structure: ä»£ç ä»“åº“ç»“æ„ä¿¡æ¯

          Returns:
              ç´¢å¼•æ–‡ä»¶åˆ—è¡¨ [(æ–‡ä»¶è·¯å¾„, æ–‡ä»¶å†…å®¹), ...]
          """
          index_files = []

          for dir_path in module_dirs:
              # åˆ›å»ºç›®å½•çš„ç´¢å¼•æ–‡ä»¶
              index_path = os.path.join(dir_path, "index.md")

              # è·å–ç›®å½•åä½œä¸ºæ ‡é¢˜
              dir_name = os.path.basename(dir_path)
              title = dir_name.replace("-", " ").title()

              # è·å–è¯¥ç›®å½•ä¸‹çš„æ‰€æœ‰æ¨¡å—
              dir_modules = []
              if module_info_dict and repo_structure:
                  for module_name, info in module_info_dict.items():
                      module_path = repo_structure.get(module_name, {}).get("path", "")
                      if module_path and dir_name in module_path.split(os.sep):
                          dir_modules.append((module_name, info))

              # åˆ›å»ºç´¢å¼•æ–‡ä»¶å†…å®¹ - æ›´åŠ æµç•…çš„ç»„ç»‡æ–¹å¼ï¼Œæ·»åŠ  emoji çªå‡ºé‡ç‚¹
              content = f"# ğŸ“š {title} æ¨¡å—\n\n## ğŸ“‹ æ¦‚è¿°\n\n"
              content += f"æœ¬ç›®å½•åŒ…å« {title} æ¨¡å—çš„ç›¸å…³æ–‡æ¡£ï¼Œè¿™äº›æ¨¡å—å…±åŒå®ç°äº†ç³»ç»Ÿçš„{title.lower()}åŠŸèƒ½ã€‚\n\n"

              # æ·»åŠ æ¨¡å—åˆ—è¡¨ï¼Œå¸¦æœ‰ç®€çŸ­æè¿°
              if dir_modules:
                  content += "## ğŸ“¦ æ¨¡å—åˆ—è¡¨\n\n"

                  for module_name, info in dir_modules:
                      module_file = module_name.replace("_", "-").lower()
                      module_title = module_name.replace("_", " ").title()
                      description = info.get("short_description", "").split(".")[0]  # åªå–ç¬¬ä¸€å¥

                      # åˆ›å»ºå¸¦æœ‰ç®€çŸ­æè¿°çš„é“¾æ¥ï¼Œæ·»åŠ  emoji çªå‡ºé‡ç‚¹
                      content += f"- ğŸ”¹ [{module_title}]({module_file}.md) - {description}\n"

                  # æ·»åŠ åŠŸèƒ½æ¦‚è§ˆéƒ¨åˆ†
                  content += "\n## âš™ï¸ åŠŸèƒ½æ¦‚è§ˆ\n\n"
                  content += f"{title} æ¨¡å—æä¾›ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ï¼š\n\n"

                  # æå–å¹¶ç»„ç»‡ä¸»è¦åŠŸèƒ½ç‚¹
                  functions = set()
                  for _, info in dir_modules:
                      for function in info.get("functions", []):
                          functions.add(function)

                  for function in sorted(functions):
                      content += f"- âœ… {function}\n"
              else:
                  # å¦‚æœæ²¡æœ‰æ¨¡å—ä¿¡æ¯ï¼Œä½¿ç”¨ç®€å•çš„å†…å®¹åˆ—è¡¨
                  content += "## ğŸ“‘ å†…å®¹\n\n"
                  content += "æœ¬ç›®å½•åŒ…å«ä»¥ä¸‹æ–‡æ¡£ï¼š\n\n"

              # æ·»åŠ åˆ°ç´¢å¼•æ–‡ä»¶åˆ—è¡¨
              index_files.append((index_path, content))

          return index_files
      ```
    - _æ–‡ä»¶ç»“æ„ç¤ºä¾‹_:
      ```python
      # æ–‡ä»¶ç»“æ„ç¤ºä¾‹ - ç¬¦åˆ JustDoc çš„å‘½åçº¦å®š
      default_structure = {
          # æ¦‚è§ˆæ–‡ä»¶å›ºå®šä½ç½®
          "README.md": {"title": "é¡¹ç›®æ¦‚è§ˆ", "sections": ["introduction", "quick_look"]},
          "docs/index.md": {"title": "æ–‡æ¡£é¦–é¡µ", "sections": ["introduction", "navigation"]},
          "docs/overview.md": {"title": "ç³»ç»Ÿæ¶æ„", "sections": ["overall_architecture", "core_modules_summary"]},
          "docs/glossary.md": {"title": "æœ¯è¯­è¡¨", "sections": ["glossary"]},
          "docs/evolution.md": {"title": "æ¼”å˜å†å²", "sections": ["evolution_narrative"]},

          # æ¨¡å—æ–‡æ¡£æ”¾ç½®åœ¨ä¸ä»£ç ä»“åº“ç»“æ„å¯¹åº”çš„ç›®å½•ä¸­ï¼Œä½¿ç”¨ JustDoc å…¼å®¹çš„å‘½å
          # ç›®å½•ä½¿ç”¨å°å†™ï¼Œå•è¯é—´ç”¨è¿å­—ç¬¦åˆ†éš”
          # æ–‡ä»¶åä½¿ç”¨å°å†™ï¼Œå•è¯é—´ç”¨è¿å­—ç¬¦åˆ†éš”
          "docs/{module_dir}/{module_file}.md": {"title": "{module_title}", "sections": ["description", "api", "examples"]}
      }

      # æ¨¡å—æ–‡ä»¶ç”Ÿæˆç¤ºä¾‹ - ç¬¦åˆ JustDoc å‘½åçº¦å®š
      # æºä»£ç : src/auth/service.py -> æ–‡æ¡£: docs/auth/service.md
      # æºä»£ç : src/data_processor/main.py -> æ–‡æ¡£: docs/data-processor/main.md
      # æºä»£ç : utils/helpers/string_utils.py -> æ–‡æ¡£: docs/utils/helpers/string-utils.md

      # JustDoc å…¼å®¹çš„ç›®å½•ç»“æ„
      # docs/
      # â”œâ”€â”€ index.md                # æ–‡æ¡£é¦–é¡µ
      # â”œâ”€â”€ overview.md            # ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ
      # â”œâ”€â”€ auth/                  # ä¸æºç ç›®å½•ç»“æ„å¯¹åº”
      # â”‚   â”œâ”€â”€ index.md           # æ¨¡å—ç´¢å¼•é¡µ
      # â”‚   â”œâ”€â”€ service.md         # å…·ä½“æœåŠ¡æ–‡æ¡£
      # â”‚   â””â”€â”€ models.md          # æ¨¡å‹æ–‡æ¡£
      # â”œâ”€â”€ data-processor/        # æ³¨æ„ä½¿ç”¨è¿å­—ç¬¦è€Œéä¸‹åˆ’çº¿
      # â”‚   â”œâ”€â”€ index.md           # æ¨¡å—ç´¢å¼•é¡µ
      # â”‚   â””â”€â”€ main.md            # ä¸»è¦åŠŸèƒ½æ–‡æ¡£
      # â””â”€â”€ utils/
      #     â”œâ”€â”€ index.md           # å·¥å…·æ¨¡å—ç´¢å¼•
      #     â””â”€â”€ helpers/
      #         â”œâ”€â”€ index.md       # è¾…åŠ©å·¥å…·ç´¢å¼•
      #         â””â”€â”€ string-utils.md # å­—ç¬¦ä¸²å·¥å…·æ–‡æ¡£

      # ä»£ç ä»“åº“ç»“æ„ç¤ºä¾‹
      repo_structure = {
          "auth_service": {
              "path": "src/auth/service.py",
              "type": "class"
          },
          "data_processor": {
              "path": "src/data/processor.py",
              "type": "class"
          },
          "helpers": {
              "path": "utils/helpers.py",
              "type": "module"
          }
      }
      ```

14. **`generate_navigation_links(files_info, current_file, related_content=None)`** (`utils/formatter.py`)
    - _è¾“å…¥_: æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨ (list of dict), å½“å‰æ–‡ä»¶è·¯å¾„ (str), ç›¸å…³å†…å®¹ä¿¡æ¯ (list of dict, å¯é€‰)
    - _è¾“å‡º_: å¯¼èˆªé“¾æ¥ HTML/Markdown ä»£ç  (str)
    - _å¿…è¦æ€§_: åœ¨å¤šæ–‡ä»¶æ–‡æ¡£ä¸­ç”Ÿæˆå¯¼èˆªé“¾æ¥ï¼Œä¾¿äºç”¨æˆ·æµè§ˆã€‚
    - _å¯¼èˆªç±»å‹_: åŒ…æ‹¬ä¸Šä¸€é¡µ/ä¸‹ä¸€é¡µé“¾æ¥ã€ç›®å½•é“¾æ¥ã€é¢åŒ…å±‘å¯¼èˆªç­‰ã€‚
    - _ä¸Šä¸‹æ–‡ç›¸å…³_: æ ¹æ®å½“å‰é¡µé¢å†…å®¹å’Œç›¸å…³æ¨¡å—ï¼Œç”Ÿæˆæ›´åŠ ä¸Šä¸‹æ–‡ç›¸å…³çš„å¯¼èˆªé“¾æ¥ã€‚
    - _å®ç°ç¤ºä¾‹_:
      ```python
      def generate_navigation_links(files_info, current_file, related_content=None):
          """ç”Ÿæˆæ›´åŠ ä¸Šä¸‹æ–‡ç›¸å…³çš„å¯¼èˆªé“¾æ¥"""
          # æ‰¾åˆ°å½“å‰æ–‡ä»¶åœ¨æ–‡ä»¶åˆ—è¡¨ä¸­çš„ä½ç½®
          current_index = -1
          for i, file_info in enumerate(files_info):
              if file_info["path"] == current_file:
                  current_index = i
                  break

          if current_index == -1:
              return ""

          # è·å–ä¸Šä¸€é¡µå’Œä¸‹ä¸€é¡µ
          prev_file = files_info[current_index - 1] if current_index > 0 else None
          next_file = files_info[current_index + 1] if current_index < len(files_info) - 1 else None

          # åˆ›å»ºå¯¼èˆªé“¾æ¥
          nav_links = []

          if prev_file:
              prev_title = prev_file.get("title", os.path.basename(prev_file["path"]))
              nav_links.append(f"[â† {prev_title}]({os.path.relpath(prev_file['path'], os.path.dirname(current_file))})")

          # æ·»åŠ é¦–é¡µé“¾æ¥
          index_path = find_index_path(files_info, current_file)
          if index_path:
              nav_links.append(f"[ğŸ  é¦–é¡µ]({os.path.relpath(index_path, os.path.dirname(current_file))})")

          if next_file:
              next_title = next_file.get("title", os.path.basename(next_file["path"]))
              nav_links.append(f"[{next_title} â†’]({os.path.relpath(next_file['path'], os.path.dirname(current_file))})")

          nav_html = " | ".join(nav_links)

          # æ·»åŠ é¢åŒ…å±‘å¯¼èˆª
          breadcrumb = generate_breadcrumb(files_info, current_file)

          # æ·»åŠ ç›¸å…³å†…å®¹å¯¼èˆªï¼ˆå¦‚æœæœ‰ï¼‰
          related_html = ""
          if related_content:
              related_html = "\n\n### ç›¸å…³å†…å®¹\n\n"

              # æŒ‰ç›¸å…³æ€§åˆ†ç»„
              by_category = {}
              for item in related_content:
                  category = item.get("category", "å…¶ä»–")
                  if category not in by_category:
                      by_category[category] = []
                  by_category[category].append(item)

              # ä¸ºæ¯ä¸ªåˆ†ç±»åˆ›å»ºé“¾æ¥ç»„
              for category, items in by_category.items():
                  related_html += f"**{category}:** "
                  category_links = []

                  for item in items:
                      title = item.get("title", "")
                      path = item.get("path", "")
                      if title and path:
                          rel_path = os.path.relpath(path, os.path.dirname(current_file))
                          category_links.append(f"[{title}]({rel_path})")

                  related_html += ", ".join(category_links) + "\n\n"

          # ç»„åˆæ‰€æœ‰å¯¼èˆªå…ƒç´ 
          return f"{nav_html}\n\n{breadcrumb}\n{related_html}\n---\n"
      ```
    - _Markdown è¾“å‡ºç¤ºä¾‹_:
      ```markdown
      <!-- ä¸Šä¸‹æ–‡ç›¸å…³å¯¼èˆªé“¾æ¥ - ç¬¦åˆ JustDoc å‘½åçº¦å®š -->
      [â† ç³»ç»Ÿæ¶æ„](../overview.md) | [ğŸ  é¦–é¡µ](../index.md) | [æ•°æ®å¤„ç†å™¨ â†’](../data-processor/main.md)

      > å½“å‰ä½ç½®: [é¦–é¡µ](../index.md) > [Auth æ¨¡å—](index.md) > Service

      ### ç›¸å…³å†…å®¹

      **æ ¸å¿ƒæœåŠ¡:** [è®¤è¯æœåŠ¡](../auth/service.md), [ç”¨æˆ·ç®¡ç†](../auth/users.md)
      **ä¾èµ–æ¨¡å—:** [æ•°æ®å­˜å‚¨](../storage/manager.md), [æ—¥å¿—ç³»ç»Ÿ](../utils/logger.md)

      ---
      title: è®¤è¯æœåŠ¡
      category: Auth
      order: 2
      ---
      ```

15. **`create_code_links(code_references, repo_url=None, branch='main', context_text=None)`** (`utils/formatter.py`)
    - _è¾“å…¥_: ä»£ç å¼•ç”¨ä¿¡æ¯ (dict), ä»“åº“ URL (str), åˆ†æ”¯å (str), ä¸Šä¸‹æ–‡æ–‡æœ¬ (str, å¯é€‰)
    - _è¾“å‡º_: å¸¦æœ‰æºç é“¾æ¥çš„ä»£ç å¼•ç”¨ Markdown (str) æˆ–åµŒå…¥äº†é“¾æ¥çš„ä¸Šä¸‹æ–‡æ–‡æœ¬
    - _å¿…è¦æ€§_: ä¸ºä»£ç å¼•ç”¨åˆ›å»ºç›´æ¥é“¾æ¥åˆ°æºä»£ç çš„é“¾æ¥ï¼Œä¾¿äºç”¨æˆ·æŸ¥çœ‹å®Œæ•´ä»£ç ã€‚
    - _é“¾æ¥ç±»å‹_: æ”¯æŒ GitHub/GitLab é£æ ¼çš„æºç é“¾æ¥ï¼ŒåŒ…æ‹¬è¡Œå·èŒƒå›´ã€‚
    - _åµŒå…¥æ¨¡å¼_: å½“æä¾›ä¸Šä¸‹æ–‡æ–‡æœ¬æ—¶ï¼Œå°†æ¨¡å—å’Œå‡½æ•°å¼•ç”¨è‡ªç„¶åœ°åµŒå…¥åˆ°æ–‡æœ¬ä¸­ï¼Œè€Œéå•ç‹¬åˆ—å‡ºã€‚
    - _å®ç°ç¤ºä¾‹_:
      ```python
      # ä»£ç å¼•ç”¨ç¤ºä¾‹
      def create_github_link(repo_url, file_path, line_start=None, line_end=None):
          link = f"{repo_url}/blob/{branch}/{file_path}"
          if line_start:
              link += f"#L{line_start}"
              if line_end and line_end > line_start:
                  link += f"-L{line_end}"
          return link

      # ä½¿ç”¨ç¤ºä¾‹ - ä¸ä»£ç ä»“åº“ç»“æ„å¯¹åº”
      code_ref = {
          "file_path": "src/utils/formatter.py",  # ä¸ä»£ç ä»“åº“ä¸­çš„å®é™…è·¯å¾„ä¸€è‡´
          "line_start": 10,
          "line_end": 20,
          "code": "def format_markdown(...):\n    ...",
          "description": "æ ¼å¼åŒ– Markdown çš„æ ¸å¿ƒå‡½æ•°",
          "module_name": "formatter",  # å¯¹åº”çš„æ¨¡å—åï¼Œç”¨äºç”Ÿæˆæ–‡æ¡£é“¾æ¥
          "function_name": "format_markdown"  # å‡½æ•°åï¼Œç”¨äºåœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾å¼•ç”¨
      }

      # ç”Ÿæˆæ–‡æ¡£é“¾æ¥ - é“¾æ¥åˆ°å¯¹åº”æ¨¡å—çš„æ–‡æ¡£ï¼Œç¬¦åˆ JustDoc å‘½åçº¦å®š
      doc_link = f"[æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£](../utils/formatter.md#format-markdown)"

      # åµŒå…¥æ¨¡å¼ - åœ¨ä¸Šä¸‹æ–‡æ–‡æœ¬ä¸­è‡ªç„¶åµŒå…¥é“¾æ¥
      context = "ç³»ç»Ÿä½¿ç”¨ `formatter` æ¨¡å—ä¸­çš„ `format_markdown` å‡½æ•°å¤„ç†æ–‡æ¡£æ ¼å¼åŒ–ã€‚"
      context_with_links = create_code_links([code_ref], repo_url, branch, context)
      # ç»“æœ: "ç³»ç»Ÿä½¿ç”¨ [`formatter`](../utils/formatter.md) æ¨¡å—ä¸­çš„ [`format_markdown`](https://github.com/user/repo/blob/main/src/utils/formatter.py#L10-L20) å‡½æ•°å¤„ç†æ–‡æ¡£æ ¼å¼åŒ–ã€‚"
      ```
    - _æ ‡å‡†æ¨¡å¼ Markdown è¾“å‡º_:
      ```markdown
      **æ ¼å¼åŒ– Markdown çš„æ ¸å¿ƒå‡½æ•°** [æŸ¥çœ‹æºç ](https://github.com/user/repo/blob/main/src/utils/formatter.py#L10-L20) | [æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£](../utils/formatter.md#format-markdown)

      ```python
      def format_markdown(...):\n    ...
      ```

      > æ­¤å‡½æ•°ä½äº `src/utils/formatter.py` æ¨¡å—ä¸­ï¼Œè´Ÿè´£å°†ç”Ÿæˆçš„å†…å®¹è½¬æ¢ä¸ºæ ¼å¼åŒ–çš„ Markdownã€‚

      <!-- JustDoc å…¼å®¹çš„ä»£ç å¼•ç”¨å…ƒæ•°æ® -->
      ```yaml
      source:
        file: src/utils/formatter.py
        line_start: 10
        line_end: 20
      ```
      ```

    - _åµŒå…¥æ¨¡å¼å®ç°_:
      ```python
      def embed_code_references(content, code_references, repo_url=None, branch='main'):
          """åœ¨å†…å®¹ä¸­è‡ªç„¶åµŒå…¥ä»£ç å¼•ç”¨"""
          if not code_references or not content:
              return content

          # åˆ›å»ºå‡½æ•°åå’Œæ¨¡å—ååˆ°å¼•ç”¨ä¿¡æ¯çš„æ˜ å°„
          function_map = {}
          module_map = {}

          for ref in code_references:
              if "function_name" in ref:
                  function_map[ref["function_name"]] = ref
              if "module_name" in ref:
                  module_map[ref["module_name"]] = ref

          # åˆ†å‰²å†…å®¹ä¸ºæ®µè½ï¼Œé€æ®µå¤„ç†
          paragraphs = content.split("\n\n")
          result_paragraphs = []

          for para in paragraphs:
              modified_para = para

              # å¤„ç†å‡½æ•°å¼•ç”¨ - å°†çº¯æ–‡æœ¬å¼•ç”¨æ›¿æ¢ä¸ºé“¾æ¥
              for func_name, ref in function_map.items():
                  source_link = create_github_link(
                      repo_url, ref["file_path"], ref["line_start"], ref["line_end"]
                  )

                  patterns = [
                      (f"`{func_name}`", f"[`{func_name}`]({source_link})"),
                      (f" {func_name}(", f" [{func_name}]({source_link})("),
                      (f"å‡½æ•° {func_name}", f"å‡½æ•° [{func_name}]({source_link})")
                  ]

                  for pattern, replacement in patterns:
                      if pattern in modified_para:
                          # åªæ›¿æ¢ç¬¬ä¸€æ¬¡å‡ºç°ï¼Œé¿å…è¿‡åº¦é“¾æ¥
                          modified_para = modified_para.replace(pattern, replacement, 1)
                          break

              # å¤„ç†æ¨¡å—å¼•ç”¨
              for module_name, ref in module_map.items():
                  doc_path = f"../utils/{module_name.replace('_', '-').lower()}.md"

                  patterns = [
                      (f"`{module_name}`", f"[`{module_name}`]({doc_path})"),
                      (f" {module_name} æ¨¡å—", f" [{module_name}]({doc_path}) æ¨¡å—"),
                      (f"æ¨¡å— {module_name}", f"æ¨¡å— [{module_name}]({doc_path})")
                  ]

                  for pattern, replacement in patterns:
                      if pattern in modified_para:
                          modified_para = modified_para.replace(pattern, replacement, 1)
                          break

              result_paragraphs.append(modified_para)

          return "\n\n".join(result_paragraphs)
      ```

16. **`generate_module_detail_page(module_name, module_info, related_modules, code_references, repo_url)`** (`utils/formatter.py`)
    - _è¾“å…¥_: æ¨¡å—åç§° (str), æ¨¡å—ä¿¡æ¯ (dict), ç›¸å…³æ¨¡å—åˆ—è¡¨ (list), ä»£ç å¼•ç”¨ä¿¡æ¯ (list), ä»“åº“ URL (str)
    - _è¾“å‡º_: æ¨¡å—è¯¦æƒ…é¡µé¢çš„ Markdown å†…å®¹ (str)
    - _å¿…è¦æ€§_: ç”Ÿæˆæ¨¡å—è¯¦æƒ…é¡µé¢ï¼Œå°†ç›¸å…³æ¨¡å—é“¾æ¥è‡ªç„¶åµŒå…¥åˆ°æ–‡æœ¬ä¸­ï¼Œä½¿æ–‡æ¡£æ›´åŠ æµç•…ã€‚
    - _é“¾æ¥åµŒå…¥_: åœ¨æè¿°ã€API æ–‡æ¡£å’Œç¤ºä¾‹ä¸­è‡ªç„¶åµŒå…¥ç›¸å…³æ¨¡å—å’Œå‡½æ•°é“¾æ¥ï¼Œè€Œéå•ç‹¬åˆ—å‡ºã€‚
    - _å®ç°ç¤ºä¾‹_:
      ```python
      def generate_module_detail_page(module_name, module_info, related_modules, code_references, repo_url):
          """
          ç”Ÿæˆæ¨¡å—è¯¦æƒ…é¡µé¢ï¼Œå°†ç›¸å…³æ¨¡å—é“¾æ¥è‡ªç„¶åµŒå…¥åˆ°æ–‡æœ¬ä¸­

          Args:
              module_name: æ¨¡å—åç§°
              module_info: æ¨¡å—ä¿¡æ¯
              related_modules: ç›¸å…³æ¨¡å—åˆ—è¡¨
              code_references: ä»£ç å¼•ç”¨ä¿¡æ¯
              repo_url: ä»“åº“URL

          Returns:
              æ¨¡å—è¯¦æƒ…é¡µé¢çš„Markdownå†…å®¹
          """
          # åŸºæœ¬ä¿¡æ¯
          title = module_name.replace("_", " ").title()
          content = f"# ğŸ“¦ {title}\n\n"

          # æ¨¡å—æè¿°
          description = module_info.get("description", "")
          # åœ¨æè¿°ä¸­åµŒå…¥ç›¸å…³æ¨¡å—é“¾æ¥
          description_with_links = create_code_links(
              code_references,
              repo_url=repo_url,
              context_text=description
          )
          content += f"## ğŸ“‹ æ¦‚è¿°\n\n{description_with_links}\n\n"

          # API éƒ¨åˆ†
          if "api_description" in module_info:
              api_desc = module_info["api_description"]
              # åœ¨APIæè¿°ä¸­åµŒå…¥ç›¸å…³å‡½æ•°é“¾æ¥
              api_with_links = create_code_links(
                  code_references,
                  repo_url=repo_url,
                  context_text=api_desc
              )
              content += f"## ğŸ”Œ API\n\n{api_with_links}\n\n"

          # ä»£ç ç¤ºä¾‹éƒ¨åˆ†
          if "code_examples" in module_info and module_info["code_examples"]:
              content += "## ğŸ’» ç¤ºä¾‹\n\n"
              for i, example in enumerate(module_info["code_examples"]):
                  snippet = example.get("snippet", "")
                  explanation = example.get("explanation", "")

                  # åœ¨è§£é‡Šä¸­åµŒå…¥ç›¸å…³é“¾æ¥
                  explanation_with_links = create_code_links(
                      code_references,
                      repo_url=repo_url,
                      context_text=explanation
                  )

                  content += f"### ğŸ” ç¤ºä¾‹ {i+1}\n\n"
                  content += f"```python\n{snippet}\n```\n\n"
                  content += f"{explanation_with_links}\n\n"

          # å†…éƒ¨ä¾èµ–éƒ¨åˆ† - è‡ªç„¶åµŒå…¥ç›¸å…³æ¨¡å—é“¾æ¥
          if "internal_dependencies" in module_info:
              deps_text = module_info["internal_dependencies"]
              # åœ¨ä¾èµ–æè¿°ä¸­åµŒå…¥ç›¸å…³æ¨¡å—é“¾æ¥
              deps_with_links = create_code_links(
                  code_references,
                  repo_url=repo_url,
                  context_text=deps_text
              )
              content += f"## ğŸ”— ä¾èµ–å…³ç³»\n\n{deps_with_links}\n\n"

          # æ·»åŠ å¯¼èˆªé“¾æ¥
          content += "\n\n---\n\n"
          content += "**ç›¸å…³æ¨¡å—:** "

          # å°†ç›¸å…³æ¨¡å—ä½œä¸ºè¡Œå†…é“¾æ¥
          related_links = []
          for related in related_modules:
              related_name = related.replace("_", "-").lower()
              related_title = related.replace("_", " ").title()
              related_links.append(f"[{related_title}](../utils/{related_name}.md)")

          content += " | ".join(related_links)

          return content
      ```
    - _Markdown è¾“å‡ºç¤ºä¾‹_:
      ```markdown
      # ğŸ“¦ String Utils

      ## ğŸ“‹ æ¦‚è¿°

      [`string_utils`](../utils/string-utils.md) æ¨¡å—æä¾›äº†ä¸€ç³»åˆ—å­—ç¬¦ä¸²å¤„ç†å‡½æ•°ï¼Œç”¨äºåœ¨ [`formatter`](../utils/formatter.md) æ¨¡å—ä¸­è¿›è¡Œæ–‡æœ¬æ ¼å¼åŒ–ã€‚

      ## ğŸ”Œ API

      ### [`clean_text`](https://github.com/user/repo/blob/main/src/utils/string_utils.py#L10-L25)

      æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºç™½ã€‚

      ### [`format_code_block`](https://github.com/user/repo/blob/main/src/utils/string_utils.py#L28-L45)

      æ ¼å¼åŒ–ä»£ç å—ï¼Œç¡®ä¿æ­£ç¡®çš„ç¼©è¿›å’Œè¯­æ³•é«˜äº®ã€‚

      ## ğŸ’» ç¤ºä¾‹

      ### ğŸ” ç¤ºä¾‹ 1

      ```python
      from utils.string_utils import clean_text

      text = "  å¤šä½™çš„ç©ºæ ¼   å’Œç‰¹æ®Š\tå­—ç¬¦\n\n"
      cleaned = clean_text(text)
      print(cleaned)  # è¾“å‡º: "å¤šä½™çš„ç©ºæ ¼ å’Œç‰¹æ®Š å­—ç¬¦"
      ```

      è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ [`clean_text`](https://github.com/user/repo/blob/main/src/utils/string_utils.py#L10-L25) å‡½æ•°æ¸…ç†æ–‡æœ¬ä¸­çš„å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦ã€‚

      ## ğŸ”— ä¾èµ–å…³ç³»

      æœ¬æ¨¡å—è¢« [`formatter`](../utils/formatter.md) æ¨¡å—ä¾èµ–ï¼Œç”¨äºå¤„ç†æ–‡æœ¬æ ¼å¼åŒ–å‰çš„é¢„å¤„ç†å·¥ä½œã€‚

      ---

      **ç›¸å…³æ¨¡å—:** [Formatter](../utils/formatter.md) | [Text Processor](../utils/text-processor.md)
      ```

17. **`format_markdown(content_dict, template=None, toc=True, nav_links=True, add_emojis=True)`** (`utils/formatter.py`)
    - _è¾“å…¥_: åŒ…å«æ•™ç¨‹å„éƒ¨åˆ†å†…å®¹çš„å­—å…¸ (dict), æ¨¡æ¿ (str), æ˜¯å¦ç”Ÿæˆç›®å½• (bool), æ˜¯å¦ç”Ÿæˆå¯¼èˆªé“¾æ¥ (bool), æ˜¯å¦æ·»åŠ  emoji (bool)
    - _è¾“å‡º_: æ ¼å¼åŒ–åçš„å®Œæ•´ Markdown æ–‡æœ¬ (str), **åŒ…å«å†…éƒ¨å¯¼èˆªé“¾æ¥å’Œé€‚ç”¨äº Web æ–‡æ¡£ï¼ˆå¦‚ GitHub Pagesï¼‰çš„ç»“æ„**
    - _å¿…è¦æ€§_: ç»„åˆæ‰€æœ‰ç”Ÿæˆçš„å†…å®¹å¹¶åº”ç”¨é€‚åˆ GitHub Pages ç­‰å¹³å°çš„ Markdown æ ¼å¼ã€‚
    - _æ¨¡æ¿æ”¯æŒ_: å…è®¸ä½¿ç”¨è‡ªå®šä¹‰æ¨¡æ¿å®šåˆ¶è¾“å‡ºæ ¼å¼ã€‚
    - _éªŒè¯_: éªŒè¯ç”Ÿæˆçš„ Markdown æ˜¯å¦ç¬¦åˆè§„èŒƒï¼Œæ£€æŸ¥é“¾æ¥æœ‰æ•ˆæ€§ã€‚
    - _ä¸å¤šæ–‡ä»¶é›†æˆ_: ä¸ `split_content_into_files` å’Œ `generate_navigation_links` é…åˆä½¿ç”¨ï¼Œç”Ÿæˆå®Œæ•´çš„å¤šæ–‡ä»¶æ–‡æ¡£ã€‚
    - _Emoji æ”¯æŒ_: è‡ªåŠ¨ä¸ºæ ‡é¢˜æ·»åŠ é€‚å½“çš„ emojiï¼Œä½¿æ–‡æ¡£é‡ç‚¹æ›´åŠ çªå‡ºã€‚
    - _å®ç°ç¤ºä¾‹_:
      ```python
      def format_markdown(content_dict, template=None, toc=True, nav_links=True, add_emojis=True):
          """æ ¼å¼åŒ– Markdown å†…å®¹

          Args:
              content_dict: åŒ…å«æ•™ç¨‹å„éƒ¨åˆ†å†…å®¹çš„å­—å…¸
              template: å¯é€‰çš„æ¨¡æ¿å­—ç¬¦ä¸²
              toc: æ˜¯å¦ç”Ÿæˆç›®å½•
              nav_links: æ˜¯å¦ç”Ÿæˆå¯¼èˆªé“¾æ¥
              add_emojis: æ˜¯å¦æ·»åŠ  emoji åˆ°æ ‡é¢˜

          Returns:
              æ ¼å¼åŒ–åçš„å®Œæ•´ Markdown æ–‡æœ¬
          """
          # ä½¿ç”¨é»˜è®¤æ¨¡æ¿æˆ–è‡ªå®šä¹‰æ¨¡æ¿
          if template is None:
              template = """
              # {title}

              {toc}

              ## ç®€ä»‹

              {introduction}

              ## ç³»ç»Ÿæ¶æ„

              {architecture}

              ## æ ¸å¿ƒæ¨¡å—

              {core_modules}

              ## ä½¿ç”¨ç¤ºä¾‹

              {examples}

              ## å¸¸è§é—®é¢˜

              {faq}

              ## å‚è€ƒèµ„æ–™

              {references}
              """

          # å¡«å……æ¨¡æ¿
          content = template.format(**content_dict)

          # ç”Ÿæˆç›®å½•
          if toc:
              toc_content = generate_toc(content)
              content = content.replace("{toc}", toc_content)
          else:
              content = content.replace("{toc}", "")

          # æ·»åŠ å¯¼èˆªé“¾æ¥
          if nav_links:
              nav_content = generate_navigation_links(content_dict.get("files_info", []),
                                                     content_dict.get("current_file", ""),
                                                     content_dict.get("related_content", []))
              content = nav_content + content

          # æ·»åŠ  emoji åˆ°æ ‡é¢˜
          if add_emojis:
              content = add_emojis_to_headings(content)

          # éªŒè¯ Markdown æ ¼å¼
          validation_result = validate_markdown(content)
          if not validation_result["valid"]:
              log_and_notify(f"Markdown éªŒè¯å¤±è´¥: {validation_result['errors']}", "warning")

          return content


      def add_emojis_to_headings(markdown_text):
          """ä¸º Markdown æ ‡é¢˜æ·»åŠ  emojiï¼Œä½¿æ–‡æ¡£é‡ç‚¹æ›´åŠ çªå‡º

          Args:
              markdown_text: åŸå§‹ Markdown æ–‡æœ¬

          Returns:
              æ·»åŠ äº† emoji çš„ Markdown æ–‡æœ¬
          """
          # å®šä¹‰æ ‡é¢˜çº§åˆ«å¯¹åº”çš„ emoji
          heading_emojis = {
              "# ": "ğŸ“š ",  # ä¸€çº§æ ‡é¢˜: ä¹¦ç±
              "## ": "ğŸ“‹ ",  # äºŒçº§æ ‡é¢˜: æ–‡æ¡£
              "### ": "ğŸ” ",  # ä¸‰çº§æ ‡é¢˜: æ”¾å¤§é•œ
              "#### ": "ğŸ”¹ ",  # å››çº§æ ‡é¢˜: è“è‰²å°è±å½¢
              "##### ": "âœï¸ ",  # äº”çº§æ ‡é¢˜: é“…ç¬”
              "###### ": "ğŸ“ "  # å…­çº§æ ‡é¢˜: å›å½¢é’ˆ
          }

          # ç‰¹å®šå†…å®¹çš„ emoji æ˜ å°„
          content_emojis = {
              "æ¦‚è¿°": "ğŸ“‹",
              "ç®€ä»‹": "ğŸ“",
              "ä»‹ç»": "ğŸ“",
              "å®‰è£…": "âš™ï¸",
              "é…ç½®": "ğŸ”§",
              "ä½¿ç”¨æ–¹æ³•": "ğŸ“˜",
              "ç¤ºä¾‹": "ğŸ’»",
              "API": "ğŸ”Œ",
              "å‡½æ•°": "âš¡",
              "ç±»": "ğŸ§©",
              "æ¨¡å—": "ğŸ“¦",
              "ä¾èµ–": "ğŸ”—",
              "æ¶æ„": "ğŸ—ï¸",
              "æµç¨‹": "ğŸ”„",
              "æ•°æ®ç»“æ„": "ğŸ“Š",
              "ç®—æ³•": "ğŸ§®",
              "æ€§èƒ½": "âš¡",
              "ä¼˜åŒ–": "ğŸš€",
              "æµ‹è¯•": "ğŸ§ª",
              "éƒ¨ç½²": "ğŸš¢",
              "å¸¸è§é—®é¢˜": "â“",
              "æ•…éšœæ’é™¤": "ğŸ”§",
              "è´¡çŒ®": "ğŸ‘¥",
              "è®¸å¯è¯": "ğŸ“œ",
              "å‚è€ƒ": "ğŸ“š",
              "ç»“è®º": "ğŸ¯",
              "æ€»ç»“": "ğŸ“",
              "é™„å½•": "ğŸ“"
          }

          lines = markdown_text.split("\n")
          result_lines = []

          for line in lines:
              # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜è¡Œ
              is_heading = False
              for heading_prefix, emoji in heading_emojis.items():
                  if line.startswith(heading_prefix):
                      # æ£€æŸ¥æ ‡é¢˜å†…å®¹æ˜¯å¦æœ‰ç‰¹å®šçš„ emoji æ˜ å°„
                      title_text = line[len(heading_prefix):].strip()
                      custom_emoji = None

                      for content_key, content_emoji in content_emojis.items():
                          if content_key in title_text.lower():
                              custom_emoji = content_emoji
                              break

                      # å¦‚æœæ ‡é¢˜å·²ç»åŒ…å« emojiï¼Œä¸å†æ·»åŠ 
                      if any(char in title_text for char in "ğŸ”ğŸ“šğŸ“‹ğŸ”¹âœï¸ğŸ“ğŸ“âš™ï¸ğŸ”§ğŸ“˜ğŸ’»ğŸ”Œâš¡ğŸ§©ğŸ“¦ğŸ”—ğŸ—ï¸ğŸ”„ğŸ“ŠğŸ§®âš¡ğŸš€ğŸ§ªğŸš¢â“ğŸ‘¥ğŸ“œğŸ¯"):
                          result_lines.append(line)
                      else:
                          # ä½¿ç”¨ç‰¹å®šå†…å®¹çš„ emoji æˆ–é»˜è®¤çš„æ ‡é¢˜çº§åˆ« emoji
                          emoji_to_use = custom_emoji or emoji.strip()
                          result_lines.append(f"{heading_prefix}{emoji_to_use} {title_text}")

                      is_heading = True
                      break

              # å¦‚æœä¸æ˜¯æ ‡é¢˜è¡Œï¼Œç›´æ¥æ·»åŠ 
              if not is_heading:
                  result_lines.append(line)

          return "\n".join(result_lines)
      ```

18. **`convert_to_pdf(markdown_files, output_path, style=None, toc=True, cover_page=True)`** (`utils/formatter.py`)
    - _è¾“å…¥_: Markdown æ–‡ä»¶è·¯å¾„åˆ—è¡¨ (list of str), è¾“å‡º PDF è·¯å¾„ (str), æ ·å¼é…ç½® (dict), æ˜¯å¦åŒ…å«ç›®å½• (bool), æ˜¯å¦åŒ…å«å°é¢ (bool)
    - _è¾“å‡º_: PDF æ–‡ä»¶è·¯å¾„ (str) æˆ–äºŒè¿›åˆ¶å†…å®¹
    - _å¿…è¦æ€§_: æä¾› PDF æ ¼å¼çš„æ•™ç¨‹è¾“å‡ºï¼Œæ”¯æŒå¤šæ–‡ä»¶åˆå¹¶ã€‚
    - _æ ·å¼å®šåˆ¶_: æ”¯æŒè‡ªå®šä¹‰ PDF æ ·å¼ï¼Œå¦‚å­—ä½“ã€é¢œè‰²ã€é¡µçœ‰é¡µè„šç­‰ã€‚
    - _é”™è¯¯å¤„ç†_: å¤„ç†è½¬æ¢è¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œå¦‚ç‰¹æ®Šå­—ç¬¦ã€å¤æ‚è¡¨æ ¼ç­‰ã€‚
    - _å¤šæ–‡ä»¶åˆå¹¶_: å°†å¤šä¸ª Markdown æ–‡ä»¶åˆå¹¶ä¸ºå•ä¸ª PDFï¼Œä¿æŒç›®å½•ç»“æ„å’Œå†…éƒ¨é“¾æ¥ã€‚

19. **`publish_to_platform(content_dir, platform, target_repo, auth, config=None)`** (`utils/publisher.py`)
    - _è¾“å…¥_: å†…å®¹ç›®å½• (str), å¹³å° (str), ç›®æ ‡ä»“åº“ä¿¡æ¯ (str), è®¤è¯ä¿¡æ¯ (dict), å¹³å°ç‰¹å®šé…ç½® (dict)
    - _è¾“å‡º_: å‘å¸ƒ URL æˆ–æˆåŠŸçŠ¶æ€ (str/bool), è¯¦ç»†ä¿¡æ¯ (dict)
    - _å¿…è¦æ€§_: å®ç°å°†æ•™ç¨‹è‡ªåŠ¨å‘å¸ƒåˆ°å¤šç§æ–‡æ¡£å¹³å°çš„åŠŸèƒ½ã€‚
    - _æ”¯æŒçš„å¹³å°_:
      - **GitHub Pages**: é…ç½®å’Œè§¦å‘ GitHub Pages æ„å»º
      - **GitLab Pages**: æ”¯æŒ GitLab CI/CD è‡ªåŠ¨éƒ¨ç½²
      - **ReadTheDocs**: é›†æˆ ReadTheDocs æ–‡æ¡£æ‰˜ç®¡
      - **Netlify**: æ”¯æŒ Netlify é™æ€ç½‘ç«™æ‰˜ç®¡
      - **Vercel**: æ”¯æŒ Vercel éƒ¨ç½²
      - **Gitbook**: æ”¯æŒå‘å¸ƒåˆ° Gitbook å¹³å°
      - **Docsify**: ç”Ÿæˆ Docsify é…ç½®æ–‡ä»¶
      - **VuePress**: ç”Ÿæˆ VuePress é…ç½®æ–‡ä»¶
      - **MkDocs**: ç”Ÿæˆ MkDocs é…ç½®æ–‡ä»¶
      - **JustDoc**: æ”¯æŒ JustDoc æ–‡æ¡£ç³»ç»Ÿ
    - _é”™è¯¯å¤„ç†_: å¤„ç†è®¤è¯å¤±è´¥ã€æƒé™é—®é¢˜ã€ç½‘ç»œé”™è¯¯ç­‰æƒ…å†µã€‚
    - _éªŒè¯_: å‘å¸ƒåéªŒè¯å†…å®¹æ˜¯å¦æ­£ç¡®æ˜¾ç¤ºï¼Œé“¾æ¥æ˜¯å¦æœ‰æ•ˆã€‚
    - _å¤šæ–‡ä»¶æ”¯æŒ_: ä¿æŒç›®å½•ç»“æ„å’Œæ–‡ä»¶é—´é“¾æ¥å…³ç³»ï¼Œç¡®ä¿å‘å¸ƒåçš„å¯¼èˆªæ­£å¸¸å·¥ä½œã€‚
    - _å®ç°ç¤ºä¾‹_:
      ```python
      def publish_to_platform(content_dir, platform, target_repo, auth, config=None):
          """å°†æ–‡æ¡£å‘å¸ƒåˆ°æŒ‡å®šå¹³å°"""
          platform = platform.lower()
          config = config or {}

          # é€‰æ‹©åˆé€‚çš„å‘å¸ƒå™¨
          if platform == "github":
              publisher = GitHubPublisher(auth, config.get("github_pages", {}))
          elif platform == "gitlab":
              publisher = GitLabPublisher(auth, config.get("gitlab_pages", {}))
          elif platform == "readthedocs":
              publisher = ReadTheDocsPublisher(auth, config.get("rtd", {}))
          elif platform == "netlify":
              publisher = NetlifyPublisher(auth, config.get("netlify", {}))
          elif platform == "vercel":
              publisher = VercelPublisher(auth, config.get("vercel", {}))
          elif platform == "gitbook":
              publisher = GitbookPublisher(auth, config.get("gitbook", {}))
          elif platform == "docsify":
              publisher = DocsifyPublisher(config.get("docsify", {}))
          elif platform == "vuepress":
              publisher = VuePressPublisher(config.get("vuepress", {}))
          elif platform == "mkdocs":
              publisher = MkDocsPublisher(config.get("mkdocs", {}))
          elif platform == "justdoc":
              publisher = JustDocPublisher(auth, config.get("justdoc", {}))
          else:
              raise ValueError(f"ä¸æ”¯æŒçš„å¹³å°: {platform}")

          # å‡†å¤‡å‘å¸ƒ
          prepare_result = publisher.prepare(content_dir, target_repo)
          if not prepare_result["success"]:
              return {
                  "success": False,
                  "error": prepare_result["error"],
                  "details": prepare_result.get("details", {})
              }

          # æ‰§è¡Œå‘å¸ƒ
          publish_result = publisher.publish()
          if not publish_result["success"]:
              return {
                  "success": False,
                  "error": publish_result["error"],
                  "details": publish_result.get("details", {})
              }

          # éªŒè¯å‘å¸ƒç»“æœ
          verify_result = publisher.verify()

          return {
              "success": True,
              "url": publish_result["url"],
              "details": {
                  "platform": platform,
                  "target": target_repo,
                  "verification": verify_result
              }
          }
      ```

### è¾…åŠ©å‡½æ•°

20. **`detect_natural_language(text)`** (`utils/language_utils.py`)
    - _è¾“å…¥_: æ–‡æœ¬ (str)
    - _è¾“å‡º_: æ£€æµ‹åˆ°çš„è‡ªç„¶è¯­è¨€ (str), ç½®ä¿¡åº¦ (float)
    - _å¿…è¦æ€§_: æ£€æµ‹æ³¨é‡Šã€æ–‡æ¡£ç­‰ä¸­ä½¿ç”¨çš„è‡ªç„¶è¯­è¨€ï¼Œæ”¯æŒå¤šè¯­è¨€å¤„ç†ã€‚
    - _å®ç°å»ºè®®_: ä½¿ç”¨è¯­è¨€æ£€æµ‹åº“å¦‚ langdetect æˆ– fastTextã€‚

21. **`generate_visualization(data, vis_type, options=None)`** (`utils/visualizer.py`)
    - _è¾“å…¥_: æ•°æ® (dict/list), å¯è§†åŒ–ç±»å‹ (str), é€‰é¡¹ (dict)
    - _è¾“å‡º_: å¯è§†åŒ–ä»£ç  (str), æ ¼å¼ (str)
    - _å¿…è¦æ€§_: ç”Ÿæˆä¸°å¯Œçš„å¯è§†åŒ–å›¾è¡¨ï¼Œæé«˜æ–‡æ¡£çš„å¯è¯»æ€§å’Œç†è§£æ€§ã€‚
    - _æ”¯æŒçš„å¯è§†åŒ–ç±»å‹_:
      - **æ¶æ„å›¾**: ä½¿ç”¨ Mermaid ç”Ÿæˆæ¶æ„å›¾
      - **ä¾èµ–å…³ç³»å›¾**: ä½¿ç”¨ Mermaid æˆ– D3.js ç”Ÿæˆä¾èµ–å…³ç³»å›¾
      - **æ—¶åºå›¾**: å±•ç¤ºä»£ç æ‰§è¡Œæµç¨‹æˆ–ç³»ç»Ÿäº¤äº’
      - **çŠ¶æ€å›¾**: å±•ç¤ºçŠ¶æ€è½¬æ¢å’Œç”Ÿå‘½å‘¨æœŸ
      - **ç±»å›¾**: å±•ç¤ºç±»ä¹‹é—´çš„å…³ç³»
      - **æµç¨‹å›¾**: å±•ç¤ºç®—æ³•æˆ–ä¸šåŠ¡æµç¨‹
      - **ç”˜ç‰¹å›¾**: å±•ç¤ºé¡¹ç›®æ—¶é—´çº¿å’Œé‡Œç¨‹ç¢‘
      - **äº¤äº’å¼å›¾è¡¨**: ä½¿ç”¨ Plotly æˆ– Vega-Lite ç”Ÿæˆäº¤äº’å¼å›¾è¡¨
    - _å®ç°ç¤ºä¾‹_:
      ```python
      def generate_visualization(data, vis_type, options=None):
          """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨

          Args:
              data: å¯è§†åŒ–æ•°æ®
              vis_type: å¯è§†åŒ–ç±»å‹ï¼Œå¦‚ "architecture", "dependency", "sequence", "state", "class", "flow", "gantt", "interactive"
              options: å¯è§†åŒ–é€‰é¡¹

          Returns:
              (visualization_code, format): å¯è§†åŒ–ä»£ç å’Œæ ¼å¼
          """
          options = options or {}

          # è®¾ç½®é»˜è®¤é€‰é¡¹
          default_options = {
              "theme": "default",
              "direction": "TB",  # æ–¹å‘: TB, BT, LR, RL
              "include_legend": True,
              "max_depth": 3,  # ä¾èµ–å›¾æœ€å¤§æ·±åº¦
              "highlight_nodes": [],  # é«˜äº®èŠ‚ç‚¹
              "width": 800,
              "height": 600
          }

          # åˆå¹¶é€‰é¡¹
          for key, value in default_options.items():
              if key not in options:
                  options[key] = value

          # æ ¹æ®å¯è§†åŒ–ç±»å‹ç”Ÿæˆä»£ç 
          if vis_type == "architecture":
              return generate_architecture_diagram(data, options), "mermaid"

          elif vis_type == "dependency":
              if options.get("interactive", False):
                  return generate_interactive_dependency(data, options), "html"
              else:
                  return generate_mermaid_dependency(data, options), "mermaid"

          elif vis_type == "sequence":
              return generate_sequence_diagram(data, options), "mermaid"

          elif vis_type == "state":
              return generate_state_diagram(data, options), "mermaid"

          elif vis_type == "class":
              return generate_class_diagram(data, options), "mermaid"

          elif vis_type == "flow":
              return generate_flow_diagram(data, options), "mermaid"

          elif vis_type == "gantt":
              return generate_gantt_chart(data, options), "mermaid"

          elif vis_type == "interactive":
              chart_type = options.get("chart_type", "bar")
              if chart_type in ["bar", "line", "pie", "scatter"]:
                  return generate_plotly_chart(data, chart_type, options), "html"
              else:
                  return generate_vega_lite_chart(data, chart_type, options), "html"

          else:
              raise ValueError(f"ä¸æ”¯æŒçš„å¯è§†åŒ–ç±»å‹: {vis_type}")

      def generate_architecture_diagram(architecture_data, options):
          """ç”Ÿæˆæ¶æ„å›¾"""
          direction = options.get("direction", "TB")
          theme = options.get("theme", "default")

          # ç”Ÿæˆ Mermaid ä»£ç 
          mermaid_code = [f"graph {direction}"]

          # æ·»åŠ ä¸»é¢˜
          if theme != "default":
              mermaid_code.append(f"    %% theme: {theme}")

          # æ·»åŠ èŠ‚ç‚¹
          for node in architecture_data.get("nodes", []):
              node_id = node.get("id")
              node_label = node.get("label", node_id)
              node_type = node.get("type", "default")

              # æ ¹æ®èŠ‚ç‚¹ç±»å‹è®¾ç½®å½¢çŠ¶
              if node_type == "service":
                  shape = "([{label}])"
              elif node_type == "database":
                  shape = "[({label})]"
              elif node_type == "external":
                  shape = ">{label}]"
              elif node_type == "component":
                  shape = "{{{label}}}"
              else:
                  shape = "[{label}]"

              # æ›¿æ¢æ ‡ç­¾
              shape = shape.replace("{label}", node_label)

              # æ·»åŠ èŠ‚ç‚¹å®šä¹‰
              mermaid_code.append(f"    {node_id}{shape}")

              # æ·»åŠ æ ·å¼
              if node.get("style"):
                  mermaid_code.append(f"    style {node_id} {node.get('style')}")

              # é«˜äº®èŠ‚ç‚¹
              if node_id in options.get("highlight_nodes", []):
                  mermaid_code.append(f"    style {node_id} fill:#f96,stroke:#333,stroke-width:2px")

          # æ·»åŠ è¿æ¥
          for edge in architecture_data.get("edges", []):
              source = edge.get("source")
              target = edge.get("target")
              label = edge.get("label", "")
              edge_type = edge.get("type", "default")

              # æ ¹æ®è¾¹ç±»å‹è®¾ç½®çº¿æ¡æ ·å¼
              if edge_type == "async":
                  line = "-.->"
              elif edge_type == "bidirectional":
                  line = "<-->"
              elif edge_type == "dependency":
                  line = "-..->"
              else:
                  line = "-->"

              # æ·»åŠ è¾¹å®šä¹‰
              if label:
                  mermaid_code.append(f"    {source} {line}|{label}| {target}")
              else:
                  mermaid_code.append(f"    {source} {line} {target}")

          # æ·»åŠ å›¾ä¾‹
          if options.get("include_legend", True):
              mermaid_code.append("    %% å›¾ä¾‹")
              mermaid_code.append("    subgraph å›¾ä¾‹")
              mermaid_code.append("        legend_service([æœåŠ¡])")
              mermaid_code.append("        legend_db[(æ•°æ®åº“)]")
              mermaid_code.append("        legend_component{{ç»„ä»¶}}")
              mermaid_code.append("        legend_external>å¤–éƒ¨ç³»ç»Ÿ]")
              mermaid_code.append("    end")

          return "\n".join(mermaid_code)
      ```

22. **`extract_technical_terms(text, domain=None, language=None)`** (`utils/language_utils.py`)
    - _è¾“å…¥_: æ–‡æœ¬ (str), é¢†åŸŸ (str), è¯­è¨€ (str)
    - _è¾“å‡º_: æå–çš„æŠ€æœ¯æœ¯è¯­åˆ—è¡¨ (list of str)
    - _å¿…è¦æ€§_: è¯†åˆ«éœ€è¦åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­ä¿ç•™çš„æŠ€æœ¯æœ¯è¯­ã€‚
    - _å®ç°å»ºè®®_: ç»“åˆè§„åˆ™å’Œ NLP æŠ€æœ¯è¯†åˆ«ä¸“ä¸šæœ¯è¯­ã€‚
    - _å¤šè¯­è¨€æ”¯æŒ_: æ”¯æŒå¤šç§è¯­è¨€çš„æœ¯è¯­è¯†åˆ«ï¼ŒåŒ…æ‹¬è‹±æ–‡ã€ä¸­æ–‡ç­‰ã€‚
    - _å®ç°ç¤ºä¾‹_:
      ```python
      def extract_technical_terms(text, domain=None, language=None):
          """æå–æŠ€æœ¯æœ¯è¯­

          Args:
              text: è¾“å…¥æ–‡æœ¬
              domain: é¢†åŸŸï¼Œå¦‚ "python", "web", "machine_learning" ç­‰
              language: è¯­è¨€ï¼Œå¦‚ "en", "zh" ç­‰ï¼Œå¦‚æœä¸º None åˆ™è‡ªåŠ¨æ£€æµ‹

          Returns:
              æŠ€æœ¯æœ¯è¯­åˆ—è¡¨
          """
          # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
          if language is None:
              language, _ = detect_natural_language(text)

          # åŠ è½½é¢†åŸŸç‰¹å®šçš„æœ¯è¯­åº“
          domain_terms = []
          if domain:
              domain_file = os.path.join(
                  os.path.dirname(__file__),
                  "data",
                  "terms",
                  f"{domain}_{language}.txt"
              )
              if os.path.exists(domain_file):
                  with open(domain_file, "r", encoding="utf-8") as f:
                      domain_terms = [line.strip() for line in f if line.strip()]

          # åŸºäºè§„åˆ™çš„æœ¯è¯­æå–
          terms = []

          # è‹±æ–‡æœ¯è¯­æå–
          if language == "en":
              # æå–å¤§å†™ç¼©å†™è¯
              abbr_pattern = r'\b[A-Z][A-Z0-9]{1,5}\b'
              abbreviations = re.findall(abbr_pattern, text)
              terms.extend(abbreviations)

              # æå–é©¼å³°å‘½åçš„æœ¯è¯­
              camel_pattern = r'\b[A-Z][a-z]+(?:[A-Z][a-z]+)+\b'
              camel_terms = re.findall(camel_pattern, text)
              terms.extend(camel_terms)

              # æå–ä¸‹åˆ’çº¿è¿æ¥çš„æœ¯è¯­
              underscore_pattern = r'\b[a-z]+(?:_[a-z]+)+\b'
              underscore_terms = re.findall(underscore_pattern, text)
              terms.extend(underscore_terms)

          # ä¸­æ–‡æœ¯è¯­æå–
          elif language == "zh":
              # æå–è‹±æ–‡æœ¯è¯­ï¼ˆåœ¨ä¸­æ–‡æ–‡æœ¬ä¸­ï¼‰
              en_pattern = r'[a-zA-Z][a-zA-Z0-9_]*(?:\.[a-zA-Z][a-zA-Z0-9_]*)*'
              en_terms = re.findall(en_pattern, text)
              terms.extend(en_terms)

              # æå–ä¸­æ–‡æœ¯è¯­ï¼ˆåŸºäºå¸¸è§æ¨¡å¼ï¼‰
              zh_patterns = [
                  r'[\u4e00-\u9fa5]+æ¡†æ¶',
                  r'[\u4e00-\u9fa5]+åº“',
                  r'[\u4e00-\u9fa5]+ç®—æ³•',
                  r'[\u4e00-\u9fa5]+æ¨¡å‹',
                  r'[\u4e00-\u9fa5]+åè®®'
              ]
              for pattern in zh_patterns:
                  zh_terms = re.findall(pattern, text)
                  terms.extend(zh_terms)

          # æ·»åŠ é¢†åŸŸç‰¹å®šæœ¯è¯­
          for term in domain_terms:
              if term in text and term not in terms:
                  terms.append(term)

          # å»é‡å¹¶æ’åº
          unique_terms = list(set(terms))
          unique_terms.sort(key=len, reverse=True)

          return unique_terms
      ```

23. **`log_and_notify(message, level='info', notify=False)`** (`utils/logger.py`)
    - _è¾“å…¥_: æ¶ˆæ¯ (str), æ—¥å¿—çº§åˆ« (str), æ˜¯å¦é€šçŸ¥ç”¨æˆ· (bool)
    - _è¾“å‡º_: æ— 
    - _å¿…è¦æ€§_: ç»Ÿä¸€çš„æ—¥å¿—è®°å½•å’Œç”¨æˆ·é€šçŸ¥æœºåˆ¶ã€‚
    - _å®ç°å»ºè®®_: ä½¿ç”¨æ ‡å‡†æ—¥å¿—åº“ï¼Œç»“åˆç”¨æˆ·ç•Œé¢é€šçŸ¥åŠŸèƒ½ã€‚

24. **`parallel_process(items, process_func, max_workers=None, chunk_size=1, show_progress=True)`** (`utils/performance.py`)
    - _è¾“å…¥_: å¾…å¤„ç†é¡¹åˆ—è¡¨ (list), å¤„ç†å‡½æ•° (callable), æœ€å¤§å·¥ä½œçº¿ç¨‹æ•° (int), åˆ†å—å¤§å° (int), æ˜¯å¦æ˜¾ç¤ºè¿›åº¦ (bool)
    - _è¾“å‡º_: å¤„ç†ç»“æœåˆ—è¡¨ (list)
    - _å¿…è¦æ€§_: æä¾›é«˜æ•ˆçš„å¹¶è¡Œå¤„ç†èƒ½åŠ›ï¼ŒåŠ é€Ÿå¤§å‹ä»£ç åº“çš„åˆ†æå’Œå†…å®¹ç”Ÿæˆã€‚
    - _å®ç°å»ºè®®_: ä½¿ç”¨ `concurrent.futures` åº“å®ç°çº¿ç¨‹æ± æˆ–è¿›ç¨‹æ± ã€‚
    - _é”™è¯¯å¤„ç†_: æ•è·å¹¶è®°å½•å•ä¸ªé¡¹å¤„ç†å¤±è´¥ï¼Œä¸å½±å“æ•´ä½“æµç¨‹ã€‚
    - _è¿›åº¦è·Ÿè¸ª_: æ”¯æŒå®æ—¶è¿›åº¦æ˜¾ç¤ºï¼Œæä¾› ETA ä¼°è®¡ã€‚
    - _å®ç°ç¤ºä¾‹_:
      ```python
      def parallel_process(items, process_func, max_workers=None, chunk_size=1, show_progress=True):
          """å¹¶è¡Œå¤„ç†é¡¹ç›®åˆ—è¡¨

          Args:
              items: å¾…å¤„ç†é¡¹åˆ—è¡¨
              process_func: å¤„ç†å‡½æ•°ï¼Œæ¥å—å•ä¸ªé¡¹ä½œä¸ºè¾“å…¥
              max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹/è¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸º CPU æ ¸å¿ƒæ•° * 2
              chunk_size: æ¯ä¸ªå·¥ä½œå•å…ƒçš„é¡¹ç›®æ•°
              show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡

          Returns:
              å¤„ç†ç»“æœåˆ—è¡¨
          """
          if max_workers is None:
              max_workers = min(32, os.cpu_count() * 2)

          results = []
          total = len(items)
          processed = 0
          errors = []

          # åˆ›å»ºè¿›åº¦æ¡
          if show_progress:
              from tqdm import tqdm
              progress_bar = tqdm(total=total, desc="Processing", unit="item")

          # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
          with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
              # æäº¤æ‰€æœ‰ä»»åŠ¡
              future_to_item = {
                  executor.submit(process_func, item): i
                  for i, item in enumerate(items)
              }

              # å¤„ç†å®Œæˆçš„ä»»åŠ¡
              for future in concurrent.futures.as_completed(future_to_item):
                  item_index = future_to_item[future]
                  try:
                      result = future.result()
                      results.append((item_index, result))
                  except Exception as e:
                      errors.append((item_index, str(e)))
                      log_and_notify(
                          f"å¤„ç†é¡¹ {item_index} å¤±è´¥: {str(e)}",
                          level="error"
                      )

                  # æ›´æ–°è¿›åº¦
                  processed += 1
                  if show_progress:
                      progress_bar.update(1)

          # å…³é—­è¿›åº¦æ¡
          if show_progress:
              progress_bar.close()

          # è®°å½•é”™è¯¯ç»Ÿè®¡
          if errors:
              log_and_notify(
                  f"å¹¶è¡Œå¤„ç†å®Œæˆï¼Œå…± {len(errors)}/{total} é¡¹å¤±è´¥",
                  level="warning"
              )

          # æŒ‰åŸå§‹é¡ºåºæ’åºç»“æœ
          sorted_results = [r[1] for r in sorted(results, key=lambda x: x[0])]
          return sorted_results
      ```

25. **`optimize_cache_strategy(cache_dir, ttl=86400, max_size_gb=5, priority_func=None)`** (`utils/cache_manager.py`)
    - _è¾“å…¥_: ç¼“å­˜ç›®å½• (str), ç¼“å­˜æœ‰æ•ˆæœŸ (int, ç§’), æœ€å¤§ç¼“å­˜å¤§å° (float, GB), ä¼˜å…ˆçº§å‡½æ•° (callable)
    - _è¾“å‡º_: ä¼˜åŒ–åçš„ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ (dict)
    - _å¿…è¦æ€§_: æ™ºèƒ½ç®¡ç†ç¼“å­˜ï¼Œæé«˜ç³»ç»Ÿæ€§èƒ½ï¼Œé¿å…é‡å¤è®¡ç®—å’Œ API è°ƒç”¨ã€‚
    - _å®ç°å»ºè®®_: ä½¿ç”¨ LRU (æœ€è¿‘æœ€å°‘ä½¿ç”¨) ç­–ç•¥ç»“åˆè‡ªå®šä¹‰ä¼˜å…ˆçº§ã€‚
    - _ç¼“å­˜ç±»å‹_: æ”¯æŒå¤šç§ç¼“å­˜ç±»å‹ (LLM è°ƒç”¨ã€åµŒå…¥å‘é‡ã€è§£æç»“æœç­‰)ã€‚
    - _å®ç°ç¤ºä¾‹_:
      ```python
      def optimize_cache_strategy(cache_dir, ttl=86400, max_size_gb=5, priority_func=None):
          """ä¼˜åŒ–ç¼“å­˜ç­–ç•¥

          Args:
              cache_dir: ç¼“å­˜ç›®å½•
              ttl: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
              max_size_gb: æœ€å¤§ç¼“å­˜å¤§å°ï¼ˆGBï¼‰
              priority_func: è‡ªå®šä¹‰ä¼˜å…ˆçº§å‡½æ•°ï¼Œæ¥å—ç¼“å­˜é¡¹å…ƒæ•°æ®ï¼Œè¿”å›ä¼˜å…ˆçº§åˆ†æ•°

          Returns:
              ä¼˜åŒ–åçš„ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
          """
          # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
          os.makedirs(cache_dir, exist_ok=True)

          # é»˜è®¤ä¼˜å…ˆçº§å‡½æ•° - åŸºäºè®¿é—®æ—¶é—´å’Œä½¿ç”¨é¢‘ç‡
          if priority_func is None:
              def priority_func(metadata):
                  # è®¡ç®—åŸºäºæ—¶é—´çš„ä¼˜å…ˆçº§ (0-1)
                  age = time.time() - metadata.get("last_access", 0)
                  age_factor = min(1.0, age / ttl)

                  # ä½¿ç”¨é¢‘ç‡å› å­ (0-1)
                  freq = metadata.get("access_count", 0)
                  freq_factor = 1.0 / (1.0 + math.log(1 + freq))

                  # å¤§å°å› å­ (0-1)
                  size = metadata.get("size", 0) / (1024 * 1024)  # MB
                  size_factor = min(1.0, size / 100)  # 100MB ä½œä¸ºå‚è€ƒç‚¹

                  # ç»¼åˆä¼˜å…ˆçº§ (è¶Šé«˜è¶Šå…ˆè¢«æ¸…é™¤)
                  return 0.5 * age_factor + 0.3 * freq_factor + 0.2 * size_factor

          # æ‰«æç¼“å­˜ç›®å½•
          cache_items = []
          total_size = 0

          for root, _, files in os.walk(cache_dir):
              for file in files:
                  if file.endswith(".metadata"):
                      continue

                  file_path = os.path.join(root, file)
                  metadata_path = file_path + ".metadata"

                  # è·å–æ–‡ä»¶å¤§å°
                  try:
                      size = os.path.getsize(file_path)
                      total_size += size

                      # è¯»å–å…ƒæ•°æ®
                      metadata = {}
                      if os.path.exists(metadata_path):
                          with open(metadata_path, "r") as f:
                              metadata = json.load(f)

                      # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
                      metadata["path"] = file_path
                      metadata["size"] = size

                      # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                      is_expired = False
                      if "created_at" in metadata:
                          age = time.time() - metadata["created_at"]
                          is_expired = age > ttl

                      cache_items.append({
                          "path": file_path,
                          "metadata": metadata,
                          "is_expired": is_expired,
                          "priority": priority_func(metadata)
                      })
                  except Exception as e:
                      log_and_notify(f"å¤„ç†ç¼“å­˜é¡¹ {file_path} å¤±è´¥: {str(e)}", "warning")

          # è½¬æ¢ä¸º GB
          total_size_gb = total_size / (1024 * 1024 * 1024)

          # æ¸…ç†è¿‡æœŸé¡¹
          expired_items = [item for item in cache_items if item["is_expired"]]
          for item in expired_items:
              try:
                  os.remove(item["path"])
                  metadata_path = item["path"] + ".metadata"
                  if os.path.exists(metadata_path):
                      os.remove(metadata_path)
              except Exception as e:
                  log_and_notify(f"åˆ é™¤è¿‡æœŸç¼“å­˜é¡¹ {item['path']} å¤±è´¥: {str(e)}", "warning")

          # å¦‚æœç¼“å­˜ä»ç„¶è¶…è¿‡æœ€å¤§å¤§å°ï¼Œæ¸…ç†ä¼˜å…ˆçº§æœ€é«˜çš„é¡¹
          if total_size_gb > max_size_gb:
              # æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆé™åºï¼‰
              remaining_items = [item for item in cache_items if not item["is_expired"]]
              remaining_items.sort(key=lambda x: x["priority"], reverse=True)

              # è®¡ç®—éœ€è¦æ¸…ç†çš„å¤§å°
              size_to_free = total_size - (max_size_gb * 1024 * 1024 * 1024)
              freed_size = 0

              for item in remaining_items:
                  if freed_size >= size_to_free:
                      break

                  try:
                      os.remove(item["path"])
                      metadata_path = item["path"] + ".metadata"
                      if os.path.exists(metadata_path):
                          os.remove(metadata_path)

                      freed_size += item["metadata"]["size"]
                  except Exception as e:
                      log_and_notify(f"åˆ é™¤ç¼“å­˜é¡¹ {item['path']} å¤±è´¥: {str(e)}", "warning")

          # è¿”å›ä¼˜åŒ–åçš„ç»Ÿè®¡ä¿¡æ¯
          return {
              "total_items_before": len(cache_items),
              "expired_items_removed": len(expired_items),
              "size_before_gb": total_size_gb,
              "size_after_gb": (total_size - freed_size) / (1024 * 1024 * 1024),
              "ttl_seconds": ttl,
              "max_size_gb": max_size_gb
          }
      ```

## ğŸ§  èŠ‚ç‚¹è®¾è®¡ (Node Design)

> AI æç¤º: èŠ‚ç‚¹å®ç°åº”åŠ›æ±‚**ç®€æ´é«˜æ•ˆ**ï¼

### ğŸ’¾ å…±äº«å†…å­˜ (Shared Memory)

> AI æç¤º: å°½é‡å‡å°‘æ•°æ®å†—ä½™ï¼Œå¯å¢åŠ  AI ä¸­é—´ç»“æœå­—æ®µã€‚

å…±äº«å†…å­˜ç»“æ„ç»„ç»‡å¦‚ä¸‹:

```python
shared = {
    # è¾“å…¥å‚æ•°
    "repo_source": None,        # ç»Ÿä¸€çš„ä»£ç åº“æ¥æº (URL æˆ–æœ¬åœ°è·¯å¾„) (str)
    "user_query": None,         # ç”¨æˆ·äº¤äº’å¼é—®é¢˜ (str, å¯é€‰)
    "target_language": "en",    # ç›®æ ‡æ•™ç¨‹è¯­è¨€ ('en', 'zh') (str)
    "output_format": "markdown", # è¾“å‡ºæ ¼å¼ ('markdown', 'pdf') (str)
    "publish_target": None,     # å‘å¸ƒç›®æ ‡å¹³å° ('github', 'gitlab', None) (str)
    "publish_repo": None,       # å‘å¸ƒç›®æ ‡ä»“åº“ä¿¡æ¯ (str, å¯é€‰)
    "auth_info": None,          # å‘å¸ƒæ‰€éœ€çš„è®¤è¯ä¿¡æ¯ (dict/object, å¯é€‰)

    # å¤„ç†åçš„ä»£ç åº“è·¯å¾„å’ŒåŸºæœ¬ä¿¡æ¯
    "local_repo_path": None,    # ç»è¿‡å¤„ç†å’ŒéªŒè¯çš„æœ¬åœ°ä»£ç åº“è·¯å¾„ (str)
    "repo_size_info": None,     # ä»£ç åº“å¤§å°ç»Ÿè®¡ä¿¡æ¯ (dict)
    "detected_languages": None, # æ£€æµ‹åˆ°çš„ç¼–ç¨‹è¯­è¨€ {file_path: {"language": lang, "confidence": conf}}

    # å¤„ç†çŠ¶æ€å’Œé”™è¯¯è·Ÿè¸ª
    "process_status": {
        "current_stage": None,  # å½“å‰å¤„ç†é˜¶æ®µ (str)
        "progress": 0.0,        # æ€»ä½“è¿›åº¦ (float, 0.0-1.0)
        "stage_progress": {},   # å„é˜¶æ®µè¿›åº¦ {stage_name: progress_float}
        "errors": [],           # é”™è¯¯è®°å½• [{"stage": stage, "error": err, "timestamp": time}]
        "warnings": []          # è­¦å‘Šè®°å½• [{"stage": stage, "warning": warn, "timestamp": time}]
    },

    # åˆ†æé˜¶æ®µäº§å‡º
    "code_structure": None,     # è§£æ: åŸºç¡€ä»£ç ç»“æ„ (AST, ç­¾å, åŸå§‹æ³¨é‡Š)
    "dependencies": None,       # è§£æ: æ–‡ä»¶/æ¨¡å—é—´ä¾èµ–
    "raw_comments": None,       # è§£æ: æå–çš„åŸå§‹æ³¨é‡Š
    "commit_history": None,     # è§£æ: Commit å†å²è®°å½•
    "text_chunks": None,        # RAG: æ–‡æœ¬å— (ä»£ç , æ³¨é‡Š, å¯èƒ½å« AI æ‘˜è¦)
    "embeddings": None,         # RAG: åµŒå…¥å‘é‡
    "vector_index": None,       # RAG: å‘é‡ç´¢å¼•
    "cache": {                  # ç¼“å­˜æ•°æ®
        "llm_calls": {},        # LLM è°ƒç”¨ç¼“å­˜ {hash(prompt+context): response}
        "embeddings": {},       # åµŒå…¥ç¼“å­˜ {hash(text): embedding}
        "parsed_files": {}      # è§£æç¼“å­˜ {file_path: parsed_result}
    },

    # AI åˆ†æ/ç†è§£äº§å‡º (æ–°å¢æˆ–æ•´åˆ)
    "ai_analysis": {
        "core_modules_explanation": None, # AI: å¯¹æ ¸å¿ƒæ¨¡å—çš„è¯†åˆ«å’Œè§£é‡Š
        "function_summaries": {},       # AI: (å¯é€‰) å¯¹å…³é”®å‡½æ•°çš„æ‘˜è¦ {func_id: summary}
        "overall_architecture_summary": None, # AI: å¯¹æ•´ä½“æ¶æ„çš„ç†è§£æ‘˜è¦
        "quality_metrics": {            # AI ç”Ÿæˆå†…å®¹çš„è´¨é‡è¯„ä¼°
            "architecture_understanding": None, # æ¶æ„ç†è§£è´¨é‡è¯„åˆ† (float)
            "code_coverage": None,      # ä»£ç è¦†ç›–ç‡è¯„åˆ† (float)
            "explanation_clarity": None # è§£é‡Šæ¸…æ™°åº¦è¯„åˆ† (float)
        },
        "technical_terms": [],          # è¯†åˆ«çš„æŠ€æœ¯æœ¯è¯­åˆ—è¡¨
        # ... å…¶ä»– AI ç†è§£çš„ä¸­é—´ç»“æœ
    },

    # AI å†…å®¹ç”Ÿæˆé˜¶æ®µäº§å‡º (æ›´ä¾èµ– AI, éœ€ä½“ç°å±‚çº§ç»“æ„)
    "generated_content": {
        # æ•´ä½“éƒ¨åˆ†
        "overall_architecture_diagram": None, # å¯è§†åŒ–: Mermaid æ¶æ„å›¾
        "overall_architecture_explanation": None,# AI: å¯¹æ•´ä½“æ¶æ„çš„è§£é‡Š
        "core_modules_summary": None,      # AI: æ ¸å¿ƒæ¨¡å—åˆ—è¡¨åŠèŒè´£æ¦‚è§ˆ
        "evolution_narrative": None, # AI: å¯¹ä»£ç æ¼”å˜å†å²çš„è§£è¯»å’Œå™è¿°
        "dependency_overview_graph": None, # å¯è§†åŒ–: é¡¶å±‚ä¾èµ–å›¾
        "dependency_overview_explanation": None,# AI: å¯¹é¡¶å±‚ä¾èµ–çš„è§£é‡Š
        "glossary": None,           # AI: ç”Ÿæˆçš„æœ¯è¯­è§£é‡Š
        "quick_look": None,         # AI: ç”Ÿæˆçš„äº”åˆ†é’Ÿé€Ÿè§ˆå†…å®¹

        # åˆ†æ¨¡å—ç»†èŠ‚ (ä¾‹å¦‚ï¼Œä½¿ç”¨å­—å…¸å­˜å‚¨ï¼Œé”®ä¸ºæ¨¡å—å)
        "module_details": {
            # "module_name_A": {
            #    "description": "AIç”Ÿæˆçš„æ¨¡å—è¯¦ç»†æè¿°",
            #    "api_description": "AIç”Ÿæˆçš„è¯¥æ¨¡å—APIè¯´æ˜",
            #    "code_examples": [{"snippet": "...", "explanation": "..."}],
            #    "internal_dependencies": "AIè§£é‡Šçš„æ¨¡å—å†…éƒ¨ä¾èµ–æˆ–ä¸å…¶ä»–æ¨¡å—å…³ç³»",
            #    "quality_score": 0.0  # å†…å®¹è´¨é‡è¯„åˆ† (float, 0.0-1.0)
            # },
            # "module_name_B": { ... }
        },

        # äº¤äº’å¼é—®ç­”ç»“æœ
        "custom_answers": [],       # Agent/RAG + AI: å›ç­”ç”¨æˆ·é—®é¢˜çš„ç»“æœ
        "user_feedback": [],        # ç”¨æˆ·å¯¹å›ç­”çš„åé¦ˆ [{"question_id": id, "rating": rating, "comment": comment}]
    },

    # è¾“å‡ºé˜¶æ®µäº§å‡º
    "final_tutorial_markdown": None, # æœ€ç»ˆç»„åˆå’Œç¿»è¯‘åçš„ Markdown æ–‡æœ¬ (str)
    "output_file_path": None,      # ç”Ÿæˆçš„æœ¬åœ°æ–‡ä»¶è·¯å¾„ (str)
    "publish_url": None,           # å‘å¸ƒåçš„ URL (str, å¯é€‰)
    "gh_pages_config": None,       # GitHub Pages é…ç½®ä¿¡æ¯ (dict)
    "validation_results": {        # è¾“å‡ºéªŒè¯ç»“æœ
        "markdown_validation": None, # Markdown éªŒè¯ç»“æœ (dict)
        "link_validation": None,   # é“¾æ¥éªŒè¯ç»“æœ (dict)
        "publish_validation": None # å‘å¸ƒéªŒè¯ç»“æœ (dict)
    }
}
```

### ğŸ“ èŠ‚ç‚¹è®¾è®¡ä¸å®ç°é˜¶æ®µ

#### èŠ‚ç‚¹ä¾èµ–å…³ç³»å›¾

ä¸‹å›¾å±•ç¤ºäº†ä¸»è¦èŠ‚ç‚¹ä¹‹é—´çš„ä¾èµ–å…³ç³»å’Œæ•°æ®æµåŠ¨ï¼š

```mermaid
graph TD
    Input[InputNode] --> PrepareRepo[PrepareRepoNode]
    PrepareRepo --> AnalyzeRepo[AnalyzeRepoFlow]

    subgraph AnalyzeRepoFlow
        ParseCode[ParseCodeBatchNode] --> AIUnderstand[AIUnderstandCoreModulesNode]
        ParseCode --> AnalyzeHistory[AnalyzeHistoryNode]
        AIUnderstand --> PrepareRAG[PrepareRAGDataNode]
        AnalyzeHistory --> PrepareRAG
    end

    AnalyzeRepo --> GenerateContent[GenerateContentFlow]

    subgraph GenerateContentFlow
        GenOverall[GenerateOverallArchitectureNode] --> QualityCheck1[ContentQualityCheckNode]
        GenAPI[GenerateApiDocsNode] --> QualityCheck1
        GenTimeline[GenerateTimelineNode] --> QualityCheck1
        GenDependency[GenerateDependencyNode] --> QualityCheck1
        GenGlossary[GenerateGlossaryNode] --> QualityCheck1
        GenQuickLook[GenerateQuickLookNode] --> QualityCheck1

        QualityCheck1 --> GenModules[GenerateModuleDetailsNode]
        GenModules --> QualityCheck2[ModuleQualityCheckNode]
    end

    GenerateContent --> CombineTranslate[CombineAndTranslateNode]
    CombineTranslate --> FormatOutput[FormatOutputNode]
    FormatOutput --> InteractiveQA[InteractiveQANode]
    InteractiveQA --> Publish[PublishNode]
```

#### èŠ‚ç‚¹é˜¶æ®µåˆ’åˆ†æ¦‚è§ˆ (Node Allocation Overview)

ä¸‹è¡¨æ¦‚è¿°äº†ä¸»è¦èŠ‚ç‚¹/æµç¨‹åœ¨å„å®ç°é˜¶æ®µä¸­çš„å¼•å…¥æˆ–å…³é”®å¢å¼ºï¼š

| èŠ‚ç‚¹/æµç¨‹ (Node/Flow)           | ä¸»è¦å®ç°é˜¶æ®µ | å¯¹åº”æµç¨‹é˜¶æ®µ | é”™è¯¯å¤„ç†ç­–ç•¥ | å¯æ‰©å±•æ€§è®¾è®¡ |
| :------------------------------ | :------------ | :------------ | :----------- | :----------- |
| `InputNode`                     | é˜¶æ®µ 1        | ğŸ·ï¸ 1: è¾“å…¥ä¸å‡†å¤‡ | è¾“å…¥éªŒè¯ï¼Œæä¾›é»˜è®¤å€¼ | æ”¯æŒè‡ªå®šä¹‰å‚æ•°æ‰©å±• |
| `PrepareRepoNode`               | é˜¶æ®µ 1        | ğŸ·ï¸ 1: è¾“å…¥ä¸å‡†å¤‡ | å¤„ç†ç½‘ç»œé”™è¯¯ï¼Œæƒé™é—®é¢˜ | æ”¯æŒå¤šç§ä»£ç åº“æ¥æº |
| `AnalyzeRepoFlow`               | é˜¶æ®µ 2        | ğŸ·ï¸ 2: AI ç†è§£ | åˆå¹¶å¯ç”¨åˆ†æç»“æœ | æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒæ–°åˆ†æå™¨ |
| â†³ `ParseCodeBatchNode`          | é˜¶æ®µ 2        | ğŸ·ï¸ 2.1: ä»£ç è§£æ | é™çº§è§£æï¼Œè·³è¿‡é—®é¢˜æ–‡ä»¶ | æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ |
| â†³ `AIUnderstandCoreModulesNode` | é˜¶æ®µ 2        | ğŸ·ï¸ 2.2: AI æ ¸å¿ƒç†è§£ | LLM è°ƒç”¨é‡è¯•ï¼Œç»“æœéªŒè¯ | å¯é…ç½®ç†è§£æ·±åº¦ï¼Œæ”¯æŒå¤šè¯­è¨€ |
| â†³ `AnalyzeHistoryNode`          | é˜¶æ®µ 2        | ğŸ·ï¸ 2.1: ä»£ç è§£æ | å¤„ç†ç©ºä»“åº“ï¼Œå†å²æˆªæ–­ | æ”¯æŒè¿‡æ»¤å’Œèšç„¦ |
| â†³ `PrepareRAGDataNode`          | é˜¶æ®µ 2        | ğŸ·ï¸ 2.3: RAG æ•°æ®å‡†å¤‡ | å¤„ç†å¤§æ–‡ä»¶ï¼Œä¼˜åŒ–åˆ†å— | å¯é…ç½®ç´¢å¼•ç±»å‹å’Œå‚æ•° |
| `GenerateContentFlow`           | é˜¶æ®µ 3        | ğŸ·ï¸ 3: AI ç”Ÿæˆ | å†…å®¹è´¨é‡æ£€æŸ¥ï¼Œé‡æ–°ç”Ÿæˆ | æ’ä»¶å¼å†…å®¹ç”Ÿæˆå™¨ |
| â†³ `GenerateOverallArchitectureNode` | é˜¶æ®µ 3    | ğŸ·ï¸ 3.1: ç”Ÿæˆæ•´ä½“å†…å®¹ | ç»“æ„éªŒè¯ï¼Œé™çº§ç”Ÿæˆ | æ”¯æŒå¤šç§æ¶æ„è¡¨ç¤ºï¼Œå¢å¼ºå¯è§†åŒ– |
| â†³ `GenerateApiDocsNode`         | é˜¶æ®µ 3        | ğŸ·ï¸ 3.1: ç”Ÿæˆæ•´ä½“å†…å®¹ | API æå–å¤±è´¥å¤„ç† | æ”¯æŒå¤šç§ API é£æ ¼ |
| â†³ `ContentQualityCheckNode`     | é˜¶æ®µ 3        | ğŸ·ï¸ 3.1: ç”Ÿæˆæ•´ä½“å†…å®¹ | è´¨é‡è¯„ä¼°åé¦ˆ | å¯é…ç½®è´¨é‡æ ‡å‡† |
| â†³ `GenerateModuleDetailsNode`   | é˜¶æ®µ 3        | ğŸ·ï¸ 3.2: ç”Ÿæˆæ¨¡å—ç»†èŠ‚ | æ¨¡å—ç¼ºå¤±å¤„ç† | æ”¯æŒè‡ªå®šä¹‰æ¨¡å—æ¨¡æ¿ |
| â†³ `ModuleQualityCheckNode`      | é˜¶æ®µ 3        | ğŸ·ï¸ 3.2: ç”Ÿæˆæ¨¡å—ç»†èŠ‚ | è´¨é‡è¯„ä¼°åé¦ˆ | å¯é…ç½®è´¨é‡æ ‡å‡† |
| â†³ `GenerateTimelineNode`        | é˜¶æ®µ 3        | ğŸ·ï¸ 3.1: ç”Ÿæˆæ•´ä½“å†…å®¹ | å†å²æ•°æ®ä¸è¶³å¤„ç† | æ”¯æŒå¤šç§æ—¶é—´çº¿æ ¼å¼ |
| â†³ `GenerateDependencyNode`      | é˜¶æ®µ 3        | ğŸ·ï¸ 3.1: ç”Ÿæˆæ•´ä½“å†…å®¹ | ä¾èµ–åˆ†æå¤±è´¥å¤„ç† | æ”¯æŒå¤šç§ä¾èµ–å›¾è¡¨ç¤º |
| â†³ `GenerateGlossaryNode`        | é˜¶æ®µ 3        | ğŸ·ï¸ 3.1: ç”Ÿæˆæ•´ä½“å†…å®¹ | æœ¯è¯­æå–å¤±è´¥å¤„ç† | æ”¯æŒé¢†åŸŸç‰¹å®šæœ¯è¯­ |
| â†³ `GenerateQuickLookNode`       | é˜¶æ®µ 3        | ğŸ·ï¸ 3.1: ç”Ÿæˆæ•´ä½“å†…å®¹ | å†…å®¹ä¸è¶³å¤„ç† | å¯é…ç½®æ¦‚è§ˆæ·±åº¦ |
| `CombineAndTranslateNode`       | é˜¶æ®µ 4        | ğŸ·ï¸ 4.1: å†…å®¹ç»„åˆ & 4.2.1: ç¿»è¯‘æ£€æŸ¥ | å†…å®¹ç¼ºå¤±å¤„ç†ï¼Œç¿»è¯‘é”™è¯¯ | æ”¯æŒå¤šè¯­è¨€å’Œè‡ªå®šä¹‰æ¨¡æ¿ï¼Œå¢å¼ºæœ¯è¯­å¤„ç† |
| `FormatOutputNode`              | é˜¶æ®µ 4        | ğŸ·ï¸ 4.2.2: æ ¼å¼åŒ–è¾“å‡º | æ ¼å¼è½¬æ¢é”™è¯¯å¤„ç† | æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ |
| `InteractiveQANode`             | é˜¶æ®µ 5        | ğŸ·ï¸ 5: äº¤äº’é—®ç­” | é—®é¢˜ç†è§£å¤±è´¥ï¼ŒRAG æ£€ç´¢å¤±è´¥ | æ”¯æŒå¤šè½®å¯¹è¯å’Œåé¦ˆ |
| `PublishNode`                   | é˜¶æ®µ 6        | ğŸ·ï¸ 6: å‘å¸ƒ | è®¤è¯å¤±è´¥ï¼Œç½‘ç»œé”™è¯¯ | æ”¯æŒå¤šå¹³å°å‘å¸ƒï¼ŒåŒ…æ‹¬ GitHub Pagesã€GitLab Pagesã€ReadTheDocsã€Netlifyã€Vercelã€Gitbookã€Docsifyã€VuePressã€MkDocs å’Œ JustDoc |

#### æ ¸å¿ƒèŠ‚ç‚¹è¯¦ç»†è®¾è®¡

##### 1. `PrepareRepoNode`

- **ç›®çš„**: å‡†å¤‡æœ¬åœ°ä»£ç åº“ï¼Œå¤„ç† URL æˆ–æœ¬åœ°è·¯å¾„ï¼ŒéªŒè¯æƒé™ï¼Œåˆ†æä»£ç åº“å¤§å°ã€‚
- **è¾“å…¥**: `shared["repo_source"]`
- **è¾“å‡º**: `shared["local_repo_path"]`, `shared["repo_size_info"]`
- **é”™è¯¯å¤„ç†**:
  - å¤„ç†æ— æ•ˆ URL/è·¯å¾„: æä¾›è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼Œå»ºè®®ä¿®å¤æ­¥éª¤
  - å¤„ç†æƒé™é—®é¢˜: è¯·æ±‚å¿…è¦æƒé™ï¼Œæä¾›æ›¿ä»£æ–¹æ¡ˆ
  - å¤„ç†è¶…å¤§ä»£ç åº“: å®ç°åˆ†å‰²ç­–ç•¥ï¼Œæˆ–æä¾›éƒ¨åˆ†åˆ†æé€‰é¡¹
- **å®ç°ç»†èŠ‚ (ä½¿ç”¨ Pydantic)**:
  ```python
  from pydantic import BaseModel, Field, validator, ConfigDict
  from typing import Dict, Optional, Any, List
  from datetime import datetime
  import os
  import re

  # ä½¿ç”¨ Pydantic å®šä¹‰è¾“å…¥æ¨¡å‹
  class PrepareRepoInput(BaseModel):
      """PrepareRepoNode çš„è¾“å…¥æ¨¡å‹"""
      repo_source: str = Field(..., description="ä»£ç åº“æ¥æº (URL æˆ–æœ¬åœ°è·¯å¾„)")

      @validator("repo_source")
      def validate_repo_source(cls, v):
          """éªŒè¯ä»£ç åº“æ¥æºæ ¼å¼"""
          # éªŒè¯ URL æ ¼å¼
          url_pattern = r'^(https?://|git@)[\w.-]+(/[\w.-]+)+(.git)?$'
          # éªŒè¯æœ¬åœ°è·¯å¾„æ ¼å¼
          path_pattern = r'^(/[\w.-]+)+$|^([A-Za-z]:\\[\w.-]+)+$'

          if re.match(url_pattern, v) or re.match(path_pattern, v):
              return v
          raise ValueError("æ— æ•ˆçš„ä»£ç åº“æ¥æºæ ¼å¼ï¼Œå¿…é¡»æ˜¯æœ‰æ•ˆçš„ URL æˆ–æœ¬åœ°è·¯å¾„")

  # ä½¿ç”¨ Pydantic å®šä¹‰è¾“å‡ºæ¨¡å‹
  class RepoSizeInfo(BaseModel):
      """ä»£ç åº“å¤§å°ä¿¡æ¯æ¨¡å‹"""
      total_size: int = Field(..., description="æ€»å¤§å°ï¼ˆå­—èŠ‚ï¼‰")
      file_count: int = Field(..., description="æ–‡ä»¶æ•°é‡")
      language_stats: Dict[str, int] = Field(default_factory=dict, description="å„è¯­è¨€ä»£ç è¡Œæ•°")

      @validator("total_size")
      def validate_total_size(cls, v):
          """éªŒè¯æ€»å¤§å°åœ¨åˆç†èŒƒå›´å†…"""
          if v < 0:
              raise ValueError("æ€»å¤§å°ä¸èƒ½ä¸ºè´Ÿæ•°")
          return v

  class PrepareRepoOutput(BaseModel):
      """PrepareRepoNode çš„è¾“å‡ºæ¨¡å‹"""
      success: bool = Field(..., description="æ“ä½œæ˜¯å¦æˆåŠŸ")
      local_path: Optional[str] = Field(None, description="æœ¬åœ°ä»£ç åº“è·¯å¾„")
      size_info: Optional[RepoSizeInfo] = Field(None, description="ä»£ç åº“å¤§å°ä¿¡æ¯")
      error: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯")
      from_cache: bool = Field(False, description="æ˜¯å¦ä½¿ç”¨äº†ç¼“å­˜")

      @validator("local_path")
      def validate_local_path(cls, v, values):
          """éªŒè¯æœ¬åœ°è·¯å¾„åœ¨æˆåŠŸæ—¶å¿…é¡»å­˜åœ¨"""
          if values.get("success") and (v is None or not os.path.exists(v)):
              raise ValueError("æˆåŠŸçŠ¶æ€ä¸‹å¿…é¡»æä¾›æœ‰æ•ˆçš„æœ¬åœ°è·¯å¾„")
          return v

      @validator("error")
      def validate_error(cls, v, values):
          """éªŒè¯å¤±è´¥æ—¶å¿…é¡»æä¾›é”™è¯¯ä¿¡æ¯"""
          if not values.get("success") and not v:
              raise ValueError("å¤±è´¥çŠ¶æ€ä¸‹å¿…é¡»æä¾›é”™è¯¯ä¿¡æ¯")
          return v

  # èŠ‚ç‚¹å®ç°
  class PrepareRepoNode:
      """å‡†å¤‡ä»£ç åº“èŠ‚ç‚¹ï¼Œä½¿ç”¨ Pydantic è¿›è¡Œè¾“å…¥/è¾“å‡ºéªŒè¯"""

      def __init__(self, config=None):
          self.config = config or {}
          self.max_repo_size = self.config.get("max_repo_size", 100_000_000)  # 100MB
          self.split_threshold = self.config.get("split_threshold", 50_000_000)  # 50MB

      def prep(self, shared):
          """å‡†å¤‡é˜¶æ®µï¼Œä»å…±äº«å†…å­˜ä¸­æå–è¾“å…¥å¹¶éªŒè¯"""
          try:
              # ä½¿ç”¨ Pydantic æ¨¡å‹éªŒè¯è¾“å…¥
              input_model = PrepareRepoInput(repo_source=shared["repo_source"])
              return input_model.repo_source
          except Exception as e:
              # è¾“å…¥éªŒè¯å¤±è´¥ï¼Œè®°å½•é”™è¯¯
              log_and_notify(f"è¾“å…¥éªŒè¯å¤±è´¥: {str(e)}", "error")
              return None

      def validate_source(self, repo_source):
          """éªŒè¯æºç±»å‹å¹¶è·å–æœ¬åœ°è·¯å¾„"""
          if is_url(repo_source):
              # å…‹éš†è¿œç¨‹ä»“åº“ï¼Œä½¿ç”¨ç¼“å­˜æœºåˆ¶
              temp_path = temp_dir()
              # ä»ç¯å¢ƒå˜é‡è·å–ç¼“å­˜è®¾ç½®
              use_cache = os.getenv("REPO_CACHE_ENABLED", "true").lower() == "true"

              result = git_clone(
                  repo_url=repo_source,
                  local_path=temp_path,
                  use_cache=use_cache,  # æ ¹æ®ç¯å¢ƒå˜é‡å†³å®šæ˜¯å¦å¯ç”¨ç¼“å­˜
                  cache_ttl=None  # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é»˜è®¤å€¼
              )

              if not result["success"]:
                  return {"success": False, "error": result["error"]}

              local_path = result["path"]
              from_cache = result.get("from_cache", False)

              # è®°å½•ç¼“å­˜ä½¿ç”¨æƒ…å†µ
              if from_cache:
                  log_and_notify(f"ä½¿ç”¨ç¼“å­˜çš„ä»£ç åº“: {repo_source}", "info")
              else:
                  log_and_notify(f"å…‹éš†æ–°ä»£ç åº“: {repo_source}", "info")

              return {"success": True, "path": local_path, "from_cache": from_cache}
          else:
              # éªŒè¯æœ¬åœ°è·¯å¾„
              if not os.path.exists(repo_source):
                  return {"success": False, "error": f"è·¯å¾„ä¸å­˜åœ¨: {repo_source}"}
              local_path = repo_source

              return {"success": True, "path": local_path, "from_cache": False}

      def validate_repo(self, local_path):
          """éªŒè¯ä»£ç åº“å¤§å°å’Œæƒé™"""
          # åˆ†æä»£ç åº“å¤§å°
          size_info_dict = analyze_code_size(local_path)

          # ä½¿ç”¨ Pydantic æ¨¡å‹éªŒè¯å¤§å°ä¿¡æ¯
          try:
              size_info = RepoSizeInfo(**size_info_dict)
          except Exception as e:
              return {"success": False, "error": f"ä»£ç åº“å¤§å°ä¿¡æ¯æ— æ•ˆ: {str(e)}"}

          # æ£€æŸ¥ä»£ç åº“å¤§å°æ˜¯å¦è¶…è¿‡é™åˆ¶
          if size_info.total_size > self.max_repo_size:
              return {
                  "success": False,
                  "error": f"ä»£ç åº“å¤§å° ({size_info.total_size} å­—èŠ‚) è¶…è¿‡æœ€å¤§é™åˆ¶ ({self.max_repo_size} å­—èŠ‚)"
              }

          # æ£€æŸ¥æƒé™
          if not has_read_permission(local_path):
              return {"success": False, "error": "ç¼ºå°‘è¯»å–æƒé™"}

          return {"success": True, "size_info": size_info}

      def exec(self, repo_source):
          """æ‰§è¡Œé˜¶æ®µï¼Œå¤„ç†ä»£ç åº“"""
          if repo_source is None:
              return {"success": False, "error": "æ— æ•ˆçš„ä»£ç åº“æ¥æº"}

          # éªŒè¯æºå¹¶è·å–æœ¬åœ°è·¯å¾„
          source_result = self.validate_source(repo_source)
          if not source_result["success"]:
              return {"success": False, "error": source_result["error"]}

          local_path = source_result["path"]
          from_cache = source_result.get("from_cache", False)

          # éªŒè¯ä»£ç åº“
          repo_result = self.validate_repo(local_path)
          if not repo_result["success"]:
              return {"success": False, "error": repo_result["error"]}

          # ä½¿ç”¨ Pydantic æ¨¡å‹éªŒè¯è¾“å‡º
          try:
              output = PrepareRepoOutput(
                  success=True,
                  local_path=local_path,
                  size_info=repo_result["size_info"],
                  from_cache=from_cache
              )
              return output.dict()
          except Exception as e:
              return {"success": False, "error": f"è¾“å‡ºéªŒè¯å¤±è´¥: {str(e)}"}

      def post(self, shared, prep_res, exec_res):
          """åå¤„ç†é˜¶æ®µï¼Œæ›´æ–°å…±äº«å†…å­˜"""
          if exec_res["success"]:
              shared["local_repo_path"] = exec_res["local_path"]
              shared["repo_size_info"] = exec_res["size_info"]
              shared["process_status"]["current_stage"] = "ä»£ç åº“å‡†å¤‡å®Œæˆ"
              log_and_notify(f"ä»£ç åº“å‡†å¤‡å®Œæˆ: {shared['local_repo_path']}", "info")
          else:
              shared["process_status"]["errors"].append({
                  "stage": "å‡†å¤‡ä»£ç åº“",
                  "error": exec_res["error"],
                  "timestamp": datetime.now()
              })
              log_and_notify(f"ä»£ç åº“å‡†å¤‡å¤±è´¥: {exec_res['error']}", "error", notify=True)
  ```

##### 2. `AIUnderstandCoreModulesNode`

- **ç›®çš„**: åˆ©ç”¨ AI ç†è§£ä»£ç åº“çš„æ ¸å¿ƒæ¨¡å—å’Œæ•´ä½“æ¶æ„ã€‚
- **è¾“å…¥**: `shared["code_structure"]`, `shared["dependencies"]`
- **è¾“å‡º**: `shared["ai_analysis"]["core_modules_explanation"]`, `shared["ai_analysis"]["overall_architecture_summary"]`
- **é”™è¯¯å¤„ç†**:
  - å¤„ç† LLM è°ƒç”¨å¤±è´¥: å®ç°é‡è¯•æœºåˆ¶ï¼Œé™çº§å¤„ç†
  - å¤„ç†ç†è§£è´¨é‡ä¸ä½³: å®ç°è´¨é‡è¯„ä¼°å’Œè¿­ä»£ç»†åŒ–
- **å®ç°ç»†èŠ‚**:
  ```python
  def prep(self, shared):
      return shared["code_structure"], shared["dependencies"]

  def create_prompt(self, code_structure, dependencies):
      """åˆ›å»ºåˆ†ææç¤º"""
      return f"""
      åˆ†æä»¥ä¸‹ä»£ç åº“ç»“æ„å’Œä¾èµ–å…³ç³»ï¼Œè¯†åˆ«æ ¸å¿ƒæ¨¡å—å¹¶è§£é‡Šå…¶åŠŸèƒ½å’Œå…³ç³»:

      ä»£ç ç»“æ„:
      {json.dumps(code_structure, indent=2)}

      ä¾èµ–å…³ç³»:
      {json.dumps(dependencies, indent=2)}

      è¯·æä¾›:
      1. æ ¸å¿ƒæ¨¡å—åˆ—è¡¨åŠå…¶åŠŸèƒ½
      2. æ•´ä½“æ¶æ„æ¦‚è¿°
      3. æ¨¡å—é—´å…³ç³»
      """

  def call_model(self, prompt, target_language):
      """è°ƒç”¨LLMå¹¶å¤„ç†ç»“æœ"""
      result, success, metadata = call_llm(
          prompt=prompt,
          task_type="understand_code",
          target_language=target_language
      )

      if not success:
          return None, None, False

      # è§£æç»“æœ
      parsed_result = self.parse_llm_response(result)

      # è´¨é‡è¯„ä¼°
      quality_score = self.evaluate_llm_output(
          result,
          "understand_code",
          {"completeness": 0.7, "accuracy": 0.8, "clarity": 0.6}
      )

      return parsed_result, quality_score, True

  def parse_llm_response(self, response: str) -> Dict[str, Any]:
      """è§£æ LLM å“åº”ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯

      Args:
          response: LLM ç”Ÿæˆçš„å“åº”æ–‡æœ¬

      Returns:
          è§£æåçš„ç»“æ„åŒ–æ•°æ®
      """
      # å°è¯•è§£æä¸º JSON
      try:
          # æŸ¥æ‰¾ JSON å—
          json_match = re.search(r'```(?:json)?\s*({[\s\S]*?})```', response)
          if json_match:
              json_str = json_match.group(1)
              return json.loads(json_str)
      except:
          pass

      # å°è¯•è§£æä¸º YAML
      try:
          # æŸ¥æ‰¾ YAML å—
          yaml_match = re.search(r'```(?:yaml)?\s*([\s\S]*?)```', response)
          if yaml_match:
              yaml_str = yaml_match.group(1)
              import yaml
              return yaml.safe_load(yaml_str)
      except:
          pass

      # å¦‚æœæ— æ³•è§£æä¸ºç»“æ„åŒ–æ ¼å¼ï¼Œå°è¯•æå–å…³é”®ä¿¡æ¯
      result = {
          "core_modules": [],
          "architecture": "",
          "module_relationships": []
      }

      # æå–æ ¸å¿ƒæ¨¡å—
      modules_section = self._extract_section(response, ["æ ¸å¿ƒæ¨¡å—", "Core Modules", "ä¸»è¦æ¨¡å—"])
      if modules_section:
          # å°è¯•æå–åˆ—è¡¨é¡¹
          modules = re.findall(r'[*\-â€¢]\s*([^:\n]+)(?::|\n|ï¼š)(.*?)(?=\n[*\-â€¢]|\n\n|$)', modules_section, re.DOTALL)
          if modules:
              result["core_modules"] = [
                  {"name": m[0].strip(), "description": m[1].strip()}
                  for m in modules
              ]
          else:
              # å°è¯•æå–æ®µè½æè¿°çš„æ¨¡å—
              modules = re.findall(r'([A-Za-z0-9_]+(?:\.[A-Za-z0-9_]+)*)(?::|ï¼š)\s*(.*?)(?=\n\n|$)', modules_section, re.DOTALL)
              if modules:
                  result["core_modules"] = [
                      {"name": m[0].strip(), "description": m[1].strip()}
                      for m in modules
                  ]

      # æå–æ¶æ„æ¦‚è¿°
      architecture_section = self._extract_section(response, ["æ•´ä½“æ¶æ„", "æ¶æ„æ¦‚è¿°", "Architecture", "Overall Architecture"])
      if architecture_section:
          result["architecture"] = architecture_section.strip()

      # æå–æ¨¡å—å…³ç³»
      relationships_section = self._extract_section(response, ["æ¨¡å—å…³ç³»", "Module Relationships", "ä¾èµ–å…³ç³»", "ç»„ä»¶å…³ç³»"])
      if relationships_section:
          # å°è¯•æå–å…³ç³»æè¿°
          relationships = re.findall(r'[*\-â€¢]\s*(.*?)(?=\n[*\-â€¢]|\n\n|$)', relationships_section, re.DOTALL)
          if relationships:
              result["module_relationships"] = [r.strip() for r in relationships]

      return result

  def _extract_section(self, text: str, section_titles: List[str]) -> Optional[str]:
      """ä»æ–‡æœ¬ä¸­æå–ç‰¹å®šç« èŠ‚

      Args:
          text: å®Œæ•´æ–‡æœ¬
          section_titles: å¯èƒ½çš„ç« èŠ‚æ ‡é¢˜åˆ—è¡¨

      Returns:
          æå–çš„ç« èŠ‚å†…å®¹ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
      """
      for title in section_titles:
          # æŸ¥æ‰¾ç« èŠ‚æ ‡é¢˜
          pattern = rf'(?:^|\n)(?:#{1,3}\s*)?{re.escape(title)}[:\s]*(.*?)(?=\n(?:#{1,3}\s*|$)|\Z)'
          match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
          if match:
              return match.group(1).strip()

      return None

  def evaluate_llm_output(self, output: str, task_type: str, criteria: Optional[Dict[str, float]] = None) -> Dict[str, float]:
      """è¯„ä¼° LLM è¾“å‡ºè´¨é‡

      Args:
          output: LLM ç”Ÿæˆçš„è¾“å‡º
          task_type: ä»»åŠ¡ç±»å‹
          criteria: è¯„ä¼°æ ‡å‡†å’Œæƒé‡

      Returns:
          åŒ…å«å„é¡¹è¯„åˆ†å’Œæ€»ä½“è¯„åˆ†çš„å­—å…¸
      """
      # é»˜è®¤è¯„ä¼°æ ‡å‡†
      default_criteria = {
          "understand_code": {
              "completeness": 0.4,  # å®Œæ•´æ€§
              "accuracy": 0.4,      # å‡†ç¡®æ€§
              "clarity": 0.2        # æ¸…æ™°åº¦
          }
      }

      # ä½¿ç”¨é»˜è®¤æ ‡å‡†æˆ–åˆå¹¶è‡ªå®šä¹‰æ ‡å‡†
      task_criteria = default_criteria.get(task_type, {"quality": 1.0})
      if criteria:
          for key, value in criteria.items():
              task_criteria[key] = value

      # åŸºäºè¾“å‡ºé•¿åº¦å’Œå†…å®¹çš„åŸºæœ¬è¯„ä¼°
      scores = {}

      # 1. é•¿åº¦æ£€æŸ¥
      min_length = 200  # ä»£ç ç†è§£éœ€è¦è¶³å¤Ÿçš„é•¿åº¦
      length_score = min(1.0, len(output) / min_length)

      # 2. å†…å®¹è´¨é‡æ£€æŸ¥
      # æ£€æŸ¥ç»“æ„åŒ–å†…å®¹
      structure_indicators = ["æ¨¡å—", "ç±»", "å‡½æ•°", "æ–¹æ³•", "æ¥å£", "ç»„ä»¶"]
      structure_count = sum(indicator in output for indicator in structure_indicators)
      structure_score = min(1.0, structure_count / 5)

      # æ£€æŸ¥å…³ç³»æè¿°
      relation_indicators = ["è°ƒç”¨", "ä¾èµ–", "ç»§æ‰¿", "åŒ…å«", "ä½¿ç”¨", "å…³è”"]
      relation_count = sum(indicator in output for indicator in relation_indicators)
      relation_score = min(1.0, relation_count / 3)

      # è®¡ç®—å„é¡¹æŒ‡æ ‡å¾—åˆ†
      scores["completeness"] = length_score
      scores["accuracy"] = (structure_score + relation_score) / 2
      scores["clarity"] = structure_score

      # è®¡ç®—åŠ æƒæ€»åˆ†
      overall_score = 0.0
      for criterion, weight in task_criteria.items():
          if criterion in scores:
              overall_score += scores[criterion] * weight
          else:
              overall_score += 0.5 * weight

      # æ·»åŠ æ€»ä½“è¯„åˆ†
      scores["overall"] = min(1.0, overall_score)

      return scores

  def exec(self, inputs):
      code_structure, dependencies = inputs
      target_language = self.shared.get("target_language", "en")

      # å‡†å¤‡æç¤º
      prompt = self.create_prompt(code_structure, dependencies)

      # å°è¯•è°ƒç”¨LLM (æœ€å¤š3æ¬¡)
      for attempt in range(3):
          try:
              parsed_result, quality_score, success = self.call_model(
                  prompt, target_language
              )

              if success and quality_score["overall"] >= 0.7:
                  return {
                      "success": True,
                      "core_modules": parsed_result["core_modules"],
                      "architecture": parsed_result["architecture"],
                      "quality_score": quality_score
                  }
              elif success:
                  log_and_notify(
                      f"ç†è§£è´¨é‡ä¸ä½³ (åˆ†æ•°: {quality_score['overall']}), é‡è¯•ä¸­...",
                      "warning"
                  )
          except Exception as e:
              log_and_notify(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}, é‡è¯•ä¸­...", "warning")

      # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é™çº§ç»“æœ
      return {
          "success": False,
          "error": "æ— æ³•è·å¾—é«˜è´¨é‡çš„ä»£ç ç†è§£",
          "fallback_result": self.generate_basic_understanding(code_structure)
      }

  def generate_basic_understanding(self, code_structure: Dict[str, Any]) -> Dict[str, Any]:
      """ç”ŸæˆåŸºæœ¬çš„ä»£ç ç†è§£ç»“æœï¼Œä½œä¸º LLM è°ƒç”¨å¤±è´¥æ—¶çš„é™çº§æ–¹æ¡ˆ

      Args:
          code_structure: ä»£ç ç»“æ„ä¿¡æ¯

      Returns:
          åŒ…å«æ ¸å¿ƒæ¨¡å—å’Œæ¶æ„æ¦‚è¿°çš„å­—å…¸
      """
      # åˆå§‹åŒ–ç»“æœ
      result = {
          "core_modules": [],
          "architecture": "æ— æ³•é€šè¿‡ AI åˆ†æç”Ÿæˆè¯¦ç»†æ¶æ„æ¦‚è¿°ã€‚ä»¥ä¸‹æ˜¯åŸºäºä»£ç ç»“æ„çš„åŸºæœ¬ä¿¡æ¯ã€‚"
      }

      # æå–æ–‡ä»¶å’Œç›®å½•ä¿¡æ¯
      files = []
      directories = []

      # ä»ä»£ç ç»“æ„ä¸­æå–æ–‡ä»¶å’Œç›®å½•
      for path, info in code_structure.items():
          if isinstance(info, dict) and "type" in info:
              if info["type"] == "file":
                  files.append(path)
              elif info["type"] == "directory":
                  directories.append(path)

      # è¯†åˆ«å¯èƒ½çš„æ ¸å¿ƒæ¨¡å—
      potential_modules = []

      # 1. æŸ¥æ‰¾ __init__.py æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•ï¼Œè¿™äº›é€šå¸¸æ˜¯ Python æ¨¡å—
      python_modules = [d for d in directories if f"{d}/__init__.py" in files]
      potential_modules.extend(python_modules)

      # 2. æŸ¥æ‰¾å¯èƒ½çš„ä¸»è¦æ–‡ä»¶
      main_files = [f for f in files if any(pattern in f.lower() for pattern in
                                          ["main", "app", "core", "server", "client", "api", "service"])]
      potential_modules.extend(main_files)

      # 3. æŒ‰æ–‡ä»¶æ‰©å±•ååˆ†ç»„
      file_extensions = {}
      for f in files:
          ext = f.split(".")[-1] if "." in f else "unknown"
          if ext not in file_extensions:
              file_extensions[ext] = []
          file_extensions[ext].append(f)

      # ä¸ºæ¯ä¸ªå¯èƒ½çš„æ ¸å¿ƒæ¨¡å—åˆ›å»ºæè¿°
      for module in potential_modules[:10]:  # é™åˆ¶æ•°é‡
          if module in python_modules:
              description = f"Python æ¨¡å—ï¼Œå¯èƒ½åŒ…å«æ ¸å¿ƒåŠŸèƒ½"
          elif module in main_files:
              description = f"å¯èƒ½çš„ä¸»è¦æ–‡ä»¶ï¼ŒåŒ…å«ç¨‹åºå…¥å£ç‚¹æˆ–æ ¸å¿ƒé€»è¾‘"
          else:
              description = f"å¯èƒ½çš„æ ¸å¿ƒç»„ä»¶"

          result["core_modules"].append({
              "name": module,
              "description": description
          })

      # ç”ŸæˆåŸºæœ¬æ¶æ„æè¿°
      architecture_parts = [
          "ä»£ç åº“åŒ…å«ä»¥ä¸‹ç»„ä»¶ï¼š",
          f"- {len(directories)} ä¸ªç›®å½•",
          f"- {len(files)} ä¸ªæ–‡ä»¶"
      ]

      # æ·»åŠ æ–‡ä»¶ç±»å‹ç»Ÿè®¡
      if file_extensions:
          architecture_parts.append("æ–‡ä»¶ç±»å‹åˆ†å¸ƒï¼š")
          for ext, ext_files in file_extensions.items():
              if len(ext_files) > 0:
                  architecture_parts.append(f"- {ext}: {len(ext_files)} ä¸ªæ–‡ä»¶")

      # æ·»åŠ å¯èƒ½çš„æ¶æ„æ¨¡å¼è¯†åˆ«
      if any("test" in f.lower() for f in files):
          architecture_parts.append("- åŒ…å«æµ‹è¯•æ–‡ä»¶ï¼Œå¯èƒ½éµå¾ªæµ‹è¯•é©±åŠ¨å¼€å‘æ¨¡å¼")

      if any("model" in f.lower() for f in files) and any("view" in f.lower() for f in files):
          architecture_parts.append("- å¯èƒ½é‡‡ç”¨ MVC æˆ–ç±»ä¼¼æ¶æ„æ¨¡å¼")

      if any("api" in f.lower() for f in files) and any("service" in f.lower() for f in files):
          architecture_parts.append("- å¯èƒ½é‡‡ç”¨å¾®æœåŠ¡æˆ–æœåŠ¡å¯¼å‘æ¶æ„")

      result["architecture"] = "\n".join(architecture_parts)

      return result

  def update_success_state(self, shared, exec_res):
      """æ›´æ–°æˆåŠŸçŠ¶æ€"""
      shared["ai_analysis"]["core_modules_explanation"] = exec_res["core_modules"]
      shared["ai_analysis"]["overall_architecture_summary"] = exec_res["architecture"]
      shared["ai_analysis"]["quality_metrics"]["architecture_understanding"] = (
          exec_res["quality_score"]["overall"]
      )
      shared["process_status"]["current_stage"] = "AI æ ¸å¿ƒç†è§£å®Œæˆ"

  def update_failure_state(self, shared, exec_res):
      """æ›´æ–°å¤±è´¥çŠ¶æ€"""
      shared["process_status"]["errors"].append({
          "stage": "AI æ ¸å¿ƒç†è§£",
          "error": exec_res["error"],
          "timestamp": datetime.now()
      })
      # ä½¿ç”¨é™çº§ç»“æœ
      shared["ai_analysis"]["core_modules_explanation"] = (
          exec_res["fallback_result"]["core_modules"]
      )
      shared["ai_analysis"]["overall_architecture_summary"] = (
          exec_res["fallback_result"]["architecture"]
      )
      shared["ai_analysis"]["quality_metrics"]["architecture_understanding"] = 0.4
      log_and_notify("ä½¿ç”¨é™çº§çš„ä»£ç ç†è§£ç»“æœ", "warning", notify=True)

  def post(self, shared, prep_res, exec_res):
      if exec_res["success"]:
          self.update_success_state(shared, exec_res)
      else:
          self.update_failure_state(shared, exec_res)
  ```

##### 3. `InteractiveQANode`

- **ç›®çš„**: å¤„ç†ç”¨æˆ·çš„äº¤äº’å¼é—®é¢˜ï¼Œåˆ©ç”¨ RAG å’Œ AI ç”Ÿæˆå›ç­”ï¼Œæ”¯æŒå¤šè½®å¯¹è¯å’Œä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„ã€‚
- **è¾“å…¥**: `shared["user_query"]`, `shared["vector_index"]`, `shared["text_chunks"]`, `shared["conversation_history"]`
- **è¾“å‡º**: `shared["generated_content"]["custom_answers"]`, `shared["conversation_history"]`
- **é”™è¯¯å¤„ç†**:
  - å¤„ç†é—®é¢˜ç†è§£å¤±è´¥: è¯·æ±‚æ¾„æ¸…ï¼Œæä¾›æ›¿ä»£è§£é‡Š
  - å¤„ç† RAG æ£€ç´¢å¤±è´¥: ä½¿ç”¨å¤‡ç”¨ä¸Šä¸‹æ–‡ï¼ŒåŸºäºå·²æœ‰ç†è§£ç”Ÿæˆå›ç­”
- **å¢å¼ºåŠŸèƒ½**:
  - å¤šè½®å¯¹è¯å†å²: ä¿å­˜å®Œæ•´å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œæ”¯æŒåç»­ç›¸å…³é—®é¢˜
  - ä¸»åŠ¨æ¾„æ¸…æœºåˆ¶: å½“ç”¨æˆ·é—®é¢˜ä¸æ˜ç¡®æ—¶ä¸»åŠ¨è¯·æ±‚æ¾„æ¸…
  - ä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„: åŸºäºç”¨æˆ·é—®é¢˜å’Œåé¦ˆç”Ÿæˆå®šåˆ¶åŒ–å­¦ä¹ è·¯å¾„
  - äº¤äº’å¼æ–‡æ¡£å¯¼èˆª: å…è®¸é€šè¿‡å¯¹è¯æ–¹å¼å¯¼èˆªå’Œæ¢ç´¢æ–‡æ¡£
- **å®ç°ç»†èŠ‚**:
  ```python
  def prep(self, shared):
      return (
          shared["user_query"],
          shared["vector_index"],
          shared["text_chunks"],
          shared["ai_analysis"],
          shared.get("conversation_history", [])
      )

  def analyze_query(self, user_query, conversation_history):
      """åˆ†æç”¨æˆ·é—®é¢˜ç±»å‹å’Œæ„å›¾ï¼Œæ”¯æŒä¸Šä¸‹æ–‡ç†è§£"""
      # æ„å»ºåˆ†ææç¤ºï¼ŒåŒ…å«å¯¹è¯å†å²
      history_context = ""
      if conversation_history:
          history_context = "å¯¹è¯å†å²:\n" + "\n".join([
              f"ç”¨æˆ·: {turn['question']}\nå›ç­”: {turn['answer'][:100]}..."
              for turn in conversation_history[-3:]  # åªä½¿ç”¨æœ€è¿‘çš„3è½®å¯¹è¯
          ])

      analysis_prompt = f"""
      åˆ†æä»¥ä¸‹å…³äºä»£ç åº“çš„é—®é¢˜ï¼Œç¡®å®šé—®é¢˜ç±»å‹ã€æ„å›¾å’Œæ¸…æ™°åº¦:

      {history_context}

      å½“å‰é—®é¢˜: {user_query}

      è¯·æä¾›:
      1. é—®é¢˜ç±»å‹ (å¦‚ "æ¶æ„ç›¸å…³", "APIä½¿ç”¨", "åŠŸèƒ½è§£é‡Š", "å¯¼èˆªè¯·æ±‚" ç­‰)
      2. ç”¨æˆ·æ„å›¾ (å¦‚ "å¯»æ±‚ä¿¡æ¯", "è¯·æ±‚ç¤ºä¾‹", "æ¯”è¾ƒé€‰é¡¹", "å®šä½æ–‡æ¡£" ç­‰)
      3. é—®é¢˜æ¸…æ™°åº¦ (0-10åˆ†ï¼Œ10åˆ†è¡¨ç¤ºå®Œå…¨æ¸…æ™°)
      4. æ˜¯å¦ä¾èµ–ä¸Šä¸‹æ–‡ (true/false)
      5. å¦‚æœé—®é¢˜ä¸æ¸…æ™°ï¼Œéœ€è¦æ¾„æ¸…çš„å…·ä½“æ–¹é¢
      """

      query_analysis, success, metadata = call_llm(
          prompt=analysis_prompt,
          task_type="analyze_question"
      )

      if not success:
          return {
              "type": "general",
              "intent": "unknown",
              "clarity": 5,
              "context_dependent": False,
              "needs_clarification": False
          }

      # è§£æ LLM è¿”å›çš„åˆ†æç»“æœ
      try:
          parsed_analysis = json.loads(query_analysis)
          return parsed_analysis
      except:
          # å¦‚æœæ— æ³•è§£æä¸º JSONï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–ä¿¡æ¯
          analysis = {
              "type": "general",
              "intent": "unknown",
              "clarity": 5,
              "context_dependent": "ä¸Šä¸‹æ–‡" in query_analysis or "å†å²" in query_analysis,
              "needs_clarification": "ä¸æ¸…æ™°" in query_analysis or "æ¾„æ¸…" in query_analysis
          }

          # å°è¯•æå–é—®é¢˜ç±»å‹
          type_match = re.search(r"ç±»å‹[ï¼š:]\s*[\"']?([^\"'\n]+)[\"']?", query_analysis)
          if type_match:
              analysis["type"] = type_match.group(1).strip()

          # å°è¯•æå–æ„å›¾
          intent_match = re.search(r"æ„å›¾[ï¼š:]\s*[\"']?([^\"'\n]+)[\"']?", query_analysis)
          if intent_match:
              analysis["intent"] = intent_match.group(1).strip()

          # å°è¯•æå–æ¸…æ™°åº¦
          clarity_match = re.search(r"æ¸…æ™°åº¦[ï¼š:]\s*(\d+)", query_analysis)
          if clarity_match:
              analysis["clarity"] = int(clarity_match.group(1))

          return analysis

  def request_clarification(self, user_query, analysis, target_language):
      """å½“é—®é¢˜ä¸æ¸…æ™°æ—¶ï¼Œç”Ÿæˆæ¾„æ¸…è¯·æ±‚"""
      clarification_prompt = f"""
      ç”¨æˆ·é—®é¢˜: "{user_query}"

      è¿™ä¸ªé—®é¢˜ä¸å¤Ÿæ¸…æ™°ï¼Œéœ€è¦æ›´å¤šä¿¡æ¯ã€‚è¯·ç”Ÿæˆ 2-3 ä¸ªç®€çŸ­ã€å…·ä½“çš„åç»­é—®é¢˜ï¼Œ
      å¸®åŠ©æ¾„æ¸…ç”¨æˆ·çš„æ„å›¾ã€‚é—®é¢˜åº”è¯¥å‹å¥½ã€æœ‰é’ˆå¯¹æ€§ï¼Œå¹¶ç›´æ¥å…³è”åˆ°ç”¨æˆ·å¯èƒ½æƒ³äº†è§£çš„ä»£ç åº“æ–¹é¢ã€‚
      """

      clarification, success, metadata = call_llm(
          prompt=clarification_prompt,
          task_type="generate_clarification",
          target_language=target_language
      )

      if not success:
          return "æ‚¨çš„é—®é¢˜ä¸å¤Ÿæ¸…æ™°ï¼Œèƒ½å¦æä¾›æ›´å¤šç»†èŠ‚ï¼Ÿä¾‹å¦‚ï¼Œæ‚¨æƒ³äº†è§£å“ªä¸ªç‰¹å®šæ¨¡å—æˆ–åŠŸèƒ½ï¼Ÿ"

      return clarification

  def retrieve_context(self, user_query, analysis, vector_index, text_chunks, ai_analysis, conversation_history):
      """å¢å¼ºçš„ä¸Šä¸‹æ–‡æ£€ç´¢ï¼Œæ”¯æŒå¯¹è¯å†å²å’Œæ··åˆæ£€ç´¢"""
      try:
          # æ„å»ºå¢å¼ºæŸ¥è¯¢
          enhanced_query = user_query

          # å¦‚æœé—®é¢˜ä¾èµ–ä¸Šä¸‹æ–‡ï¼Œå°†æœ€è¿‘çš„å¯¹è¯èå…¥æŸ¥è¯¢
          if analysis.get("context_dependent", False) and conversation_history:
              recent_context = conversation_history[-1]["question"] + " " + conversation_history[-1]["answer"][:100]
              enhanced_query = recent_context + " " + user_query

          # è·å–é—®é¢˜çš„åµŒå…¥å‘é‡
          query_embedding = get_embedding(enhanced_query)

          # ç¡®å®šæ£€ç´¢æ•°é‡
          top_k = self.config.get("top_k", 5)

          # æ ¹æ®é—®é¢˜ç±»å‹è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼
          similarity_threshold = self.config.get("similarity_threshold", 0.65)
          if analysis["type"] in ["æ¶æ„ç›¸å…³", "æ¦‚è§ˆ"]:
              similarity_threshold = 0.6  # æ¶æ„é—®é¢˜éœ€è¦æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡
          elif analysis["type"] in ["APIä½¿ç”¨", "å‡½æ•°ç»†èŠ‚"]:
              similarity_threshold = 0.75  # API é—®é¢˜éœ€è¦æ›´ç²¾ç¡®çš„åŒ¹é…

          # RAG æ£€ç´¢ç›¸å…³å†…å®¹
          relevant_ids, scores = vector_search(
              query_embedding,
              vector_index,
              top_k=top_k,
              similarity_threshold=similarity_threshold
          )

          # æ··åˆæ£€ç´¢ç­–ç•¥ï¼šç»“åˆå‘é‡æ£€ç´¢å’Œå…³é”®è¯åŒ¹é…
          if len(relevant_ids) < 2:  # å¦‚æœå‘é‡æ£€ç´¢ç»“æœä¸è¶³
              keyword_matches = keyword_search(
                  user_query,
                  text_chunks,
                  max_results=3
              )
              # åˆå¹¶ç»“æœå¹¶å»é‡
              all_ids = list(set(relevant_ids + keyword_matches))
              relevant_ids = all_ids[:top_k]

          # è·å–ç›¸å…³æ–‡æœ¬å—
          relevant_chunks = [text_chunks[idx] for idx in relevant_ids]

          # æ ¹æ®é—®é¢˜ç±»å‹æ·»åŠ ç‰¹å®šä¸Šä¸‹æ–‡
          if analysis["type"] == "æ¶æ„ç›¸å…³":
              relevant_chunks.append(f"æ¶æ„æ¦‚è¿°: {ai_analysis['overall_architecture_summary']}")
          elif analysis["type"] == "æ¨¡å—å…³ç³»":
              relevant_chunks.append(f"æ ¸å¿ƒæ¨¡å—: {ai_analysis['core_modules_explanation']}")

          # ç»„åˆä¸Šä¸‹æ–‡
          context = "\n\n".join(relevant_chunks)

          return context, relevant_ids

      except Exception as e:
          # RAG æ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨ä¸Šä¸‹æ–‡
          log_and_notify(f"RAG æ£€ç´¢å¤±è´¥: {str(e)}", "warning")
          context = f"""
          æ•´ä½“æ¶æ„æ¦‚è¿°:
          {ai_analysis['overall_architecture_summary']}

          æ ¸å¿ƒæ¨¡å—:
          {ai_analysis['core_modules_explanation']}
          """
          return context, []

  def generate_answer(self, user_query, analysis, context, conversation_history, target_language):
      """å¢å¼ºçš„å›ç­”ç”Ÿæˆï¼Œæ”¯æŒå¯¹è¯å†å²å’Œä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„"""
      # æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡
      history_context = ""
      if conversation_history and analysis.get("context_dependent", False):
          history_context = "å¯¹è¯å†å²:\n" + "\n".join([
              f"ç”¨æˆ·: {turn['question']}\nå›ç­”: {turn['answer'][:150]}..."
              for turn in conversation_history[-3:]  # åªä½¿ç”¨æœ€è¿‘çš„3è½®å¯¹è¯
          ])

      # æ ¹æ®é—®é¢˜ç±»å‹å®šåˆ¶æç¤º
      if analysis["type"] == "å¯¼èˆªè¯·æ±‚" or "æŸ¥æ‰¾" in analysis["intent"]:
          answer_prompt = f"""
          ç”¨æˆ·æ­£åœ¨å¯»æ‰¾ä»£ç åº“ä¸­çš„ç‰¹å®šå†…å®¹ã€‚åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼Œæä¾›å‡†ç¡®çš„å¯¼èˆªæŒ‡å—:

          {history_context}

          ç”¨æˆ·é—®é¢˜: {user_query}

          ä¸Šä¸‹æ–‡ä¿¡æ¯:
          {context}

          è¯·æä¾›:
          1. ç›´æ¥å›ç­”ç”¨æˆ·çš„å¯¼èˆªé—®é¢˜
          2. ç›¸å…³æ–‡ä»¶æˆ–æ¨¡å—çš„å…·ä½“ä½ç½®
          3. å¦‚ä½•æ‰¾åˆ°å’Œä½¿ç”¨è¿™äº›å†…å®¹çš„ç®€çŸ­æŒ‡å—
          4. 1-2ä¸ªç›¸å…³çš„åç»­é—®é¢˜å»ºè®®ï¼Œå¸®åŠ©ç”¨æˆ·ç»§ç»­æ¢ç´¢
          """
      elif analysis["type"] == "APIä½¿ç”¨" or "ç¤ºä¾‹" in analysis["intent"]:
          answer_prompt = f"""
          ç”¨æˆ·æ­£åœ¨è¯¢é—®APIä½¿ç”¨æ–¹æ³•ã€‚åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼Œæä¾›è¯¦ç»†çš„APIä½¿ç”¨æŒ‡å—:

          {history_context}

          ç”¨æˆ·é—®é¢˜: {user_query}

          ä¸Šä¸‹æ–‡ä¿¡æ¯:
          {context}

          è¯·æä¾›:
          1. APIçš„è¯¦ç»†è¯´æ˜
          2. å‚æ•°å’Œè¿”å›å€¼è§£é‡Š
          3. ä¸€ä¸ªå®Œæ•´çš„ä»£ç ç¤ºä¾‹
          4. å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ³•
          5. 1-2ä¸ªç›¸å…³çš„åç»­é—®é¢˜å»ºè®®ï¼Œå¸®åŠ©ç”¨æˆ·æ·±å…¥ç†è§£
          """
      elif analysis.get("clarity", 10) < 5:
          # é—®é¢˜ä¸å¤Ÿæ¸…æ™°ï¼Œç”Ÿæˆæ¾„æ¸…å›ç­”
          answer_prompt = f"""
          ç”¨æˆ·çš„é—®é¢˜ä¸å¤Ÿæ¸…æ™°ã€‚åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼Œæä¾›æœ‰å¸®åŠ©çš„å›åº”:

          {history_context}

          ç”¨æˆ·é—®é¢˜: {user_query}

          ä¸Šä¸‹æ–‡ä¿¡æ¯:
          {context}

          è¯·æä¾›:
          1. å¯¹ç”¨æˆ·å¯èƒ½æ„å›¾çš„æœ€ä½³çŒœæµ‹
          2. 2-3ä¸ªå…·ä½“çš„æ¾„æ¸…é—®é¢˜ï¼Œå¸®åŠ©æ›´å¥½åœ°ç†è§£ç”¨æˆ·éœ€æ±‚
          3. åŸºäºå½“å‰ç†è§£çš„åˆæ­¥å›ç­”
          """
      else:
          # ä¸€èˆ¬é—®é¢˜çš„æ ‡å‡†æç¤º
          answer_prompt = f"""
          åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼Œå›ç­”å…³äºä»£ç åº“çš„é—®é¢˜:

          {history_context}

          ç”¨æˆ·é—®é¢˜: {user_query}

          ä¸Šä¸‹æ–‡ä¿¡æ¯:
          {context}

          è¯·æä¾›å‡†ç¡®ã€æ¸…æ™°ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œ
          è¯·è¯´æ˜å¹¶åŸºäºä¸€èˆ¬è½¯ä»¶å·¥ç¨‹åŸåˆ™æä¾›æœ€ä½³çŒœæµ‹ã€‚

          åœ¨å›ç­”æœ«å°¾ï¼Œæä¾›1-2ä¸ªç›¸å…³çš„åç»­é—®é¢˜å»ºè®®ï¼Œå¸®åŠ©ç”¨æˆ·ç»§ç»­æ¢ç´¢ã€‚
          """

      # è°ƒç”¨ LLM ç”Ÿæˆå›ç­”
      answer, success, metadata = call_llm(
          prompt=answer_prompt,
          task_type="answer_question",
          target_language=target_language
      )

      if not success:
          return None, False, None

      # è¯„ä¼°å›ç­”è´¨é‡
      quality_score = self.evaluate_llm_output(
          answer,
          "answer_question",
          {"relevance": 0.8, "accuracy": 0.7, "helpfulness": 0.6}
      )

      # æå–å»ºè®®çš„åç»­é—®é¢˜
      follow_up_questions = self.extract_follow_up_questions(answer)

      return answer, quality_score["overall"], follow_up_questions

  def evaluate_llm_output(self, output: str, task_type: str, criteria: Optional[Dict[str, float]] = None) -> Dict[str, float]:
      """è¯„ä¼° LLM è¾“å‡ºè´¨é‡

      Args:
          output: LLM ç”Ÿæˆçš„è¾“å‡º
          task_type: ä»»åŠ¡ç±»å‹
          criteria: è¯„ä¼°æ ‡å‡†å’Œæƒé‡

      Returns:
          åŒ…å«å„é¡¹è¯„åˆ†å’Œæ€»ä½“è¯„åˆ†çš„å­—å…¸
      """
      # é»˜è®¤è¯„ä¼°æ ‡å‡†
      default_criteria = {
          "answer_question": {
              "relevance": 0.4,  # ç›¸å…³æ€§
              "accuracy": 0.3,   # å‡†ç¡®æ€§
              "helpfulness": 0.3  # æœ‰ç”¨æ€§
          },
          "understand_code": {
              "completeness": 0.4,  # å®Œæ•´æ€§
              "accuracy": 0.4,      # å‡†ç¡®æ€§
              "clarity": 0.2        # æ¸…æ™°åº¦
          },
          "generate_learning_path": {
              "structure": 0.3,     # ç»“æ„æ€§
              "relevance": 0.4,     # ç›¸å…³æ€§
              "practicality": 0.3   # å®ç”¨æ€§
          },
          "analyze_question": {
              "accuracy": 0.5,      # å‡†ç¡®æ€§
              "completeness": 0.3,  # å®Œæ•´æ€§
              "insight": 0.2        # æ´å¯ŸåŠ›
          }
      }

      # ä½¿ç”¨é»˜è®¤æ ‡å‡†æˆ–åˆå¹¶è‡ªå®šä¹‰æ ‡å‡†
      task_criteria = default_criteria.get(task_type, {"quality": 1.0})
      if criteria:
          for key, value in criteria.items():
              task_criteria[key] = value

      # åŸºäºè¾“å‡ºé•¿åº¦å’Œå†…å®¹çš„åŸºæœ¬è¯„ä¼°
      scores = {}

      # 1. é•¿åº¦æ£€æŸ¥
      min_lengths = {
          "answer_question": 100,
          "understand_code": 200,
          "generate_learning_path": 300,
          "analyze_question": 50
      }

      min_length = min_lengths.get(task_type, 100)
      length_score = min(1.0, len(output) / min_length)

      # 2. å†…å®¹è´¨é‡æ£€æŸ¥
      # è¿™é‡Œä½¿ç”¨ç®€å•çš„å¯å‘å¼è§„åˆ™ï¼Œå®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„è¯„ä¼°æ–¹æ³•

      # æ£€æŸ¥ç»“æ„åŒ–å†…å®¹ï¼ˆå¦‚ JSONã€åˆ—è¡¨ç­‰ï¼‰
      structure_indicators = ["```", "ï¼š", ":", "-", "â€¢", "1.", "#", "{", "ã€Œ"]
      has_structure = any(indicator in output for indicator in structure_indicators)
      structure_score = 0.8 if has_structure else 0.5

      # æ£€æŸ¥ä¸“ä¸šæœ¯è¯­
      tech_terms = ["å‡½æ•°", "æ–¹æ³•", "ç±»", "æ¨¡å—", "API", "æ¥å£", "ä»£ç ", "ç®—æ³•",
                   "function", "method", "class", "module", "interface", "code", "algorithm"]
      term_count = sum(term in output.lower() for term in tech_terms)
      term_score = min(1.0, term_count / 3)

      # æ£€æŸ¥ä»£ç ç¤ºä¾‹
      has_code = "```" in output or "    " in output
      code_score = 0.9 if has_code and task_type in ["answer_question", "understand_code"] else 0.6

      # æ ¹æ®ä»»åŠ¡ç±»å‹è®¡ç®—å„é¡¹æŒ‡æ ‡å¾—åˆ†
      if task_type == "answer_question":
          scores["relevance"] = (structure_score + term_score) / 2
          scores["accuracy"] = term_score
          scores["helpfulness"] = (structure_score + code_score) / 2
      elif task_type == "understand_code":
          scores["completeness"] = length_score
          scores["accuracy"] = term_score
          scores["clarity"] = structure_score
      elif task_type == "generate_learning_path":
          scores["structure"] = structure_score
          scores["relevance"] = term_score
          scores["practicality"] = (length_score + code_score) / 2
      elif task_type == "analyze_question":
          scores["accuracy"] = term_score
          scores["completeness"] = length_score
          scores["insight"] = structure_score
      else:
          scores["quality"] = (length_score + structure_score + term_score) / 3

      # è®¡ç®—åŠ æƒæ€»åˆ†
      overall_score = 0.0
      for criterion, weight in task_criteria.items():
          if criterion in scores:
              overall_score += scores[criterion] * weight
          else:
              # å¦‚æœæ²¡æœ‰ç‰¹å®šæ ‡å‡†çš„åˆ†æ•°ï¼Œä½¿ç”¨å¹³å‡åˆ†
              overall_score += 0.5 * weight

      # æ·»åŠ æ€»ä½“è¯„åˆ†
      scores["overall"] = min(1.0, overall_score)

      return scores

  def extract_follow_up_questions(self, text: str) -> List[str]:
      """ä»æ–‡æœ¬ä¸­æå–åç»­é—®é¢˜å»ºè®®

      Args:
          text: åŒ…å«åç»­é—®é¢˜çš„æ–‡æœ¬

      Returns:
          æå–çš„é—®é¢˜åˆ—è¡¨
      """
      # å¸¸è§çš„åç»­é—®é¢˜æ ‡è®°æ¨¡å¼
      patterns = [
          r"(?:åç»­é—®é¢˜|ç›¸å…³é—®é¢˜|å»ºè®®é—®é¢˜|æ‚¨å¯èƒ½æƒ³é—®|ä½ å¯èƒ½æƒ³é—®|Further questions|Related questions)[:ï¼š]?\s*((?:\d+[\.\)ã€][\s]*[^ï¼Ÿ\?\n]+[\ï¼Ÿ\?][\s]*)+)",
          r"(?:\n\s*\d+[\.\)ã€][\s]*[^ï¼Ÿ\?\n]+[\ï¼Ÿ\?][\s]*)+$",
          r"(?:æ‚¨è¿˜å¯ä»¥é—®|ä½ è¿˜å¯ä»¥é—®|You might also ask)[:ï¼š]?\s*((?:[^ï¼Ÿ\?\n]+[\ï¼Ÿ\?][\s]*)+)"
      ]

      # å°è¯•ä½¿ç”¨ä¸åŒæ¨¡å¼æå–é—®é¢˜
      for pattern in patterns:
          matches = re.findall(pattern, text, re.MULTILINE)
          if matches:
              # æå–åˆ°æ•´ä¸ªé—®é¢˜å—
              questions_block = matches[0] if isinstance(matches[0], str) else matches[0][0]
              # åˆ†å‰²æˆå•ç‹¬çš„é—®é¢˜
              questions = re.findall(r"\d+[\.\)ã€]?\s*([^ï¼Ÿ\?\n]+[\ï¼Ÿ\?])", questions_block)
              if questions:
                  return [q.strip() for q in questions]

      # å¦‚æœä¸Šé¢çš„æ¨¡å¼éƒ½æ²¡åŒ¹é…åˆ°ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾é—®å·ç»“å°¾çš„å¥å­
      questions = re.findall(r"([^ã€‚ï¼Ÿï¼\?\.\!\n]{10,}[\ï¼Ÿ\?])", text)

      # è¿‡æ»¤æ‰ä¸åƒé—®é¢˜çš„å¥å­ï¼ˆå¤ªçŸ­æˆ–ä¸æ˜¯é—®å¥ï¼‰
      filtered_questions = []
      for q in questions:
          q = q.strip()
          # é—®é¢˜é€šå¸¸ä»¥ç–‘é—®è¯å¼€å¤´æˆ–åŒ…å«ç‰¹å®šè¯è¯­
          if (len(q) > 10 and
              (q.startswith(("å¦‚ä½•", "ä¸ºä»€ä¹ˆ", "æ˜¯å¦", "æ€æ ·", "ä»€ä¹ˆ", "å“ªäº›", "å¦‚æœ", "èƒ½å¦")) or
               any(word in q for word in ["å¯ä»¥", "èƒ½å¤Ÿ", "åº”è¯¥", "éœ€è¦", "æœ‰æ²¡æœ‰", "æ˜¯ä¸æ˜¯"]))):
              filtered_questions.append(q)

      # é™åˆ¶è¿”å›çš„é—®é¢˜æ•°é‡
      return filtered_questions[:3]

  def generate_learning_path(self, user_queries, answers, ai_analysis):
      """åŸºäºç”¨æˆ·é—®é¢˜å’Œå›ç­”ç”Ÿæˆä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„"""
      if len(user_queries) < 2:
          return None  # éœ€è¦è‡³å°‘ä¸¤ä¸ªé—®é¢˜æ‰èƒ½ç”Ÿæˆå­¦ä¹ è·¯å¾„

      learning_path_prompt = f"""
      åŸºäºç”¨æˆ·çš„é—®é¢˜å†å²å’Œä»£ç åº“ç»“æ„ï¼Œç”Ÿæˆä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„:

      ç”¨æˆ·é—®é¢˜å†å²:
      {json.dumps(user_queries, indent=2, ensure_ascii=False)}

      ä»£ç åº“æ ¸å¿ƒæ¨¡å—:
      {ai_analysis['core_modules_explanation']}

      è¯·ç”Ÿæˆä¸€ä¸ªç»“æ„åŒ–çš„å­¦ä¹ è·¯å¾„ï¼ŒåŒ…å«:
      1. ç”¨æˆ·ä¼¼ä¹æ„Ÿå…´è¶£çš„ä¸»è¦é¢†åŸŸ
      2. å»ºè®®çš„å­¦ä¹ é¡ºåºï¼ˆä»åŸºç¡€åˆ°é«˜çº§ï¼‰
      3. æ¯ä¸ªæ­¥éª¤éœ€è¦äº†è§£çš„å…³é”®æ¦‚å¿µå’Œæ¨¡å—
      4. æ¨èçš„å®è·µç»ƒä¹ 

      ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µ:
      - interest_areas: å…´è¶£é¢†åŸŸåˆ—è¡¨
      - learning_steps: å­¦ä¹ æ­¥éª¤æ•°ç»„ï¼Œæ¯æ­¥åŒ…å« title, description, modules, concepts, practice
      """

      learning_path, success, metadata = call_llm(
          prompt=learning_path_prompt,
          task_type="generate_learning_path"
      )

      if not success:
          return None

      try:
          # å°è¯•è§£æä¸ºJSON
          return json.loads(learning_path)
      except:
          # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
          return {"raw_text": learning_path}

  def exec(self, inputs):
      user_query, vector_index, text_chunks, ai_analysis, conversation_history = inputs

      if not user_query:
          return {"success": True, "skip": True}

      # åˆå§‹åŒ–é…ç½®
      self.config = self.shared.get("config", {}).get("InteractiveQANode", {})
      target_language = self.shared.get("target_language", "en")

      # åˆ†æé—®é¢˜
      query_analysis = self.analyze_query(user_query, conversation_history)

      # æ£€æŸ¥é—®é¢˜æ¸…æ™°åº¦ï¼Œå¦‚æœä¸æ¸…æ™°ä¸”éœ€è¦æ¾„æ¸…ï¼Œç”Ÿæˆæ¾„æ¸…è¯·æ±‚
      if query_analysis.get("clarity", 10) < 5 and query_analysis.get("needs_clarification", False):
          clarification = self.request_clarification(user_query, query_analysis, target_language)
          return {
              "success": True,
              "requires_clarification": True,
              "question": user_query,
              "clarification_request": clarification,
              "analysis": query_analysis
          }

      # æ£€ç´¢ä¸Šä¸‹æ–‡
      context, relevant_ids = self.retrieve_context(
          user_query, query_analysis, vector_index, text_chunks, ai_analysis, conversation_history
      )

      # ç”Ÿæˆå›ç­”
      answer, quality_score, follow_up_questions = self.generate_answer(
          user_query, query_analysis, context, conversation_history, target_language
      )

      if answer is None:
          return {
              "success": False,
              "error": "æ— æ³•ç”Ÿæˆå›ç­”",
              "fallback_answer": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°æˆ–è¯¢é—®å…¶ä»–é—®é¢˜ã€‚"
          }

      # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆå­¦ä¹ è·¯å¾„
      generate_path = False
      if len(conversation_history) >= 3:  # è‡³å°‘æœ‰3è½®å¯¹è¯åè€ƒè™‘ç”Ÿæˆå­¦ä¹ è·¯å¾„
          # æå–æ‰€æœ‰é—®é¢˜
          user_queries = [turn["question"] for turn in conversation_history] + [user_query]
          # æ£€æŸ¥é—®é¢˜å¤šæ ·æ€§
          if len(set([query_analysis["type"] for query in user_queries])) >= 2:
              generate_path = True

      learning_path = None
      if generate_path:
          user_queries = [turn["question"] for turn in conversation_history] + [user_query]
          answers = [turn["answer"] for turn in conversation_history] + [answer]
          learning_path = self.generate_learning_path(user_queries, answers, ai_analysis)

      return {
          "success": True,
          "skip": False,
          "question": user_query,
          "answer": answer,
          "quality_score": quality_score,
          "context_sources": relevant_ids,
          "analysis": query_analysis,
          "follow_up_questions": follow_up_questions,
          "learning_path": learning_path
      }

  def handle_success(self, shared, exec_res):
      """å¤„ç†æˆåŠŸæƒ…å†µ"""
      # å¦‚æœéœ€è¦æ¾„æ¸…ï¼Œä¸æ·»åŠ åˆ°å›ç­”åˆ—è¡¨ï¼Œè€Œæ˜¯è¯·æ±‚ç”¨æˆ·æ¾„æ¸…
      if exec_res.get("requires_clarification", False):
          log_and_notify(
              exec_res["clarification_request"],
              "info",
              notify=True
          )
          shared["clarification_needed"] = True
          shared["last_question"] = exec_res["question"]
          return

      # åˆ›å»ºå›ç­”æ¡ç›®
      answer_entry = {
          "question": exec_res["question"],
          "answer": exec_res["answer"],
          "quality_score": exec_res["quality_score"],
          "timestamp": datetime.now().isoformat(),
          "context_sources": exec_res["context_sources"],
          "analysis": exec_res["analysis"],
          "follow_up_questions": exec_res.get("follow_up_questions", [])
      }

      shared["generated_content"]["custom_answers"].append(answer_entry)
      shared["process_status"]["current_stage"] = "é—®ç­”å¤„ç†å®Œæˆ"

      # è¯·æ±‚ç”¨æˆ·åé¦ˆ
      log_and_notify("å·²ç”Ÿæˆå›ç­”ï¼Œè¯·æä¾›åé¦ˆ", "info", notify=True)

  def handle_failure(self, shared, exec_res):
      """å¤„ç†å¤±è´¥æƒ…å†µ"""
      shared["process_status"]["errors"].append({
          "stage": "äº¤äº’é—®ç­”",
          "error": exec_res["error"],
          "timestamp": datetime.now()
      })

      # ä½¿ç”¨é™çº§å›ç­”
      answer_entry = {
          "question": shared["user_query"],
          "answer": exec_res["fallback_answer"],
          "quality_score": 0.3,
          "timestamp": datetime.now(),
          "context_sources": []
      }

      shared["generated_content"]["custom_answers"].append(answer_entry)
      log_and_notify("ä½¿ç”¨é™çº§å›ç­”", "warning", notify=True)

  def post(self, shared, prep_res, exec_res):
      if exec_res.get("skip", False):
          shared["process_status"]["current_stage"] = "è·³è¿‡é—®ç­”é˜¶æ®µ"
          return

      if exec_res["success"]:
          self.handle_success(shared, exec_res)
      else:
          self.handle_failure(shared, exec_res)
  ```

#### å¯æ‰©å±•æ€§è®¾è®¡

ç³»ç»Ÿè®¾è®¡éµå¾ªä»¥ä¸‹å¯æ‰©å±•æ€§åŸåˆ™ï¼š

1. **æ’ä»¶å¼æ¶æ„**: æ¯ä¸ªèŠ‚ç‚¹éƒ½å¯ä»¥è¢«æ›¿æ¢æˆ–æ‰©å±•ï¼Œåªè¦éµå¾ªç›¸åŒçš„æ¥å£ã€‚

2. **é…ç½®é©±åŠ¨**: å…³é”®å‚æ•°é€šè¿‡é…ç½®æ–‡ä»¶æ§åˆ¶ï¼Œæ— éœ€ä¿®æ”¹ä»£ç å³å¯è°ƒæ•´è¡Œä¸ºã€‚

3. **ä¸­é—´ä»¶æ”¯æŒ**: æ”¯æŒåœ¨èŠ‚ç‚¹é—´æ·»åŠ ä¸­é—´ä»¶ï¼Œç”¨äºæ—¥å¿—è®°å½•ã€æ€§èƒ½ç›‘æ§ã€é”™è¯¯å¤„ç†ç­‰ã€‚

4. **è‡ªå®šä¹‰é’©å­**: åœ¨å…³é”®ç‚¹æä¾›é’©å­ï¼Œå…è®¸ç”¨æˆ·æ³¨å…¥è‡ªå®šä¹‰é€»è¾‘ã€‚

5. **æ¨¡å—åŒ–è®¾è®¡**: åŠŸèƒ½è¢«åˆ†è§£ä¸ºç‹¬ç«‹æ¨¡å—ï¼Œå¯ä»¥å•ç‹¬å‡çº§æˆ–æ›¿æ¢ã€‚

ç¤ºä¾‹é…ç½®æ–‡ä»¶ç»“æ„ï¼š

```yaml
# config.yaml
general:
  target_language: "zh"
  output_format: "markdown"
  cache_enabled: true
  # ç¯å¢ƒå˜é‡é…ç½®
  env:
    # æŒ‡å®š .env æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    env_file: ".env"
    # æ˜¯å¦åœ¨å¯åŠ¨æ—¶éªŒè¯å¿…è¦çš„ç¯å¢ƒå˜é‡
    validate_on_start: true
    # å¿…è¦çš„ç¯å¢ƒå˜é‡åˆ—è¡¨
    required_vars:
      - "LLM_API_KEY"

# æ€§èƒ½ä¼˜åŒ–é…ç½®
performance:
  # å¹¶è¡Œå¤„ç†é…ç½®
  parallel:
    enabled: true
    max_workers: 8  # æœ€å¤§å·¥ä½œçº¿ç¨‹/è¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸º CPU æ ¸å¿ƒæ•° * 2
    chunk_size: 5   # æ¯ä¸ªå·¥ä½œå•å…ƒçš„é¡¹ç›®æ•°
    show_progress: true  # æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡

  # ç¼“å­˜ä¼˜åŒ–é…ç½®
  cache:
    enabled: true
    ttl: 86400  # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
    max_size_gb: 5  # æœ€å¤§ç¼“å­˜å¤§å°ï¼ˆGBï¼‰
    optimization_interval: 3600  # ç¼“å­˜ä¼˜åŒ–é—´éš”ï¼ˆç§’ï¼‰
    priority_strategy: "lru"  # ç¼“å­˜ä¼˜å…ˆçº§ç­–ç•¥: lru, lfu, size, custom

# å¤šè¯­è¨€æ”¯æŒé…ç½®
language:
  # æœ¯è¯­è¡¨é…ç½®
  terminology:
    enabled: true
    custom_terms_file: "./data/custom_terms.json"  # è‡ªå®šä¹‰æœ¯è¯­è¡¨æ–‡ä»¶
    domain_specific: true  # æ˜¯å¦ä½¿ç”¨é¢†åŸŸç‰¹å®šæœ¯è¯­
    preserve_case: true  # æ˜¯å¦ä¿ç•™æœ¯è¯­å¤§å°å†™

  # ç¿»è¯‘é…ç½®
  translation:
    quality_check: true  # æ˜¯å¦è¿›è¡Œç¿»è¯‘è´¨é‡æ£€æŸ¥
    technical_terms_handling: "preserve"  # æŠ€æœ¯æœ¯è¯­å¤„ç†æ–¹å¼: preserve, translate, hybrid

  # Emoji é…ç½®
  emoji:
    enabled: true  # æ˜¯å¦åœ¨æ–‡æ¡£ä¸­æ·»åŠ  emoji
    heading_emojis: true  # æ˜¯å¦ä¸ºæ ‡é¢˜æ·»åŠ  emoji
    custom_emoji_map: "./data/emoji_map.json"  # è‡ªå®šä¹‰ emoji æ˜ å°„æ–‡ä»¶
    content_based: true  # æ˜¯å¦æ ¹æ®å†…å®¹é€‰æ‹© emoji

# å¯è§†åŒ–é…ç½®
visualization:
  # æ¶æ„å›¾é…ç½®
  architecture:
    engine: "mermaid"  # å¯è§†åŒ–å¼•æ“: mermaid, d3, plantuml
    theme: "default"  # ä¸»é¢˜: default, dark, forest, neutral
    direction: "TB"  # æ–¹å‘: TB, BT, LR, RL
    include_legend: true  # æ˜¯å¦åŒ…å«å›¾ä¾‹

  # ä¾èµ–å›¾é…ç½®
  dependency:
    max_depth: 3  # æœ€å¤§æ·±åº¦
    group_by_module: true  # æ˜¯å¦æŒ‰æ¨¡å—åˆ†ç»„
    interactive: true  # æ˜¯å¦ç”Ÿæˆäº¤äº’å¼å›¾è¡¨

  # æ—¶åºå›¾é…ç½®
  sequence:
    show_activation: true  # æ˜¯å¦æ˜¾ç¤ºæ¿€æ´»æ¡
    include_notes: true  # æ˜¯å¦åŒ…å«æ³¨é‡Š

  # å…¶ä»–å›¾è¡¨é…ç½®
  charts:
    enabled: true
    types: ["bar", "line", "pie", "gantt"]  # å¯ç”¨çš„å›¾è¡¨ç±»å‹

# å‘å¸ƒé…ç½®
publishing:
  # GitHub Pages é…ç½®
  github_pages:
    branch: "gh-pages"
    custom_domain: ""
    theme: "jekyll-theme-cayman"

  # GitLab Pages é…ç½®
  gitlab_pages:
    branch: "pages"

  # ReadTheDocs é…ç½®
  readthedocs:
    config_file: ".readthedocs.yml"

  # Netlify é…ç½®
  netlify:
    build_command: "npm run build"
    publish_directory: "_site"

  # Vercel é…ç½®
  vercel:
    build_command: "npm run build"
    output_directory: "out"

  # Gitbook é…ç½®
  gitbook:
    summary_file: "SUMMARY.md"

  # Docsify é…ç½®
  docsify:
    index_file: "README.md"
    sidebar: true

  # VuePress é…ç½®
  vuepress:
    config_file: ".vuepress/config.js"

  # MkDocs é…ç½®
  mkdocs:
    config_file: "mkdocs.yml"

  # JustDoc é…ç½®
  justdoc:
    config_file: "justdoc.json"

nodes:
  PrepareRepoNode:
    max_repo_size: 100000000  # 100MB
    split_threshold: 50000000  # 50MB

  AIUnderstandCoreModulesNode:
    retry_count: 3
    quality_threshold: 0.7
    # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„æ¨¡å‹åç§°ï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼
    model: "${LLM_MODEL:-gpt-4}"
    # å¤šè¯­è¨€æ”¯æŒ
    language_detection: true
    terminology_extraction: true

  InteractiveQANode:
    top_k: 5
    similarity_threshold: 0.65
    # å¹¶è¡Œå¤„ç†ç›¸å…³é—®é¢˜
    batch_processing: true
    max_parallel_questions: 3

plugins:
  - name: "PerformanceMonitor"
    enabled: true
    config:
      log_threshold_ms: 1000

  - name: "CustomContentGenerator"
    enabled: false
    path: "./plugins/custom_generator.py"

  - name: "ParallelProcessor"
    enabled: true
    config:
      max_workers: 8

  - name: "CacheOptimizer"
    enabled: true
    config:
      run_interval: 3600
      max_size_gb: 5

  - name: "VisualizationEnhancer"
    enabled: true
    config:
      engines: ["mermaid", "d3", "plotly"]
      interactive: true
```

```bash
# .env æ–‡ä»¶ç¤ºä¾‹
# LLM é…ç½®
LLM_PROVIDER=openai  # æä¾›å•†é€‰æ‹© (openai, openrouter, alibaba, tongyi, volcengine, moonshot)
LLM_MODEL=gpt-4  # æ¨¡å‹åç§°
LLM_API_KEY=sk-your-api-key-here
LLM_BASE_URL=https://api.openai.com/v1  # å¯é€‰ï¼Œç”¨äºè‡ªå®šä¹‰ API ç«¯ç‚¹
LLM_MAX_TOKENS=4000
LLM_TEMPERATURE=0.7

# OpenRouter é…ç½®
# OPENROUTER_API_KEY=sk-your-openrouter-api-key-here
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# é˜¿é‡Œç™¾ç‚¼ (é€šä¹‰åƒé—®) é…ç½®
# ALIBABA_API_KEY=your-alibaba-api-key-here
# ALIBABA_BASE_URL=https://dashscope.aliyuncs.com/api/v1

# ç«å±±å¼•æ“é…ç½®
# VOLCENGINE_API_KEY=your-volcengine-api-key-here
# VOLCENGINE_BASE_URL=https://api.volcengine.com/ml/api/v1/services
# VOLCENGINE_SERVICE_ID=your-service-id-here

# ç¡…åŸºæµåŠ¨ (Moonshot) é…ç½®
# MOONSHOT_API_KEY=your-moonshot-api-key-here
# MOONSHOT_BASE_URL=https://api.moonshot.cn/v1

# æ€§èƒ½ä¼˜åŒ–é…ç½®
PARALLEL_ENABLED=true
PARALLEL_MAX_WORKERS=8
CACHE_ENABLED=true
CACHE_TTL=86400
CACHE_MAX_SIZE_GB=5
REPO_CACHE_ENABLED=true
REPO_CACHE_TTL=86400

# å¤šè¯­è¨€æ”¯æŒé…ç½®
TARGET_LANGUAGE=zh
TERMINOLOGY_ENABLED=true
TRANSLATION_QUALITY_CHECK=true
TECHNICAL_TERMS_HANDLING=preserve
EMOJI_ENABLED=true
EMOJI_HEADING=true
EMOJI_CONTENT_BASED=true

# å¯è§†åŒ–é…ç½®
VISUALIZATION_ENGINE=mermaid
VISUALIZATION_THEME=default
VISUALIZATION_INTERACTIVE=true

# å‘å¸ƒé…ç½®
PUBLISH_PLATFORM=github
GITHUB_TOKEN=your-github-token-here
GITHUB_REPO=username/repo-name
GITHUB_BRANCH=gh-pages
```

## ğŸ›¡ï¸ å…¨å±€é”™è¯¯å¤„ç†ä¸æ¢å¤æ¡†æ¶ (Global Error Handling and Recovery Framework)

ä¸ºç¡®ä¿ç³»ç»Ÿåœ¨å„ç§å¼‚å¸¸æƒ…å†µä¸‹çš„ç¨³å®šæ€§å’Œå¯é æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„é”™è¯¯å¤„ç†ä¸æ¢å¤æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†é”™è¯¯åˆ†ç±»å¹¶æä¾›ç»Ÿä¸€çš„å¤„ç†æœºåˆ¶ï¼Œå‡å°‘äººå·¥å¹²é¢„éœ€æ±‚ã€‚

### é”™è¯¯åˆ†ç±»ç³»ç»Ÿ

```mermaid
graph TD
    Error[é”™è¯¯ç±»å‹] --> Fatal[è‡´å‘½é”™è¯¯]
    Error --> Recoverable[å¯æ¢å¤é”™è¯¯]
    Error --> Warning[è­¦å‘Š]

    Fatal --> F1[é…ç½®é”™è¯¯]
    Fatal --> F2[æƒé™é”™è¯¯]
    Fatal --> F3[èµ„æºè€—å°½]

    Recoverable --> R1[ç½‘ç»œè¶…æ—¶]
    Recoverable --> R2[APIé™æµ]
    Recoverable --> R3[æ¨¡å‹è¾“å‡ºè´¨é‡ä¸è¶³]
    Recoverable --> R4[è§£æé”™è¯¯]

    Warning --> W1[æ€§èƒ½ä¸‹é™]
    Warning --> W2[éå…³é”®åŠŸèƒ½å¤±è´¥]
    Warning --> W3[éƒ¨åˆ†å†…å®¹ç”Ÿæˆå¤±è´¥]
```

### é”™è¯¯å¤„ç†ç­–ç•¥

| é”™è¯¯ç±»å‹ | ä¸¥é‡ç¨‹åº¦ | å¤„ç†ç­–ç•¥ | æ¢å¤æœºåˆ¶ | é€šçŸ¥çº§åˆ« |
|---------|---------|---------|---------|---------|
| **è‡´å‘½é”™è¯¯** | é«˜ | åœæ­¢æµç¨‹ï¼Œä¿å­˜çŠ¶æ€ | éœ€è¦äººå·¥å¹²é¢„ | ç«‹å³é€šçŸ¥ç”¨æˆ· |
| **å¯æ¢å¤é”™è¯¯** | ä¸­ | é‡è¯•ã€é™çº§å¤„ç† | è‡ªåŠ¨æ¢å¤æˆ–å›é€€åˆ°å¤‡é€‰æ–¹æ¡ˆ | è­¦å‘Šé€šçŸ¥ |
| **è­¦å‘Š** | ä½ | è®°å½•å¹¶ç»§ç»­ | ä¸éœ€è¦æ¢å¤ | æ—¥å¿—è®°å½• |

### å…¨å±€é”™è¯¯å¤„ç†å™¨è®¾è®¡

```python
# å…¨å±€é”™è¯¯å¤„ç†å™¨ç¤ºä¾‹ (utils/error_handler.py)
class GlobalErrorHandler:
    """å…¨å±€é”™è¯¯å¤„ç†å™¨ï¼Œç»Ÿä¸€ç®¡ç†æ‰€æœ‰èŠ‚ç‚¹çš„é”™è¯¯å¤„ç†é€»è¾‘"""

    def __init__(self, config=None):
        self.config = config or {}
        self.error_history = []
        self.recovery_strategies = {
            "network_timeout": self._handle_network_timeout,
            "api_rate_limit": self._handle_rate_limit,
            "model_quality_low": self._handle_low_quality,
            "parsing_error": self._handle_parsing_error,
            # å…¶ä»–é”™è¯¯ç±»å‹çš„å¤„ç†ç­–ç•¥...
        }

    def handle_error(self, error_type, error_info, node_name, shared_state):
        """å¤„ç†é”™è¯¯å¹¶å°è¯•æ¢å¤"""
        # è®°å½•é”™è¯¯
        error_record = {
            "type": error_type,
            "info": error_info,
            "node": node_name,
            "timestamp": datetime.now(),
            "recovered": False
        }
        self.error_history.append(error_record)

        # ç¡®å®šé”™è¯¯ä¸¥é‡ç¨‹åº¦
        severity = self._determine_severity(error_type)

        # æ ¹æ®ä¸¥é‡ç¨‹åº¦é‡‡å–ä¸åŒç­–ç•¥
        if severity == "fatal":
            return self._handle_fatal_error(error_record, shared_state)
        elif severity == "recoverable":
            return self._handle_recoverable_error(error_record, shared_state)
        else:  # warning
            return self._handle_warning(error_record, shared_state)

    def _determine_severity(self, error_type):
        """ç¡®å®šé”™è¯¯çš„ä¸¥é‡ç¨‹åº¦"""
        fatal_errors = ["config_error", "permission_denied", "resource_exhausted"]
        recoverable_errors = ["network_timeout", "api_rate_limit", "model_quality_low", "parsing_error"]

        if error_type in fatal_errors:
            return "fatal"
        elif error_type in recoverable_errors:
            return "recoverable"
        else:
            return "warning"

    def _handle_fatal_error(self, error_record, shared_state):
        """å¤„ç†è‡´å‘½é”™è¯¯"""
        # ä¿å­˜å½“å‰çŠ¶æ€
        self._save_checkpoint(shared_state)

        # é€šçŸ¥ç”¨æˆ·
        log_and_notify(
            f"è‡´å‘½é”™è¯¯: {error_record['type']} åœ¨èŠ‚ç‚¹ {error_record['node']}",
            level="error",
            notify=True
        )

        # æ›´æ–°å…±äº«çŠ¶æ€
        shared_state["process_status"]["errors"].append({
            "stage": error_record["node"],
            "error": error_record["info"],
            "severity": "fatal",
            "timestamp": error_record["timestamp"]
        })

        return {
            "success": False,
            "error": error_record["info"],
            "recoverable": False
        }

    def _handle_recoverable_error(self, error_record, shared_state):
        """å¤„ç†å¯æ¢å¤é”™è¯¯"""
        # æŸ¥æ‰¾æ¢å¤ç­–ç•¥
        recovery_strategy = self.recovery_strategies.get(
            error_record["type"],
            self._default_recovery_strategy
        )

        # å°è¯•æ¢å¤
        recovery_result = recovery_strategy(error_record, shared_state)

        # æ›´æ–°é”™è¯¯è®°å½•
        error_record["recovered"] = recovery_result["success"]

        # æ›´æ–°å…±äº«çŠ¶æ€
        shared_state["process_status"]["errors"].append({
            "stage": error_record["node"],
            "error": error_record["info"],
            "severity": "recoverable",
            "recovery_attempted": True,
            "recovery_success": recovery_result["success"],
            "timestamp": error_record["timestamp"]
        })

        # é€šçŸ¥ç”¨æˆ·
        if recovery_result["success"]:
            log_and_notify(
                f"å·²æ¢å¤é”™è¯¯: {error_record['type']} åœ¨èŠ‚ç‚¹ {error_record['node']}",
                level="warning"
            )
        else:
            log_and_notify(
                f"æ¢å¤å¤±è´¥: {error_record['type']} åœ¨èŠ‚ç‚¹ {error_record['node']}",
                level="error",
                notify=True
            )

        return recovery_result

    def _handle_warning(self, error_record, shared_state):
        """å¤„ç†è­¦å‘Š"""
        # è®°å½•è­¦å‘Š
        shared_state["process_status"]["warnings"].append({
            "stage": error_record["node"],
            "warning": error_record["info"],
            "timestamp": error_record["timestamp"]
        })

        # é€šçŸ¥ç”¨æˆ·
        log_and_notify(
            f"è­¦å‘Š: {error_record['info']} åœ¨èŠ‚ç‚¹ {error_record['node']}",
            level="warning"
        )

        return {
            "success": True,
            "warning": error_record["info"]
        }

    # å…·ä½“æ¢å¤ç­–ç•¥
    def _handle_network_timeout(self, error_record, shared_state):
        """å¤„ç†ç½‘ç»œè¶…æ—¶"""
        # å®ç°æŒ‡æ•°é€€é¿é‡è¯•
        max_retries = self.config.get("network_timeout_max_retries", 3)
        current_retries = error_record.get("retries", 0)

        if current_retries >= max_retries:
            return {"success": False, "error": "è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°"}

        # æ›´æ–°é‡è¯•è®¡æ•°
        error_record["retries"] = current_retries + 1

        # è®¡ç®—é€€é¿æ—¶é—´
        backoff_time = 2 ** current_retries
        time.sleep(backoff_time)

        return {"success": True, "action": "retried"}

    def _handle_rate_limit(self, error_record, shared_state):
        """å¤„ç†APIé™æµ"""
        # å®ç°é™æµå¤„ç†é€»è¾‘
        wait_time = self.config.get("rate_limit_wait_time", 60)
        log_and_notify(f"APIé™æµï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•", "warning")
        time.sleep(wait_time)

        return {"success": True, "action": "waited"}

    def _handle_low_quality(self, error_record, shared_state):
        """å¤„ç†æ¨¡å‹è¾“å‡ºè´¨é‡ä¸è¶³"""
        # å°è¯•ä½¿ç”¨ä¸åŒæ¨¡å‹æˆ–æç¤º
        fallback_model = self.config.get("fallback_model")
        if fallback_model:
            log_and_notify(f"åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹: {fallback_model}", "info")
            shared_state["current_model"] = fallback_model
            return {"success": True, "action": "model_switched"}

        # å¦‚æœæ²¡æœ‰å¤‡ç”¨æ¨¡å‹ï¼Œå°è¯•è°ƒæ•´æç¤º
        return {"success": True, "action": "prompt_adjusted"}

    def _handle_parsing_error(self, error_record, shared_state):
        """å¤„ç†è§£æé”™è¯¯"""
        # å®ç°è§£æé”™è¯¯å¤„ç†é€»è¾‘
        return {"success": True, "action": "simplified_parsing"}

    def _default_recovery_strategy(self, error_record, shared_state):
        """é»˜è®¤æ¢å¤ç­–ç•¥"""
        return {"success": False, "error": "æ²¡æœ‰é€‚ç”¨çš„æ¢å¤ç­–ç•¥"}

    def _save_checkpoint(self, shared_state):
        """ä¿å­˜å½“å‰å¤„ç†çŠ¶æ€çš„æ£€æŸ¥ç‚¹"""
        checkpoint_path = f"checkpoints/checkpoint_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)

        # åˆ›å»ºå¯åºåˆ—åŒ–çš„çŠ¶æ€å‰¯æœ¬
        serializable_state = self._create_serializable_copy(shared_state)

        with open(checkpoint_path, "w") as f:
            json.dump(serializable_state, f, indent=2)

        log_and_notify(f"çŠ¶æ€æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}", "info")

    def _create_serializable_copy(self, obj):
        """åˆ›å»ºå¯¹è±¡çš„å¯åºåˆ—åŒ–å‰¯æœ¬"""
        if isinstance(obj, dict):
            return {k: self._create_serializable_copy(v) for k, v in obj.items()
                   if not k.startswith('_') and k not in ['vector_index', 'embeddings']}
        elif isinstance(obj, list):
            return [self._create_serializable_copy(item) for item in obj]
        elif isinstance(obj, (str, int, float, bool, type(None))):
            return obj
        else:
            return str(obj)
```

### é”™è¯¯æŠ¥å‘Šä¸åˆ†æ

ç³»ç»Ÿå°†ç”Ÿæˆè¯¦ç»†çš„é”™è¯¯æŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š

1. **é”™è¯¯æ‘˜è¦**ï¼šé”™è¯¯ç±»å‹ã€å‘ç”Ÿä½ç½®ã€æ—¶é—´æˆ³
2. **ä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼šé”™è¯¯å‘ç”Ÿæ—¶çš„ç³»ç»ŸçŠ¶æ€å’Œè¾“å…¥æ•°æ®
3. **æ¢å¤å°è¯•**ï¼šå·²å°è¯•çš„æ¢å¤ç­–ç•¥åŠå…¶ç»“æœ
4. **å»ºè®®æ“ä½œ**ï¼šé’ˆå¯¹æ— æ³•è‡ªåŠ¨æ¢å¤çš„é”™è¯¯æä¾›å»ºè®®æ“ä½œ

é”™è¯¯æŠ¥å‘Šå°†ä»¥ç»“æ„åŒ–æ ¼å¼å­˜å‚¨ï¼Œä¾¿äºåç»­åˆ†æå’Œç³»ç»Ÿæ”¹è¿›ã€‚

## ğŸ’» Agentic ç¼–ç æœ€ä½³å®è·µ (Agentic Coding Best Practices)

æœ¬é¡¹ç›®é‡‡ç”¨ Agentic ç¼–ç æ–¹æ³•ï¼Œç”±äººç±»è®¾è®¡ç³»ç»Ÿï¼ŒAI Agent å®ç°ä»£ç ã€‚ä¸ºç¡®ä¿é«˜è´¨é‡çš„åä½œæˆæœï¼Œé¡¹ç›®å®ç°åº”éµå¾ªä»¥ä¸‹æœ€ä½³å®è·µï¼š

### äººæœºåä½œæ¨¡å¼

1. **æ˜ç¡®èŒè´£åˆ†å·¥**
   - äººç±»è´Ÿè´£ï¼šé«˜å±‚è®¾è®¡å†³ç­–ã€éœ€æ±‚æ¾„æ¸…ã€ä»£ç å®¡æŸ¥ã€è´¨é‡æŠŠæ§
   - AI Agent è´Ÿè´£ï¼šä»£ç å®ç°ã€å•å…ƒæµ‹è¯•ç¼–å†™ã€æ–‡æ¡£ç”Ÿæˆã€é‡æ„å»ºè®®
   - åŒæ–¹å…±åŒè´Ÿè´£ï¼šé—®é¢˜æ’æŸ¥ã€æ€§èƒ½ä¼˜åŒ–ã€å®‰å…¨å®¡è®¡

2. **è¿­ä»£å¼å¼€å‘æµç¨‹**
   - ä»å°è€Œç®€å•çš„è§£å†³æ–¹æ¡ˆå¼€å§‹ï¼Œé€æ­¥è¿­ä»£å®Œå–„
   - æ¯æ¬¡è¿­ä»£å‰æ˜ç¡®ç›®æ ‡å’ŒéªŒæ”¶æ ‡å‡†
   - æ¯æ¬¡è¿­ä»£åè¿›è¡Œäººç±»å®¡æŸ¥å’Œåé¦ˆ
   - ä¿æŒé¢‘ç¹æ²Ÿé€šï¼ŒåŠæ—¶è§£å†³ç–‘é—®å’Œé˜»ç¢

3. **è®¾è®¡å…ˆè¡ŒåŸåˆ™**
   - åœ¨å®ç°å‰å…ˆå®Œæˆé«˜å±‚è®¾è®¡æ–‡æ¡£ (`docs/design.md`)
   - è®¾è®¡æ–‡æ¡£åº”åŒ…å«ç³»ç»Ÿæ¶æ„ã€æ•°æ®æµã€æ¥å£å®šä¹‰å’Œå…³é”®ç®—æ³•
   - è®¾è®¡åº”è€ƒè™‘å¯æµ‹è¯•æ€§ã€å¯æ‰©å±•æ€§å’Œé”™è¯¯å¤„ç†
   - äººç±»åº”å®¡æŸ¥å¹¶æ‰¹å‡†è®¾è®¡ï¼Œå†è¿›å…¥å®ç°é˜¶æ®µ

### ä»£ç é£æ ¼ä¸ç»„ç»‡

1. **ä¸€è‡´çš„ä»£ç é£æ ¼**
   - éµå¾ª [PEP 8](https://peps.python.org/pep-0008/) ç¼–ç è§„èŒƒ
   - ä½¿ç”¨ [Black](https://github.com/psf/black) è‡ªåŠ¨æ ¼å¼åŒ–ä»£ç 
   - ä½¿ç”¨ [isort](https://pycqa.github.io/isort/) å¯¹å¯¼å…¥è¿›è¡Œæ’åº
   - ä½¿ç”¨ [flake8](https://flake8.pycqa.org/) è¿›è¡Œä»£ç è´¨é‡æ£€æŸ¥
   - åœ¨é¡¹ç›®æ ¹ç›®å½•æä¾›é…ç½®æ–‡ä»¶ï¼Œç¡®ä¿ AI å’Œäººç±»éµå¾ªç›¸åŒæ ‡å‡†

2. **ä»£ç é•¿åº¦çº¦æŸ**
   - å•æ–‡ä»¶ä»£ç è¡Œæ•°ï¼šä¸è¶…è¿‡ 300 è¡Œï¼ˆä¸å«æ³¨é‡Šå’Œç©ºè¡Œï¼‰
   - å•ä¸ªå‡½æ•°/æ–¹æ³•è¡Œæ•°ï¼šä¸è¶…è¿‡ 30 è¡Œ
   - å•è¡Œé•¿åº¦ï¼šä¸è¶…è¿‡ 88 ä¸ªå­—ç¬¦ï¼ˆä¸ Black é»˜è®¤è®¾ç½®ä¸€è‡´ï¼‰
   - å•ä¸ªç±»è¡Œæ•°ï¼šä¸è¶…è¿‡ 200 è¡Œ
   - å•ä¸ªèŠ‚ç‚¹ç±»ï¼šä¸è¶…è¿‡ 100 è¡Œ
   - åµŒå¥—å±‚çº§ï¼šä¸è¶…è¿‡ 2 å±‚
   - å‚æ•°æ•°é‡ï¼šå‡½æ•°å‚æ•°ä¸è¶…è¿‡ 5 ä¸ª

   ```python
   # åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .flake8 é…ç½®æ–‡ä»¶
   # .flake8
   [flake8]
   max-line-length = 88
   max-complexity = 6  # é™ä½å¤æ‚åº¦ä»¥é™åˆ¶åµŒå¥—
   max-function-length = 30
   ignore = E203, W503
   per-file-ignores =
       __init__.py: F401
   ```

   ```python
   # åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º pyproject.toml é…ç½®æ–‡ä»¶
   # pyproject.toml
   [tool.black]
   line-length = 88
   target-version = ['py38']
   include = '\.pyi?$'

   [tool.isort]
   profile = "black"
   line_length = 88
   ```

3. **æ¨¡å—åŒ–è®¾è®¡**
   - éµå¾ª [PocketFlow](https://github.com/The-Pocket/PocketFlow) çš„èŠ‚ç‚¹å’Œæµç¨‹è®¾è®¡æ¨¡å¼
   - æ¯ä¸ªèŠ‚ç‚¹åº”æ”¾åœ¨å•ç‹¬çš„æ–‡ä»¶ä¸­ï¼Œç›¸å…³èŠ‚ç‚¹ç»„ç»‡åœ¨åŒä¸€ä¸ªåŒ…ä¸­
   - éµå¾ªå•ä¸€èŒè´£åŸåˆ™ï¼Œæ¯ä¸ªèŠ‚ç‚¹åªè´Ÿè´£ä¸€é¡¹ä»»åŠ¡
   - ä½¿ç”¨å…±äº«å†…å­˜ (`shared`) åœ¨èŠ‚ç‚¹é—´ä¼ é€’æ•°æ®ï¼Œé¿å…å…¨å±€å˜é‡

4. **å‘½åçº¦å®š**
   - èŠ‚ç‚¹ç±»åä½¿ç”¨ `CamelCase` å¹¶ä»¥ `Node` æˆ– `Flow` ç»“å°¾
   - å‡½æ•°å’Œå˜é‡åä½¿ç”¨ `snake_case`
   - å¸¸é‡ä½¿ç”¨ `UPPER_CASE_WITH_UNDERSCORES`
   - ç§æœ‰æ–¹æ³•å’Œå±æ€§ä»¥å•ä¸‹åˆ’çº¿å¼€å¤´ `_private_method`
   - ä½¿ç”¨æœ‰æ„ä¹‰çš„ã€æè¿°æ€§çš„åç§°ï¼Œé¿å…ç¼©å†™

5. **æ–‡æ¡£ä¸æ³¨é‡Š**
   - æ‰€æœ‰èŠ‚ç‚¹ç±»å’Œå…¬å…±æ–¹æ³•å¿…é¡»æœ‰ docstringï¼Œéµå¾ª [Google é£æ ¼](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings)
   - å¤æ‚é€»è¾‘éœ€è¦æ·»åŠ æ³¨é‡Šè¯´æ˜
   - æ¯ä¸ªèŠ‚ç‚¹åº”è¯´æ˜å…¶è¾“å…¥ã€è¾“å‡ºå’Œå‰¯ä½œç”¨
   - åœ¨ docstring ä¸­åŒ…å«å‚æ•°ç±»å‹ã€è¿”å›å€¼å’Œå¼‚å¸¸ä¿¡æ¯

### AI ä¸ LLM è°ƒç”¨æœ€ä½³å®è·µ

1. **æç¤ºå·¥ç¨‹ (Prompt Engineering)**
   - è®¾è®¡æ¸…æ™°ã€å…·ä½“çš„æç¤ºæ¨¡æ¿ï¼ŒåŒ…å«ä»»åŠ¡æè¿°å’ŒæœŸæœ›è¾“å‡ºæ ¼å¼
   - æä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä½†é¿å…ä¸å¿…è¦çš„å†—ä½™
   - ä½¿ç”¨å°‘é‡ç¤ºä¾‹ (few-shot learning) å¼•å¯¼ LLM ç”Ÿæˆç¬¦åˆé¢„æœŸçš„è¾“å‡º
   - å®ç°æç¤ºæ¨¡æ¿ç®¡ç†ç³»ç»Ÿï¼Œä¾¿äºé›†ä¸­ç»´æŠ¤å’Œä¼˜åŒ–

2. **LLM è°ƒç”¨ç­–ç•¥**
   - å®ç°é‡è¯•æœºåˆ¶ï¼Œå¤„ç†ä¸´æ—¶æ€§ API é”™è¯¯
   - ä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥ï¼Œé¿å…é¢‘ç¹é‡è¯•å¯¼è‡´çš„é™æµ
   - å®ç°ç»“æœç¼“å­˜ï¼Œé¿å…é‡å¤è°ƒç”¨ç›¸åŒæˆ–ç›¸ä¼¼çš„æç¤º
   - è®¾ç½®è¶…æ—¶å’Œæœ€å¤§ token é™åˆ¶ï¼Œé˜²æ­¢èµ„æºè€—å°½

   ```python
   from pydantic import BaseModel, Field
   from typing import Dict, List, Optional, Any, Union, Tuple
   import os
   import time
   import json
   import hashlib
   from litellm import completion

   class LLMResponse(BaseModel):
       """LLM å“åº”æ¨¡å‹"""
       content: str = Field(..., description="LLM ç”Ÿæˆçš„å†…å®¹")
       model: str = Field(..., description="ä½¿ç”¨çš„æ¨¡å‹åç§°")
       provider: str = Field(..., description="ä½¿ç”¨çš„æä¾›å•†")
       usage: Dict[str, int] = Field(default_factory=dict, description="ä»¤ç‰Œä½¿ç”¨æƒ…å†µ")
       finish_reason: Optional[str] = Field(None, description="ç”Ÿæˆç»“æŸåŸå› ")

   class LLMMetadata(BaseModel):
       """LLM è°ƒç”¨å…ƒæ•°æ®"""
       provider: str = Field(..., description="ä½¿ç”¨çš„æä¾›å•†")
       model: str = Field(..., description="ä½¿ç”¨çš„æ¨¡å‹åç§°")
       timestamp: float = Field(..., description="è°ƒç”¨æ—¶é—´æˆ³")
       attempt: int = Field(1, description="å°è¯•æ¬¡æ•°")
       fallback_used: bool = Field(False, description="æ˜¯å¦ä½¿ç”¨äº†å›é€€æ¨¡å‹")
       from_cache: bool = Field(False, description="æ˜¯å¦æ¥è‡ªç¼“å­˜")
       latency: float = Field(0.0, description="è°ƒç”¨å»¶è¿Ÿï¼ˆç§’ï¼‰")
       error: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯")

   def _try_llm_call(
       messages: List[Dict[str, str]],
       model_info: Dict[str, str],
       fallbacks: List[Dict[str, str]],
       task_type: Optional[str],
       llm_config: Dict[str, Any],
       attempt: int
   ) -> Tuple[Optional[str], bool, Dict[str, Any]]:
       """å°è¯•è°ƒç”¨ LLM å¹¶å¤„ç†ç»“æœ

       Args:
           messages: æ¶ˆæ¯åˆ—è¡¨
           model_info: æ¨¡å‹ä¿¡æ¯
           fallbacks: å›é€€æ¨¡å‹åˆ—è¡¨
           task_type: ä»»åŠ¡ç±»å‹
           llm_config: LLM é…ç½®
           attempt: å½“å‰å°è¯•æ¬¡æ•°

       Returns:
           å…ƒç»„ (å“åº”å†…å®¹, æˆåŠŸæ ‡å¿—, å…ƒæ•°æ®)
       """
       start_time = time.time()
       try:
           # ä½¿ç”¨ LiteLLM ç»Ÿä¸€è°ƒç”¨æ¥å£
           response = completion(
               model=f"{model_info['provider']}/{model_info['model']}",
               messages=messages,
               fallbacks=fallbacks,
               api_key=_get_api_key(model_info["provider"], llm_config),
               max_tokens=llm_config.get("max_tokens", 4000),
               temperature=_get_temperature(task_type)
           )

           # è®¡ç®—å»¶è¿Ÿ
           latency = time.time() - start_time

           # æå–å“åº”å†…å®¹
           content = response.choices[0].message.content

           # æå–ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯
           used_model = response.model
           provider, model = _parse_model_string(used_model)

           # éªŒè¯å“åº”
           if _validate_response(content, task_type):
               # æ„å»ºå…ƒæ•°æ®
               metadata = LLMMetadata(
                   provider=provider,
                   model=model,
                   timestamp=time.time(),
                   attempt=attempt + 1,
                   fallback_used=provider != model_info["provider"] or model != model_info["model"],
                   latency=latency
               )

               return content, True, metadata.dict()

           return None, False, {"error": "å“åº”éªŒè¯å¤±è´¥"}

       except Exception as e:
           # è®°å½•é”™è¯¯
           error_msg = f"LLM è°ƒç”¨å¤±è´¥: {str(e)}"
           log_and_notify(error_msg, level="warning")
           return None, False, {"error": error_msg}

   def call_llm(
       prompt: str,
       context: Optional[str] = None,
       task_type: Optional[str] = None,
       target_language: str = 'en',
       retry_count: int = 3,
       config: Optional[Dict[str, Any]] = None
   ) -> Tuple[Optional[str], bool, Dict[str, Any]]:
       """å¢å¼ºçš„ LLM è°ƒç”¨å‡½æ•°ï¼Œæ”¯æŒæ™ºèƒ½æ¨¡å‹é€‰æ‹©å’Œå›é€€æœºåˆ¶

       Args:
           prompt: ä¸»è¦æç¤ºå†…å®¹
           context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
           task_type: ä»»åŠ¡ç±»å‹ï¼Œç”¨äºæ™ºèƒ½æ¨¡å‹é€‰æ‹©
           target_language: ç›®æ ‡è¯­è¨€
           retry_count: é‡è¯•æ¬¡æ•°
           config: è‡ªå®šä¹‰é…ç½®

       Returns:
           å…ƒç»„ (å“åº”å†…å®¹, æˆåŠŸæ ‡å¿—, å…ƒæ•°æ®)
       """
       # åŠ è½½é…ç½®
       llm_config = config or get_llm_config()

       # æ„å»ºå®Œæ•´æç¤º
       full_prompt = _build_prompt(prompt, context, task_type, target_language)

       # æ£€æŸ¥ç¼“å­˜
       cache_key = _generate_cache_key(full_prompt)
       cached_result = get_from_cache(cache_key)
       if cached_result:
           metadata = LLMMetadata(
               provider=cached_result["metadata"]["provider"],
               model=cached_result["metadata"]["model"],
               timestamp=cached_result["metadata"]["timestamp"],
               from_cache=True,
               latency=0.0
           )
           return cached_result["response"], True, metadata.dict()

       # æ™ºèƒ½æ¨¡å‹é€‰æ‹©
       model_info = _select_model_for_task(task_type, llm_config)

       # å‡†å¤‡ LiteLLM è°ƒç”¨å‚æ•°
       messages = [{"role": "user", "content": full_prompt}]

       # å‡†å¤‡å›é€€é“¾
       fallbacks = _prepare_fallback_models(model_info["provider"], model_info["model"], llm_config)

       # å°è¯•è°ƒç”¨ LLMï¼Œæ”¯æŒé‡è¯•
       for attempt in range(retry_count):
           content, success, metadata = _try_llm_call(
               messages, model_info, fallbacks, task_type, llm_config, attempt
           )

           if success:
               # ç¼“å­˜ç»“æœ
               save_to_cache(cache_key, {
                   "response": content,
                   "metadata": metadata
               })

               return content, True, metadata

           # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
           if attempt < retry_count - 1:
               # æŒ‡æ•°é€€é¿
               wait_time = 2 ** attempt
               time.sleep(wait_time)

       # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
       error_msg = "æ‰€æœ‰ LLM è°ƒç”¨å°è¯•å‡å¤±è´¥"
       log_and_notify(error_msg, level="error")

       metadata = LLMMetadata(
           provider="none",
           model="none",
           timestamp=time.time(),
           attempt=retry_count,
           error=error_msg
       )

       return None, False, metadata.dict()
   ```

3. **è¾“å‡ºè§£æä¸éªŒè¯**
   - å®šä¹‰æ¸…æ™°çš„è¾“å‡ºæ ¼å¼ï¼ˆå¦‚ JSON ç»“æ„ï¼‰
   - å®ç°å¥å£®çš„è§£æé€»è¾‘ï¼Œå¤„ç†æ ¼å¼ä¸ç¬¦åˆé¢„æœŸçš„æƒ…å†µ
   - å¯¹ LLM è¾“å‡ºè¿›è¡ŒéªŒè¯ï¼Œç¡®ä¿ç¬¦åˆä¸šåŠ¡è§„åˆ™å’Œæ•°æ®æ ¼å¼
   - å®ç°é™çº§ç­–ç•¥ï¼Œåœ¨è§£æå¤±è´¥æ—¶æä¾›å¤‡é€‰å¤„ç†æ–¹æ¡ˆ

   ```python
   def _build_prompt(
       prompt: str,
       context: Optional[str] = None,
       task_type: Optional[str] = None,
       target_language: str = 'en'
   ) -> str:
       """æ„å»ºå®Œæ•´çš„æç¤º

       Args:
           prompt: ä¸»è¦æç¤ºå†…å®¹
           context: ä¸Šä¸‹æ–‡ä¿¡æ¯
           task_type: ä»»åŠ¡ç±»å‹
           target_language: ç›®æ ‡è¯­è¨€

       Returns:
           å®Œæ•´çš„æç¤ºå­—ç¬¦ä¸²
       """
       # æ·»åŠ ä»»åŠ¡ç‰¹å®šçš„æŒ‡ä»¤
       task_instructions = {
           "summarize": "è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼Œä¿æŒç®€æ´å’Œä¿¡æ¯é‡ï¼š",
           "explain_code": "è¯·è§£é‡Šä»¥ä¸‹ä»£ç çš„åŠŸèƒ½å’Œå·¥ä½œåŸç†ï¼š",
           "analyze_question": "è¯·åˆ†æä»¥ä¸‹é—®é¢˜ï¼Œæä¾›ç»“æ„åŒ–çš„ç†è§£ï¼š",
           "generate_learning_path": "è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„ï¼š",
           "answer_question": "è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼ŒåŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ï¼š"
       }

       instruction = task_instructions.get(task_type, "")

       # æ·»åŠ è¯­è¨€æŒ‡ä»¤
       language_instruction = ""
       if target_language and target_language.lower() != 'en':
           language_instruction = f"\nè¯·ç”¨{target_language}è¯­è¨€å›ç­”ã€‚"

       # ç»„åˆæç¤º
       if context:
           full_prompt = f"{instruction}\n\nä¸Šä¸‹æ–‡ï¼š\n{context}\n\né—®é¢˜ï¼š\n{prompt}{language_instruction}"
       else:
           full_prompt = f"{instruction}\n\n{prompt}{language_instruction}"

       return full_prompt

   def _select_model_for_task(task_type: Optional[str], config: Dict[str, Any]) -> Dict[str, str]:
       """æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹

       Args:
           task_type: ä»»åŠ¡ç±»å‹
           config: LLM é…ç½®

       Returns:
           åŒ…å« provider å’Œ model çš„å­—å…¸
       """
       # é»˜è®¤æ¨¡å‹
       default_model = {
           "provider": config.get("provider", "openai"),
           "model": config.get("model", "gpt-4")
       }

       # ä»»åŠ¡ç‰¹å®šæ¨¡å‹æ˜ å°„
       task_model_mapping = {
           "summarize": {"provider": "openai", "model": "gpt-3.5-turbo"},
           "explain_code": {"provider": "anthropic", "model": "claude-3-opus"},
           "analyze_question": {"provider": "openai", "model": "gpt-4"},
           "generate_learning_path": {"provider": "anthropic", "model": "claude-3-opus"},
           "answer_question": {"provider": "openai", "model": "gpt-4"}
       }

       # å¦‚æœæœ‰ä»»åŠ¡ç‰¹å®šæ¨¡å‹ï¼Œä½¿ç”¨å®ƒï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
       return task_model_mapping.get(task_type, default_model)

   def _prepare_fallback_models(
       primary_provider: str,
       primary_model: str,
       config: Dict[str, Any]
   ) -> List[Dict[str, str]]:
       """å‡†å¤‡æ¨¡å‹å›é€€é“¾

       Args:
           primary_provider: ä¸»è¦æä¾›å•†
           primary_model: ä¸»è¦æ¨¡å‹
           config: LLM é…ç½®

       Returns:
           å›é€€æ¨¡å‹åˆ—è¡¨
       """
       # æ·»åŠ å¤‡é€‰æ¨¡å‹
       fallbacks = []

       # æ·»åŠ  OpenAI å›é€€
       if primary_provider != "openai" or primary_model != "gpt-4":
           fallbacks.append({
               "model": "openai/gpt-4",
               "api_key": os.getenv("OPENAI_API_KEY")
           })

       # æ·»åŠ  Anthropic å›é€€
       if primary_provider != "anthropic" or primary_model != "claude-3-opus":
           fallbacks.append({
               "model": "anthropic/claude-3-opus",
               "api_key": os.getenv("ANTHROPIC_API_KEY")
           })

       # æ·»åŠ  OpenRouter å›é€€
       if primary_provider != "openrouter":
           fallbacks.append({
               "model": "openrouter/auto",
               "api_key": os.getenv("OPENROUTER_API_KEY")
           })

       # æ·»åŠ é˜¿é‡Œç™¾ç‚¼å›é€€
       if primary_provider != "alibaba":
           fallbacks.append({
               "model": "alibaba/qwen-max",
               "api_key": os.getenv("ALIBABA_API_KEY")
           })

       # æ·»åŠ ç«å±±å¼•æ“å›é€€
       if primary_provider != "volcengine":
           fallbacks.append({
               "model": "volcengine/volcengine-gpt-4",
               "api_key": os.getenv("VOLCENGINE_API_KEY")
           })

       # æ·»åŠ ç¡…åŸºæµåŠ¨å›é€€
       if primary_provider != "moonshot":
           fallbacks.append({
               "model": "moonshot/moonshot-v1-8k",
               "api_key": os.getenv("MOONSHOT_API_KEY")
           })

       return fallbacks

   def _get_api_key(provider: str, config: Dict[str, Any]) -> str:
       """è·å– API å¯†é’¥

       Args:
           provider: æä¾›å•†
           config: LLM é…ç½®

       Returns:
           API å¯†é’¥
       """
       # é¦–å…ˆå°è¯•ä»é…ç½®ä¸­è·å–
       if "api_key" in config:
           return config["api_key"]

       # ç„¶åå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
       env_var_map = {
           "openai": "OPENAI_API_KEY",
           "anthropic": "ANTHROPIC_API_KEY",
           "openrouter": "OPENROUTER_API_KEY",
           "alibaba": "ALIBABA_API_KEY",
           "volcengine": "VOLCENGINE_API_KEY",
           "moonshot": "MOONSHOT_API_KEY"
       }

       env_var = env_var_map.get(provider)
       if env_var:
           return os.getenv(env_var, "")

       return ""

   def _get_temperature(task_type: Optional[str]) -> float:
       """æ ¹æ®ä»»åŠ¡ç±»å‹è·å–æ¸©åº¦å‚æ•°

       Args:
           task_type: ä»»åŠ¡ç±»å‹

       Returns:
           æ¸©åº¦å‚æ•°
       """
       # ä½æ¸©åº¦ä»»åŠ¡ï¼ˆéœ€è¦ç¡®å®šæ€§å’Œå‡†ç¡®æ€§ï¼‰
       low_temp_tasks = ["explain_code", "summarize", "analyze_question"]

       # é«˜æ¸©åº¦ä»»åŠ¡ï¼ˆéœ€è¦åˆ›é€ æ€§ï¼‰
       high_temp_tasks = ["generate_learning_path"]

       if task_type in low_temp_tasks:
           return 0.2
       elif task_type in high_temp_tasks:
           return 0.8
       else:
           return 0.7

   def _parse_model_string(model_string: str) -> Tuple[str, str]:
       """è§£ææ¨¡å‹å­—ç¬¦ä¸²

       Args:
           model_string: æ¨¡å‹å­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º "provider/model"

       Returns:
           (provider, model) å…ƒç»„
       """
       if "/" in model_string:
           parts = model_string.split("/", 1)
           return parts[0], parts[1]
       else:
           # å¦‚æœæ²¡æœ‰æä¾›å•†å‰ç¼€ï¼Œå‡è®¾æ˜¯ OpenAI
           return "openai", model_string

   def _generate_cache_key(prompt: str) -> str:
       """ç”Ÿæˆç¼“å­˜é”®

       Args:
           prompt: æç¤ºå­—ç¬¦ä¸²

       Returns:
           ç¼“å­˜é”®
       """
       # ä½¿ç”¨ MD5 å“ˆå¸Œä½œä¸ºç¼“å­˜é”®
       return hashlib.md5(prompt.encode()).hexdigest()

   def _validate_response(response: str, task_type: Optional[str]) -> bool:
       """éªŒè¯ LLM å“åº”

       Args:
           response: LLM å“åº”å†…å®¹
           task_type: ä»»åŠ¡ç±»å‹

       Returns:
           å“åº”æ˜¯å¦æœ‰æ•ˆ
       """
       if not response:
           return False

       # åŸºæœ¬éªŒè¯
       if len(response.strip()) < 10:
           return False

       # ä»»åŠ¡ç‰¹å®šéªŒè¯
       if task_type == "generate_learning_path":
           # æ£€æŸ¥æ˜¯å¦åŒ…å« JSON ç»“æ„
           return "{" in response and "}" in response

       return True
   ```

4. **æˆæœ¬ä¸æ€§èƒ½ä¼˜åŒ–**
   - ä¼˜å…ˆä½¿ç”¨è¾ƒå°æ¨¡å‹å¤„ç†ç®€å•ä»»åŠ¡ï¼Œä»…åœ¨å¿…è¦æ—¶ä½¿ç”¨é«˜çº§æ¨¡å‹
   - å®ç°æ‰¹å¤„ç†æœºåˆ¶ï¼Œåˆå¹¶å¤šä¸ªç›¸ä¼¼è¯·æ±‚
   - ä½¿ç”¨åµŒå…¥å’Œå‘é‡æ£€ç´¢å‡å°‘ LLM ä¸Šä¸‹æ–‡é•¿åº¦
   - ç›‘æ§å’Œè®°å½• API è°ƒç”¨æˆæœ¬ï¼Œè®¾ç½®é¢„ç®—è­¦æŠ¥

   ```python
   # ç¼“å­˜ç®¡ç†å‡½æ•°
   def get_from_cache(cache_key: str) -> Optional[Dict[str, Any]]:
       """ä»ç¼“å­˜ä¸­è·å–ç»“æœ

       Args:
           cache_key: ç¼“å­˜é”®

       Returns:
           ç¼“å­˜çš„ç»“æœï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
       """
       cache_dir = os.path.join(os.path.dirname(__file__), ".cache", "llm")
       cache_file = os.path.join(cache_dir, f"{cache_key}.json")

       if not os.path.exists(cache_file):
           return None

       try:
           # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
           cache_ttl = int(os.getenv("LLM_CACHE_TTL", "86400"))  # é»˜è®¤ 24 å°æ—¶
           if time.time() - os.path.getmtime(cache_file) > cache_ttl:
               return None

           with open(cache_file, "r", encoding="utf-8") as f:
               return json.load(f)
       except Exception as e:
           log_and_notify(f"è¯»å–ç¼“å­˜å¤±è´¥: {str(e)}", level="warning")
           return None

   def save_to_cache(cache_key: str, data: Dict[str, Any]) -> bool:
       """ä¿å­˜ç»“æœåˆ°ç¼“å­˜

       Args:
           cache_key: ç¼“å­˜é”®
           data: è¦ç¼“å­˜çš„æ•°æ®

       Returns:
           æ˜¯å¦æˆåŠŸä¿å­˜
       """
       # æ£€æŸ¥æ˜¯å¦å¯ç”¨ç¼“å­˜
       if os.getenv("LLM_CACHE_ENABLED", "true").lower() != "true":
           return False

       cache_dir = os.path.join(os.path.dirname(__file__), ".cache", "llm")
       os.makedirs(cache_dir, exist_ok=True)
       cache_file = os.path.join(cache_dir, f"{cache_key}.json")

       try:
           with open(cache_file, "w", encoding="utf-8") as f:
               json.dump(data, f, ensure_ascii=False, indent=2)
           return True
       except Exception as e:
           log_and_notify(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {str(e)}", level="warning")
           return False

   def log_and_notify(message: str, level: str = "info", notify: bool = False) -> None:
       """è®°å½•æ—¥å¿—å¹¶å¯é€‰æ‹©é€šçŸ¥ç”¨æˆ·

       Args:
           message: æ¶ˆæ¯å†…å®¹
           level: æ—¥å¿—çº§åˆ« (info, warning, error)
           notify: æ˜¯å¦é€šçŸ¥ç”¨æˆ·
       """
       import logging

       # é…ç½®æ—¥å¿—
       logging.basicConfig(
           level=logging.INFO,
           format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
       )

       logger = logging.getLogger("codebase-knowledge-builder")

       # æ ¹æ®çº§åˆ«è®°å½•æ—¥å¿—
       if level == "warning":
           logger.warning(message)
       elif level == "error":
           logger.error(message)
       else:
           logger.info(message)

       # å¦‚æœéœ€è¦é€šçŸ¥ç”¨æˆ·ï¼Œå¯ä»¥åœ¨è¿™é‡Œå®ç°
       if notify:
           # è¿™é‡Œå¯ä»¥å®ç°ç”¨æˆ·é€šçŸ¥é€»è¾‘ï¼Œå¦‚å‘é€é‚®ä»¶ã€æ˜¾ç¤ºé€šçŸ¥ç­‰
           print(f"[é€šçŸ¥] {message}")
   ```

### é”™è¯¯å¤„ç†ä¸æ—¥å¿—

1. **å¼‚å¸¸å¤„ç†**
   - åˆ›å»ºè‡ªå®šä¹‰å¼‚å¸¸å±‚æ¬¡ç»“æ„ï¼ŒåŒºåˆ†ä¸åŒç±»å‹çš„é”™è¯¯
   - åœ¨èŠ‚ç‚¹çš„ `exec` æ–¹æ³•ä¸­æ•è·å¹¶å¤„ç†å¼‚å¸¸ï¼Œè¿”å›ç»“æ„åŒ–çš„é”™è¯¯ä¿¡æ¯
   - åœ¨ `post` æ–¹æ³•ä¸­æ›´æ–° `shared["process_status"]["errors"]`
   - å®ç°é™çº§ç­–ç•¥ï¼Œåœ¨å…³é”®åŠŸèƒ½å¤±è´¥æ—¶æä¾›å¤‡é€‰æ–¹æ¡ˆ

2. **æ—¥å¿—è®°å½•**
   - ä½¿ç”¨ `log_and_notify` å·¥å…·å‡½æ•°ç»Ÿä¸€æ—¥å¿—è®°å½•
   - è®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„å¼€å§‹ã€å®Œæˆå’Œå¤±è´¥äº‹ä»¶
   - è®°å½•å…³é”®å†³ç­–ç‚¹å’Œé‡è¦å‚æ•°å€¼
   - å¯¹æ•æ„Ÿä¿¡æ¯è¿›è¡Œè„±æ•å¤„ç†åå†è®°å½•

3. **çŠ¶æ€è·Ÿè¸ª**
   - åœ¨ `shared["process_status"]` ä¸­ç»´æŠ¤å½“å‰å¤„ç†é˜¶æ®µå’Œè¿›åº¦
   - è®°å½•è­¦å‘Šå’Œé”™è¯¯ï¼ŒåŒ…å«æ—¶é—´æˆ³å’Œä¸Šä¸‹æ–‡ä¿¡æ¯
   - å®ç°çŠ¶æ€æŸ¥è¯¢æ¥å£ï¼Œä¾¿äºç›‘æ§å’Œè°ƒè¯•
   - æä¾›å¤„ç†å†å²è®°å½•ï¼Œç”¨äºå®¡è®¡å’Œé—®é¢˜æ’æŸ¥

### æµ‹è¯•ä¸è´¨é‡ä¿è¯

1. **æµ‹è¯•é©±åŠ¨å¼€å‘**
   - å…ˆç¼–å†™æµ‹è¯•ï¼Œå†å®ç°åŠŸèƒ½
   - ä¸ºæ¯ä¸ªèŠ‚ç‚¹ç¼–å†™å•å…ƒæµ‹è¯•ï¼ŒéªŒè¯å…¶è¡Œä¸º
   - ä½¿ç”¨ mock å¯¹è±¡æ¨¡æ‹Ÿå¤–éƒ¨ä¾èµ–å’Œ LLM è°ƒç”¨
   - ç¼–å†™é›†æˆæµ‹è¯•éªŒè¯èŠ‚ç‚¹é—´çš„äº¤äº’

2. **AI ç”Ÿæˆå†…å®¹è´¨é‡è¯„ä¼°**
   - å®ç°è‡ªåŠ¨è¯„ä¼°æœºåˆ¶ï¼Œæ£€æŸ¥ç”Ÿæˆå†…å®¹çš„è´¨é‡
   - å®šä¹‰æ˜ç¡®çš„è´¨é‡æ ‡å‡†å’Œè¯„åˆ†è§„åˆ™
   - æ”¶é›†ç”¨æˆ·åé¦ˆï¼ŒæŒç»­æ”¹è¿›ç”Ÿæˆè´¨é‡
   - å»ºç«‹è´¨é‡åŸºå‡†ï¼Œç›‘æ§è´¨é‡å˜åŒ–è¶‹åŠ¿

3. **æŒç»­é›†æˆ**
   - é…ç½® GitHub Actions è‡ªåŠ¨è¿è¡Œæµ‹è¯•
   - åŒ…å«ä»£ç é£æ ¼æ£€æŸ¥ã€ç±»å‹æ£€æŸ¥å’Œå®‰å…¨æ‰«æ
   - ç”Ÿæˆæµ‹è¯•è¦†ç›–ç‡æŠ¥å‘Š
   - è‡ªåŠ¨åŒ–éƒ¨ç½²æµ‹è¯•ç¯å¢ƒ

### å®‰å…¨ä¸ä¼¦ç†è€ƒè™‘

1. **æ•°æ®å®‰å…¨**
   - æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚ LLM API å¯†é’¥ã€é…ç½®ï¼‰é€šè¿‡ç¯å¢ƒå˜é‡æä¾›ï¼Œä¸åœ¨ä»£ç æˆ–é…ç½®æ–‡ä»¶ä¸­ç¡¬ç¼–ç 
   - å®ç°ç¯å¢ƒå˜é‡åŠ è½½æœºåˆ¶ï¼Œæ”¯æŒ `.env` æ–‡ä»¶å’Œç³»ç»Ÿç¯å¢ƒå˜é‡
   - å®ç°è¾“å…¥éªŒè¯ï¼Œé˜²æ­¢æ³¨å…¥æ”»å‡»
   - é™åˆ¶ LLM è®¿é—®æ•æ„Ÿä¿¡æ¯çš„èƒ½åŠ›
   - å®ç°æ•°æ®è„±æ•æœºåˆ¶ï¼Œä¿æŠ¤ç”¨æˆ·éšç§

   ```python
   # ç¯å¢ƒå˜é‡å¤„ç†ç¤ºä¾‹ (utils/env_manager.py)
   import os
   from dotenv import load_dotenv
   from typing import Dict, Any, Optional

   # åŠ è½½ç¯å¢ƒå˜é‡
   def load_env_vars(env_file: Optional[str] = None) -> None:
       """åŠ è½½ç¯å¢ƒå˜é‡ï¼Œä¼˜å…ˆä»æŒ‡å®šçš„ .env æ–‡ä»¶åŠ è½½ï¼Œç„¶åä»ç³»ç»Ÿç¯å¢ƒå˜é‡åŠ è½½"""
       if env_file and os.path.exists(env_file):
           load_dotenv(env_file)
       else:
           # å°è¯•ä»é»˜è®¤ä½ç½®åŠ è½½
           load_dotenv()

   # è·å– LLM é…ç½®
   def get_llm_config() -> Dict[str, Any]:
       """ä»ç¯å¢ƒå˜é‡è·å– LLM é…ç½®"""
       config = {
           "api_key": os.getenv("LLM_API_KEY"),
           "base_url": os.getenv("LLM_BASE_URL"),
           "model": os.getenv("LLM_MODEL", "gpt-4"),
           "timeout": int(os.getenv("LLM_TIMEOUT", "60")),
           "max_tokens": int(os.getenv("LLM_MAX_TOKENS", "4000")),
           "provider": os.getenv("LLM_PROVIDER", "openai"),
       }

       # éªŒè¯å¿…è¦çš„é…ç½®
       if not config["api_key"]:
           raise ValueError("LLM API å¯†é’¥æœªè®¾ç½®ï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® LLM_API_KEY")

       return config
   ```

2. **AI ä¼¦ç†å‡†åˆ™**
   - ç¡®ä¿ç”Ÿæˆå†…å®¹ä¸åŒ…å«æœ‰å®³ã€æ­§è§†æˆ–ä¸é€‚å½“çš„å†…å®¹
   - å®ç°å†…å®¹è¿‡æ»¤æœºåˆ¶ï¼Œæ£€æµ‹å¹¶ç§»é™¤ä¸å½“å†…å®¹
   - æ˜ç¡®æ ‡è¯† AI ç”Ÿæˆçš„å†…å®¹ï¼Œé¿å…è¯¯å¯¼
   - æä¾›åé¦ˆæœºåˆ¶ï¼Œå…è®¸ç”¨æˆ·æŠ¥å‘Šé—®é¢˜

3. **ç‰ˆæƒä¸è®¸å¯**
   - å°Šé‡åŸå§‹ä»£ç çš„ç‰ˆæƒå’Œè®¸å¯
   - åœ¨ç”Ÿæˆçš„æ–‡æ¡£ä¸­å¼•ç”¨åŸå§‹ä»£ç æ¥æº
   - ç¡®ä¿ç”Ÿæˆå†…å®¹çš„è®¸å¯ä¸åŸå§‹ä»£ç å…¼å®¹
   - æä¾›æ˜ç¡®çš„ä½¿ç”¨æ¡æ¬¾å’Œé™åˆ¶

### ç‰ˆæœ¬æ§åˆ¶ä¸åä½œ

1. **Git å·¥ä½œæµ**
   - é‡‡ç”¨ [GitHub Flow](https://guides.github.com/introduction/flow/)
   - åŠŸèƒ½å¼€å‘åœ¨ç‰¹æ€§åˆ†æ”¯è¿›è¡Œ
   - é€šè¿‡ Pull Request æäº¤ä»£ç ï¼Œè¿›è¡Œäººç±»å®¡æŸ¥
   - æäº¤ä¿¡æ¯éµå¾ª [çº¦å®šå¼æäº¤](https://www.conventionalcommits.org/)

2. **äººæœºåä½œå®¡æŸ¥**
   - äººç±»å®¡æŸ¥ AI ç”Ÿæˆçš„ä»£ç ï¼Œç¡®ä¿è´¨é‡å’Œå®‰å…¨
   - AI è¾…åŠ©äººç±»å®¡æŸ¥ï¼Œæ£€æŸ¥å¸¸è§é—®é¢˜å’Œæœ€ä½³å®è·µ
   - å»ºç«‹å®¡æŸ¥æ¸…å•ï¼Œç¡®ä¿ä¸€è‡´æ€§
   - è®°å½•å®¡æŸ¥æ„è§å’Œä¿®æ”¹å†å²

3. **çŸ¥è¯†å…±äº«ä¸æ–‡æ¡£**
   - ç»´æŠ¤è®¾è®¡å†³ç­–è®°å½• (ADR)ï¼Œè®°å½•é‡è¦å†³ç­–åŠå…¶ç†ç”±
   - ç”Ÿæˆå¹¶ç»´æŠ¤ API æ–‡æ¡£å’Œç”¨æˆ·æŒ‡å—
   - æä¾›ç¤ºä¾‹å’Œæ•™ç¨‹ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£å’Œä½¿ç”¨
   - å»ºç«‹å¸¸è§é—®é¢˜è§£ç­” (FAQ)ï¼Œè§£å†³ç”¨æˆ·ç–‘é—®

## ğŸ”§ æŠ€æœ¯æ ˆçº¦æŸ (Technology Stack Constraints)

ä¸ºç¡®ä¿é¡¹ç›®çš„ä¸€è‡´æ€§ã€å¯ç»´æŠ¤æ€§å’Œé«˜æ•ˆå¼€å‘ï¼Œæœ¬é¡¹ç›®é‡‡ç”¨ä»¥ä¸‹æŠ€æœ¯æ ˆçº¦æŸï¼š

### Pydantic æ•°æ®æ¨¡å‹ç¤ºä¾‹

ä½¿ç”¨ Pydantic è¿›è¡Œæ•°æ®éªŒè¯å’Œç±»å‹æ£€æŸ¥ï¼Œç¡®ä¿é…ç½®ã€API è¯·æ±‚/å“åº”å’ŒèŠ‚ç‚¹è¾“å…¥/è¾“å‡ºçš„æ­£ç¡®æ€§ï¼š

```python
from pydantic import BaseModel, Field, validator, ConfigDict
from typing import List, Dict, Optional, Union, Literal
from enum import Enum
import os
from datetime import datetime

# æšä¸¾ç±»å‹å®šä¹‰
class LLMProvider(str, Enum):
    OPENAI = "openai"
    OPENROUTER = "openrouter"
    ALIBABA = "alibaba"
    VOLCENGINE = "volcengine"
    MOONSHOT = "moonshot"
    ANTHROPIC = "anthropic"

class CacheStrategy(str, Enum):
    LRU = "lru"
    LFU = "lfu"
    SIZE = "size"
    CUSTOM = "custom"

# é…ç½®æ¨¡å‹
class LLMConfig(BaseModel):
    """LLM é…ç½®æ¨¡å‹"""
    model_config = ConfigDict(extra="forbid")  # ç¦æ­¢é¢å¤–å­—æ®µ

    provider: LLMProvider
    model: str
    api_key: str = Field(..., description="API å¯†é’¥")
    base_url: Optional[str] = None
    max_tokens: int = Field(4000, ge=1, le=32000)
    temperature: float = Field(0.7, ge=0, le=2.0)

    @validator("api_key")
    def validate_api_key(cls, v):
        if not v or len(v) < 8:
            raise ValueError("API å¯†é’¥ä¸èƒ½ä¸ºç©ºä¸”é•¿åº¦å¿…é¡»å¤§äº 8")
        return v

    @classmethod
    def from_env(cls):
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        return cls(
            provider=os.getenv("LLM_PROVIDER", "openai"),
            model=os.getenv("LLM_MODEL", "gpt-4"),
            api_key=os.getenv("LLM_API_KEY", ""),
            base_url=os.getenv("LLM_BASE_URL"),
            max_tokens=int(os.getenv("LLM_MAX_TOKENS", "4000")),
            temperature=float(os.getenv("LLM_TEMPERATURE", "0.7"))
        )

class CacheConfig(BaseModel):
    """ç¼“å­˜é…ç½®æ¨¡å‹"""
    enabled: bool = True
    ttl: int = Field(86400, description="ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰")
    max_size_gb: float = Field(5.0, ge=0.1, le=100.0)
    optimization_interval: int = 3600
    priority_strategy: CacheStrategy = CacheStrategy.LRU

class PerformanceConfig(BaseModel):
    """æ€§èƒ½é…ç½®æ¨¡å‹"""
    parallel: Dict[str, Union[bool, int]] = {
        "enabled": True,
        "max_workers": 8,
        "chunk_size": 5,
        "show_progress": True
    }
    cache: CacheConfig = CacheConfig()

class NodeConfig(BaseModel):
    """èŠ‚ç‚¹é…ç½®åŸºç±»"""
    model_config = ConfigDict(extra="allow")  # å…è®¸é¢å¤–å­—æ®µï¼Œä¾¿äºæ‰©å±•

class PrepareRepoNodeConfig(NodeConfig):
    """PrepareRepoNode é…ç½®"""
    max_repo_size: int = 100_000_000  # 100MB
    split_threshold: int = 50_000_000  # 50MB

class AIUnderstandNodeConfig(NodeConfig):
    """AIUnderstandCoreModulesNode é…ç½®"""
    retry_count: int = Field(3, ge=1, le=10)
    quality_threshold: float = Field(0.7, ge=0, le=1.0)
    model: str = "${LLM_MODEL:-gpt-4}"
    language_detection: bool = True
    terminology_extraction: bool = True

# å®Œæ•´é…ç½®æ¨¡å‹
class AppConfig(BaseModel):
    """åº”ç”¨é…ç½®æ¨¡å‹"""
    general: Dict[str, Union[str, bool, Dict]] = {
        "target_language": "zh",
        "output_format": "markdown",
        "cache_enabled": True
    }
    performance: PerformanceConfig = PerformanceConfig()
    nodes: Dict[str, NodeConfig] = {}

    @validator("nodes")
    def validate_nodes(cls, v):
        """éªŒè¯èŠ‚ç‚¹é…ç½®"""
        # ç¡®ä¿å¿…è¦çš„èŠ‚ç‚¹å­˜åœ¨
        required_nodes = ["PrepareRepoNode", "AIUnderstandCoreModulesNode"]
        for node in required_nodes:
            if node not in v:
                v[node] = NodeConfig()
        return v

    def get_node_config(self, node_name: str) -> NodeConfig:
        """è·å–èŠ‚ç‚¹é…ç½®"""
        if node_name not in self.nodes:
            # è¿”å›é»˜è®¤é…ç½®
            if node_name == "PrepareRepoNode":
                return PrepareRepoNodeConfig()
            elif node_name == "AIUnderstandCoreModulesNode":
                return AIUnderstandNodeConfig()
            return NodeConfig()
        return self.nodes[node_name]

# èŠ‚ç‚¹è¾“å…¥/è¾“å‡ºæ¨¡å‹
class NodeInput(BaseModel):
    """èŠ‚ç‚¹è¾“å…¥åŸºç±»"""
    pass

class NodeOutput(BaseModel):
    """èŠ‚ç‚¹è¾“å‡ºåŸºç±»"""
    success: bool
    error: Optional[str] = None

class PrepareRepoInput(NodeInput):
    """PrepareRepoNode è¾“å…¥"""
    repo_url: str
    local_path: Optional[str] = None
    branch: Optional[str] = None
    use_cache: bool = True

class PrepareRepoOutput(NodeOutput):
    """PrepareRepoNode è¾“å‡º"""
    repo_path: Optional[str] = None
    file_count: Optional[int] = None
    total_size: Optional[int] = None
    from_cache: bool = False

# API è¯·æ±‚/å“åº”æ¨¡å‹
class GenerateDocRequest(BaseModel):
    """ç”Ÿæˆæ–‡æ¡£è¯·æ±‚"""
    repo_url: str
    target_language: str = "zh"
    output_format: Literal["markdown", "pdf"] = "markdown"
    include_sections: List[str] = ["overview", "architecture", "modules", "examples"]

    @validator("repo_url")
    def validate_repo_url(cls, v):
        """éªŒè¯ä»“åº“ URL"""
        if not v.startswith(("http://", "https://", "git@")):
            raise ValueError("ä»“åº“ URL å¿…é¡»ä»¥ http://, https:// æˆ– git@ å¼€å¤´")
        return v

class GenerateDocResponse(BaseModel):
    """ç”Ÿæˆæ–‡æ¡£å“åº”"""
    success: bool
    doc_files: List[str] = []
    error: Optional[str] = None
    processing_time: float
    timestamp: datetime = Field(default_factory=datetime.now)
```

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Pydantic å®šä¹‰å„ç§æ•°æ®æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š

1. **é…ç½®æ¨¡å‹**ï¼šç”¨äºéªŒè¯å’Œå¤„ç†åº”ç”¨é…ç½®
2. **èŠ‚ç‚¹è¾“å…¥/è¾“å‡ºæ¨¡å‹**ï¼šç¡®ä¿èŠ‚ç‚¹é—´æ•°æ®ä¼ é€’çš„æ­£ç¡®æ€§
3. **API è¯·æ±‚/å“åº”æ¨¡å‹**ï¼šéªŒè¯ API æ¥å£çš„è¾“å…¥å’Œè¾“å‡º

Pydantic æä¾›äº†å¼ºå¤§çš„æ•°æ®éªŒè¯åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- ç±»å‹æ£€æŸ¥å’Œè½¬æ¢
- å­—æ®µéªŒè¯ï¼ˆèŒƒå›´ã€æ ¼å¼ç­‰ï¼‰
- è‡ªå®šä¹‰éªŒè¯å™¨
- ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®
- æ¨¡å‹ç»§æ‰¿å’Œç»„åˆ

ä½¿ç”¨ Pydantic å¯ä»¥å¤§å¤§å‡å°‘è¿è¡Œæ—¶é”™è¯¯ï¼Œæé«˜ä»£ç çš„å¥å£®æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚

### å¼€å‘ç¯å¢ƒ

1. **Python ç‰ˆæœ¬**
   - ä½¿ç”¨ Python 3.10 æˆ–æ›´é«˜ç‰ˆæœ¬
   - ç¡®ä¿æ‰€æœ‰ä¾èµ–å…¼å®¹æ­¤ç‰ˆæœ¬

2. **è™šæ‹Ÿç¯å¢ƒç®¡ç†**
   - ä½¿ç”¨ [uv](https://github.com/astral-sh/uv) ç®¡ç†è™šæ‹Ÿç¯å¢ƒå’Œä¾èµ–
   - ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ›å»ºå’Œç®¡ç†ç¯å¢ƒï¼š

   ```bash
   # å®‰è£… uv
   curl -LsSf https://astral.sh/uv/install.sh | sh

   # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
   uv venv

   # æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
   source .venv/bin/activate  # Linux/macOS
   .venv\Scripts\activate     # Windows

   # å®‰è£…ä¾èµ–
   uv pip install -e .        # å¼€å‘æ¨¡å¼å®‰è£…é¡¹ç›®
   uv pip install -r requirements.txt  # ä»é”å®šæ–‡ä»¶å®‰è£…

   # ç”Ÿæˆé”å®šç‰ˆæœ¬çš„ requirements.txt
   uv pip compile pyproject.toml -o requirements.txt
   ```

3. **ä¾èµ–ç®¡ç†**
   - ä½¿ç”¨ `pyproject.toml` ä½œä¸ºä¸»è¦ä¾èµ–å£°æ˜æ–‡ä»¶
   - ä½¿ç”¨ `uv pip compile` ç”Ÿæˆé”å®šç‰ˆæœ¬çš„ `requirements.txt`
   - æ˜ç¡®æŒ‡å®šä¾èµ–çš„ç‰ˆæœ¬èŒƒå›´ï¼Œé¿å…è‡ªåŠ¨å‡çº§åˆ°ä¸å…¼å®¹ç‰ˆæœ¬

### æ ¸å¿ƒä¾èµ–

1. **LLM é›†æˆ**
   - ä½¿ç”¨ [LiteLLM](https://github.com/BerriAI/litellm) (^0.12.0) ç»Ÿä¸€è°ƒç”¨ä¸åŒçš„ LLM API
   - æ”¯æŒ OpenAI, Anthropic, Gemini ç­‰ä¸»æµ LLM æä¾›å•†

2. **ä»£ç åˆ†æ**
   - ä½¿ç”¨ [tree-sitter](https://github.com/tree-sitter/py-tree-sitter) (^0.20.1) è¿›è¡Œä»£ç è§£æ
   - ä½¿ç”¨ [GitPython](https://github.com/gitpython-developers/GitPython) (^3.1.40) å¤„ç† Git ä»“åº“å’Œå†å²

3. **å‘é‡æ£€ç´¢**
   - ä½¿ç”¨ [FAISS](https://github.com/facebookresearch/faiss) (^1.7.4) è¿›è¡Œå‘é‡ç´¢å¼•å’Œæ£€ç´¢
   - ä½¿ç”¨ [sentence-transformers](https://github.com/UKPLab/sentence-transformers) (^2.2.2) ç”Ÿæˆæ–‡æœ¬åµŒå…¥

4. **æ•°æ®éªŒè¯ä¸ç±»å‹æ£€æŸ¥**
   - ä½¿ç”¨ [Pydantic](https://docs.pydantic.dev/) (^2.5.0) è¿›è¡Œæ•°æ®éªŒè¯å’Œç±»å‹æ£€æŸ¥
   - æ”¯æŒé…ç½®æ¨¡å‹ã€API è¯·æ±‚/å“åº”æ¨¡å‹å’ŒèŠ‚ç‚¹è¾“å…¥/è¾“å‡ºéªŒè¯

5. **æ–‡æ¡£ç”Ÿæˆ**
   - ä½¿ç”¨ [Markdown](https://python-markdown.github.io/) (^3.5) å¤„ç† Markdown æ–‡æœ¬
   - ä½¿ç”¨ [WeasyPrint](https://weasyprint.org/) (^60.1) å°† Markdown è½¬æ¢ä¸º PDF

6. **Web æ¡†æ¶** (å¯é€‰)
   - ä½¿ç”¨ [FastAPI](https://fastapi.tiangolo.com/) (^0.104.1) æ„å»º API æ¥å£
   - ä½¿ç”¨ [Streamlit](https://streamlit.io/) (^1.28.0) æ„å»ºç®€å•çš„ Web UI

### å¼€å‘å·¥å…·

1. **ä»£ç è´¨é‡**
   - ä½¿ç”¨ [Black](https://github.com/psf/black) (^23.10.0) è‡ªåŠ¨æ ¼å¼åŒ–ä»£ç 
   - ä½¿ç”¨ [isort](https://pycqa.github.io/isort/) (^5.12.0) å¯¹å¯¼å…¥è¿›è¡Œæ’åº
   - ä½¿ç”¨ [flake8](https://flake8.pycqa.org/) (^6.1.0) è¿›è¡Œä»£ç è´¨é‡æ£€æŸ¥
   - ä½¿ç”¨ [Pydantic](https://docs.pydantic.dev/) (^2.5.0) è¿›è¡Œæ•°æ®éªŒè¯å’Œç±»å‹æ£€æŸ¥

2. **æµ‹è¯•å·¥å…·**
   - ä½¿ç”¨ [pytest](https://docs.pytest.org/) (^7.4.3) ç¼–å†™å’Œè¿è¡Œæµ‹è¯•
   - ä½¿ç”¨ [pytest-cov](https://github.com/pytest-dev/pytest-cov) (^4.1.0) ç”Ÿæˆæµ‹è¯•è¦†ç›–ç‡æŠ¥å‘Š
   - ä½¿ç”¨ [VCR.py](https://github.com/kevin1024/vcrpy) (^4.3.1) è®°å½•å’Œå›æ”¾ HTTP äº¤äº’

3. **CI/CD**
   - ä½¿ç”¨ GitHub Actions è¿›è¡ŒæŒç»­é›†æˆ
   - é…ç½®è‡ªåŠ¨æµ‹è¯•ã€ä»£ç è´¨é‡æ£€æŸ¥å’Œå‘å¸ƒæµç¨‹

### é¡¹ç›®ç»“æ„

```
codebase-knowledge-builder/
â”œâ”€â”€ pyproject.toml           # é¡¹ç›®å…ƒæ•°æ®å’Œä¾èµ–å£°æ˜
â”œâ”€â”€ requirements.txt         # é”å®šç‰ˆæœ¬çš„ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ README.md                # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ docs/                    # æ–‡æ¡£ç›®å½•
â”‚   â”œâ”€â”€ design.md            # è®¾è®¡æ–‡æ¡£
â”‚   â””â”€â”€ ...
â”œâ”€â”€ src/                     # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ nodes/               # èŠ‚ç‚¹å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ input_node.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ utils/               # å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_wrapper.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ main.py              # å…¥å£æ–‡ä»¶
â”œâ”€â”€ tests/                   # æµ‹è¯•ç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_nodes/
â”‚   â”œâ”€â”€ test_utils/
â”‚   â””â”€â”€ ...
â””â”€â”€ .github/                 # GitHub é…ç½®
    â””â”€â”€ workflows/           # GitHub Actions å·¥ä½œæµ
        â”œâ”€â”€ tests.yml
        â””â”€â”€ ...
```

## ğŸ“‹ æ€»ç»“ä¸åç»­æ­¥éª¤ (Summary and Next Steps)

### è®¾è®¡æ€»ç»“

æœ¬è®¾è®¡æ–‡æ¡£è¯¦ç»†æè¿°äº†ä»£ç åº“æ•™ç¨‹ç”Ÿæˆ Agent çš„æ ¸å¿ƒç†å¿µã€éœ€æ±‚ã€æµç¨‹è®¾è®¡ã€å·¥å…·å‡½æ•°ã€èŠ‚ç‚¹è®¾è®¡å’Œç¼–ç æœ€ä½³å®è·µã€‚è¯¥ Agent åˆ©ç”¨ AI æŠ€æœ¯æ·±å…¥ç†è§£ä»£ç åº“ï¼Œå¹¶ç”Ÿæˆå¯Œæœ‰æ´å¯ŸåŠ›çš„æ•™ç¨‹å†…å®¹ï¼Œæ”¯æŒå¤šç§ç”¨æˆ·è§’è‰²ã€å¤šè¯­è¨€è¾“å‡ºå’Œå¤šç§å‘å¸ƒæ–¹å¼ã€‚

ç³»ç»Ÿè®¾è®¡çš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š

1. **ä»¥ AI ä¸ºæ ¸å¿ƒå¼•æ“**ï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç†è§£ä»£ç åº“å¹¶ç”Ÿæˆå†…å®¹ï¼Œè€Œéä»…æå–å’Œæ ¼å¼åŒ–ç°æœ‰ä¿¡æ¯ã€‚

2. **å…¨é¢çš„é”™è¯¯å¤„ç†**ï¼šåœ¨å„ä¸ªé˜¶æ®µå®ç°é”™è¯¯æ£€æµ‹ã€æ—¥å¿—è®°å½•å’Œæ¢å¤æœºåˆ¶ï¼Œç¡®ä¿ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚

3. **æ¨¡å—åŒ–ä¸å¯æ‰©å±•æ€§**ï¼šé‡‡ç”¨æ’ä»¶å¼æ¶æ„å’Œé…ç½®é©±åŠ¨è®¾è®¡ï¼Œä¾¿äºæ‰©å±•å’Œå®šåˆ¶ã€‚

4. **å¤šè¯­è¨€æ”¯æŒ**ï¼šèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æŒ‡å®šè¯­è¨€ç”Ÿæˆæ•™ç¨‹å†…å®¹ï¼ŒåŒæ—¶ä¿æŒä»£ç å’ŒæŠ€æœ¯æœ¯è¯­çš„å‡†ç¡®æ€§ã€‚

5. **è´¨é‡ä¿è¯æœºåˆ¶**ï¼šå®ç°å†…å®¹è´¨é‡è¯„ä¼°å’Œç”¨æˆ·åé¦ˆæ”¶é›†ï¼ŒæŒç»­æ”¹è¿›ç”Ÿæˆç»“æœã€‚

6. **å¤šç§è¾“å‡ºæ ¼å¼**ï¼šæ”¯æŒ Markdown å’Œ PDF è¾“å‡ºï¼Œå¹¶èƒ½ä¸€é”®å‘å¸ƒåˆ° GitHub Pagesã€‚

### åç»­æ­¥éª¤

åŸºäºæœ¬è®¾è®¡æ–‡æ¡£ï¼Œå»ºè®®æŒ‰ä»¥ä¸‹æ­¥éª¤è¿›è¡Œå®æ–½ï¼š

1. **åŸºç¡€æ¡†æ¶æ­å»º**ï¼ˆ1-2å‘¨ï¼‰
   - å®ç°æ ¸å¿ƒå·¥å…·å‡½æ•°å’ŒåŸºæœ¬èŠ‚ç‚¹ç»“æ„
   - æ­å»ºé¡¹ç›®éª¨æ¶å’Œé…ç½®ç³»ç»Ÿ
   - å»ºç«‹å¼€å‘ç¯å¢ƒå’Œæµ‹è¯•æ¡†æ¶

2. **æ ¸å¿ƒåŠŸèƒ½å®ç°**ï¼ˆ2-3å‘¨ï¼‰
   - å®ç°ä»£ç è§£æå’Œ AI ç†è§£åŠŸèƒ½
   - å®ç°åŸºæœ¬å†…å®¹ç”ŸæˆåŠŸèƒ½
   - å®ç°å†…å®¹ç»„åˆå’Œæ ¼å¼åŒ–åŠŸèƒ½

3. **å¢å¼ºåŠŸèƒ½å¼€å‘**ï¼ˆ2-3å‘¨ï¼‰
   - å®ç° RAG æ•°æ®å‡†å¤‡å’Œæ£€ç´¢åŠŸèƒ½
   - å®ç°æ—¶é—´çº¿ç”ŸæˆåŠŸèƒ½
   - å®ç°äº¤äº’å¼é—®ç­”åŠŸèƒ½

4. **ä¸°å¯ŒåŠŸèƒ½å®Œå–„**ï¼ˆ2-3å‘¨ï¼‰
   - å®ç°ä¾èµ–å›¾ã€æœ¯è¯­è¡¨ç­‰ç”ŸæˆåŠŸèƒ½
   - å®ç°å¤šè¯­è¨€æ”¯æŒå’Œç¿»è¯‘æ£€æŸ¥
   - å®ç° GitHub Pages å‘å¸ƒåŠŸèƒ½

5. **æµ‹è¯•ä¸ä¼˜åŒ–**ï¼ˆ1-2å‘¨ï¼‰
   - è¿›è¡Œå•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
   - æ€§èƒ½ä¼˜åŒ–å’Œå†…å­˜ç®¡ç†
   - ç”¨æˆ·ä½“éªŒæ”¹è¿›

6. **æ–‡æ¡£ä¸å‘å¸ƒ**ï¼ˆ1å‘¨ï¼‰
   - ç¼–å†™ç”¨æˆ·æ–‡æ¡£å’Œ API æ–‡æ¡£
   - å‡†å¤‡ç¤ºä¾‹å’Œæ•™ç¨‹
   - å‘å¸ƒç¬¬ä¸€ä¸ªç‰ˆæœ¬

### å¢é‡æ›´æ–°ä¸å˜æ›´å¤„ç†

ä¸ºæé«˜ç³»ç»Ÿæ•ˆç‡å¹¶é¿å…æ¯æ¬¡éƒ½é‡æ–°ç”Ÿæˆæ‰€æœ‰å†…å®¹ï¼Œæˆ‘ä»¬è®¾è®¡äº†æ™ºèƒ½å¢é‡æ›´æ–°æœºåˆ¶ï¼š

#### å˜æ›´æ£€æµ‹æœºåˆ¶

```python
def detect_repository_changes(repo_path, previous_state_file=None):
    """æ£€æµ‹ä»£ç åº“å˜æ›´ï¼Œæ”¯æŒç²¾ç¡®åˆ°æ–‡ä»¶çº§åˆ«çš„å˜æ›´è¯†åˆ«

    Args:
        repo_path: ä»£ç åº“è·¯å¾„
        previous_state_file: ä¸Šä¸€æ¬¡å¤„ç†çŠ¶æ€æ–‡ä»¶è·¯å¾„

    Returns:
        å˜æ›´ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«æ–°å¢ã€ä¿®æ”¹ã€åˆ é™¤çš„æ–‡ä»¶åˆ—è¡¨å’Œå…ƒæ•°æ®
    """
    # è·å–å½“å‰ä»£ç åº“çŠ¶æ€
    current_state = {}

    # éå†æ‰€æœ‰æ–‡ä»¶
    for root, _, files in os.walk(repo_path):
        for file in files:
            # è·³è¿‡éšè—æ–‡ä»¶å’Œç‰¹å®šç›®å½•
            if file.startswith('.') or any(p in root for p in ['.git', '__pycache__', 'node_modules']):
                continue

            file_path = os.path.join(root, file)
            rel_path = os.path.relpath(file_path, repo_path)

            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
            file_hash = compute_file_hash(file_path)

            # è®°å½•æ–‡ä»¶ä¿¡æ¯
            current_state[rel_path] = {
                'hash': file_hash,
                'mtime': os.path.getmtime(file_path),
                'size': os.path.getsize(file_path)
            }

    # å¦‚æœæ²¡æœ‰ä¹‹å‰çš„çŠ¶æ€ï¼Œåˆ™æ‰€æœ‰æ–‡ä»¶éƒ½è§†ä¸ºæ–°å¢
    if not previous_state_file or not os.path.exists(previous_state_file):
        return {
            'added': list(current_state.keys()),
            'modified': [],
            'deleted': [],
            'unchanged': [],
            'current_state': current_state
        }

    # åŠ è½½ä¹‹å‰çš„çŠ¶æ€
    with open(previous_state_file, 'r') as f:
        previous_state = json.load(f)

    # æ¯”è¾ƒçŠ¶æ€ï¼Œè¯†åˆ«å˜æ›´
    added = []
    modified = []
    unchanged = []

    for file_path, current_info in current_state.items():
        if file_path not in previous_state:
            added.append(file_path)
        elif current_info['hash'] != previous_state[file_path]['hash']:
            modified.append(file_path)
        else:
            unchanged.append(file_path)

    # è¯†åˆ«åˆ é™¤çš„æ–‡ä»¶
    deleted = [f for f in previous_state if f not in current_state]

    return {
        'added': added,
        'modified': modified,
        'deleted': deleted,
        'unchanged': unchanged,
        'current_state': current_state
    }

def compute_file_hash(file_path):
    """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
    hasher = hashlib.md5()
    with open(file_path, 'rb') as f:
        buf = f.read(65536)
        while len(buf) > 0:
            hasher.update(buf)
            buf = f.read(65536)
    return hasher.hexdigest()
```

#### å¢é‡å¤„ç†ç­–ç•¥

ç³»ç»Ÿé‡‡ç”¨å¤šçº§å¢é‡å¤„ç†ç­–ç•¥ï¼Œæ ¹æ®å˜æ›´èŒƒå›´å’Œå½±å“ç¡®å®šå¤„ç†æ–¹å¼ï¼š

1. **æ–‡ä»¶çº§å¢é‡å¤„ç†**ï¼šä»…å¤„ç†å˜æ›´çš„æ–‡ä»¶ï¼Œä¿ç•™å…¶ä»–æ–‡ä»¶çš„å¤„ç†ç»“æœ
2. **æ¨¡å—çº§å¢é‡å¤„ç†**ï¼šå½“æ–‡ä»¶å˜æ›´å½±å“æ•´ä¸ªæ¨¡å—æ—¶ï¼Œé‡æ–°å¤„ç†æ•´ä¸ªæ¨¡å—
3. **ä¾èµ–æ„ŸçŸ¥å¤„ç†**ï¼šåˆ†æå˜æ›´æ–‡ä»¶çš„ä¾èµ–å…³ç³»ï¼Œå¤„ç†æ‰€æœ‰å—å½±å“çš„ç»„ä»¶
4. **å…¨é‡å›é€€æœºåˆ¶**ï¼šå½“å˜æ›´è¿‡å¤§æˆ–å…³é”®ç»“æ„å‘ç”Ÿå˜åŒ–æ—¶ï¼Œè‡ªåŠ¨å›é€€åˆ°å…¨é‡å¤„ç†

```python
def determine_processing_strategy(changes, dependency_graph):
    """ç¡®å®šå¤„ç†ç­–ç•¥ï¼ŒåŸºäºå˜æ›´èŒƒå›´å’Œä¾èµ–å…³ç³»

    Args:
        changes: å˜æ›´ä¿¡æ¯å­—å…¸
        dependency_graph: ä»£ç åº“ä¾èµ–å…³ç³»å›¾

    Returns:
        å¤„ç†ç­–ç•¥å­—å…¸ï¼ŒåŒ…å«å¤„ç†æ¨¡å¼å’Œéœ€è¦å¤„ç†çš„æ–‡ä»¶/æ¨¡å—åˆ—è¡¨
    """
    # å˜æ›´æ–‡ä»¶æ•°é‡
    total_changed = len(changes['added']) + len(changes['modified']) + len(changes['deleted'])
    total_files = total_changed + len(changes['unchanged'])
    change_ratio = total_changed / total_files if total_files > 0 else 1.0

    # æ£€æŸ¥æ˜¯å¦æœ‰å…³é”®æ–‡ä»¶å˜æ›´
    critical_patterns = ['setup.py', 'requirements.txt', 'package.json', 'config.', 'settings.']
    has_critical_changes = any(
        any(pattern in f for pattern in critical_patterns)
        for f in changes['added'] + changes['modified'] + changes['deleted']
    )

    # ç¡®å®šåŸºæœ¬ç­–ç•¥
    if change_ratio > 0.3 or has_critical_changes:
        # å˜æ›´è¶…è¿‡30%æˆ–æœ‰å…³é”®æ–‡ä»¶å˜æ›´ï¼Œæ‰§è¡Œå…¨é‡å¤„ç†
        return {
            'mode': 'full',
            'reason': 'Large changes or critical files modified',
            'process_all': True
        }

    # æ„å»ºéœ€è¦å¤„ç†çš„æ–‡ä»¶é›†åˆ
    files_to_process = set(changes['added'] + changes['modified'])

    # æ·»åŠ å—å½±å“çš„ä¾èµ–æ–‡ä»¶
    affected_files = set()
    for changed_file in files_to_process:
        # è·å–ä¾èµ–äºæ­¤æ–‡ä»¶çš„å…¶ä»–æ–‡ä»¶
        if changed_file in dependency_graph:
            affected_files.update(dependency_graph[changed_file]['dependents'])

    # åˆå¹¶ç›´æ¥å˜æ›´å’Œå—å½±å“çš„æ–‡ä»¶
    all_affected = files_to_process.union(affected_files)

    # æŒ‰æ¨¡å—åˆ†ç»„
    modules_to_process = {}
    for file in all_affected:
        module = determine_module(file)
        if module not in modules_to_process:
            modules_to_process[module] = []
        modules_to_process[module].append(file)

    # ç¡®å®šæœ€ç»ˆç­–ç•¥
    if len(modules_to_process) > len(all_affected) * 0.7:
        # å¦‚æœå½±å“äº†å¤§éƒ¨åˆ†æ¨¡å—ï¼Œæ‰§è¡Œå…¨é‡å¤„ç†
        return {
            'mode': 'full',
            'reason': 'Changes affect most modules',
            'process_all': True
        }
    else:
        # æ‰§è¡Œå¢é‡å¤„ç†
        return {
            'mode': 'incremental',
            'files': list(all_affected),
            'modules': modules_to_process,
            'process_all': False
        }
```

#### æ–‡æ¡£æ›´æ–°æœºåˆ¶

å¢é‡æ›´æ–°è¿‡ç¨‹ä¸­ï¼Œç³»ç»Ÿä¼šæ™ºèƒ½åˆå¹¶æ–°æ—§æ–‡æ¡£å†…å®¹ï¼Œä¿ç•™ç”¨æˆ·è‡ªå®šä¹‰å†…å®¹ï¼š

1. **ç»“æ„ä¿ç•™**ï¼šä¿æŒæ–‡æ¡£çš„æ•´ä½“ç»“æ„å’Œå¯¼èˆªå…³ç³»
2. **å†…å®¹åˆå¹¶**ï¼šæ™ºèƒ½åˆå¹¶è‡ªåŠ¨ç”Ÿæˆå†…å®¹å’Œç”¨æˆ·æ·»åŠ å†…å®¹
3. **å†²çªè§£å†³**ï¼šå½“è‡ªåŠ¨å†…å®¹å’Œç”¨æˆ·å†…å®¹å†²çªæ—¶ï¼Œæä¾›è§£å†³é€‰é¡¹
4. **ç‰ˆæœ¬è·Ÿè¸ª**ï¼šè®°å½•æ–‡æ¡£çš„æ‰€æœ‰ç‰ˆæœ¬ï¼Œæ”¯æŒå›æ»šåˆ°ä¹‹å‰ç‰ˆæœ¬

```python
def update_documentation(new_content, existing_file, user_sections_marker='<!-- USER CONTENT -->'):
    """æ›´æ–°æ–‡æ¡£ï¼Œä¿ç•™ç”¨æˆ·è‡ªå®šä¹‰å†…å®¹

    Args:
        new_content: æ–°ç”Ÿæˆçš„æ–‡æ¡£å†…å®¹
        existing_file: ç°æœ‰æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        user_sections_marker: ç”¨æˆ·å†…å®¹æ ‡è®°

    Returns:
        åˆå¹¶åçš„æ–‡æ¡£å†…å®¹
    """
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨æ–°å†…å®¹
    if not os.path.exists(existing_file):
        return new_content

    # è¯»å–ç°æœ‰æ–‡æ¡£
    with open(existing_file, 'r', encoding='utf-8') as f:
        existing_content = f.read()

    # æå–ç”¨æˆ·è‡ªå®šä¹‰éƒ¨åˆ†
    user_sections = []
    pattern = f"{user_sections_marker}\\s*?START\\s*?-->([\\s\\S]*?)<!--\\s*?{user_sections_marker}\\s*?END"
    for match in re.finditer(pattern, existing_content):
        user_sections.append({
            'content': match.group(1),
            'start_marker': f"{user_sections_marker} START -->",
            'end_marker': f"<!-- {user_sections_marker} END",
            'full_match': match.group(0)
        })

    # å¦‚æœæ²¡æœ‰ç”¨æˆ·è‡ªå®šä¹‰éƒ¨åˆ†ï¼Œç›´æ¥ä½¿ç”¨æ–°å†…å®¹
    if not user_sections:
        return new_content

    # åˆå¹¶å†…å®¹
    merged_content = new_content
    for section in user_sections:
        # æ£€æŸ¥æ–°å†…å®¹ä¸­æ˜¯å¦æœ‰ç›¸åŒä½ç½®çš„æ ‡è®°
        if section['start_marker'] in merged_content and section['end_marker'] in merged_content:
            # æ›¿æ¢æ–°å†…å®¹ä¸­çš„æ ‡è®°éƒ¨åˆ†
            pattern = f"{section['start_marker']}[\\s\\S]*?{section['end_marker']}"
            merged_content = re.sub(pattern, section['full_match'], merged_content)
        else:
            # å¦‚æœæ–°å†…å®¹ä¸­æ²¡æœ‰æ ‡è®°ï¼Œå°è¯•æ‰¾åˆ°åˆé€‚çš„ä½ç½®æ’å…¥
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„é€»è¾‘ï¼Œå¦‚åŸºäºç« èŠ‚æ ‡é¢˜åŒ¹é…ç­‰
            merged_content += f"\n\n{section['full_match']}\n"

    return merged_content
```

#### ç”¨æˆ·è‡ªå®šä¹‰å†…å®¹ä¿æŠ¤

ç³»ç»Ÿæä¾›æ˜ç¡®çš„æœºåˆ¶ï¼Œå…è®¸ç”¨æˆ·æ ‡è®°ä¸åº”è¢«è‡ªåŠ¨æ›´æ–°çš„å†…å®¹åŒºåŸŸï¼š

```markdown
<!-- USER CONTENT START -->
è¿™é‡Œæ˜¯ç”¨æˆ·æ·»åŠ çš„è‡ªå®šä¹‰å†…å®¹ï¼Œåœ¨æ–‡æ¡£æ›´æ–°æ—¶ä¼šè¢«ä¿ç•™ã€‚
å¯ä»¥åŒ…å«ä»»ä½• Markdown æ ¼å¼çš„å†…å®¹ï¼Œå¦‚ä»£ç ç¤ºä¾‹ã€æ³¨é‡Šç­‰ã€‚
<!-- USER CONTENT END -->
```

è¿™äº›æ ‡è®°åŒºåŸŸåœ¨å¢é‡æ›´æ–°è¿‡ç¨‹ä¸­ä¼šè¢«æ™ºèƒ½è¯†åˆ«å’Œä¿ç•™ï¼Œç¡®ä¿ç”¨æˆ·çš„è‡ªå®šä¹‰å†…å®¹ä¸ä¼šä¸¢å¤±ã€‚

### é£é™©ä¸ç¼“è§£ç­–ç•¥

1. **LLM API é™åˆ¶**
   - é£é™©ï¼šAPI è°ƒç”¨é™åˆ¶ã€æˆæœ¬é«˜æ˜‚
   - ç¼“è§£ï¼šå®ç°ç¼“å­˜æœºåˆ¶ã€æ‰¹å¤„ç†å’Œé™çº§ç­–ç•¥

2. **å¤§å‹ä»£ç åº“å¤„ç†**
   - é£é™©ï¼šå†…å­˜æº¢å‡ºã€å¤„ç†æ—¶é—´è¿‡é•¿
   - ç¼“è§£ï¼šå®ç°åˆ†å—å¤„ç†ã€å¢é‡åˆ†æå’Œå¹¶è¡Œå¤„ç†

3. **ç”Ÿæˆå†…å®¹è´¨é‡**
   - é£é™©ï¼šå†…å®¹ä¸å‡†ç¡®ã€ä¸å®Œæ•´æˆ–ä¸è¿è´¯
   - ç¼“è§£ï¼šå®ç°è´¨é‡è¯„ä¼°ã€å¤šè½®ç»†åŒ–å’Œç”¨æˆ·åé¦ˆæœºåˆ¶

4. **å¤šè¯­è¨€æ”¯æŒæŒ‘æˆ˜**
   - é£é™©ï¼šç¿»è¯‘ä¸å‡†ç¡®ã€æŠ€æœ¯æœ¯è¯­æ··ä¹±
   - ç¼“è§£ï¼šå®ç°æœ¯è¯­è¡¨ã€ç¿»è¯‘æ£€æŸ¥å’Œä¸“ä¸šé¢†åŸŸé€‚é…

é€šè¿‡éµå¾ªæœ¬è®¾è®¡æ–‡æ¡£ä¸­çš„åŸåˆ™å’Œæœ€ä½³å®è·µï¼Œå›¢é˜Ÿå¯ä»¥æ„å»ºä¸€ä¸ªé«˜è´¨é‡ã€å¯é ä¸”æ˜“äºæ‰©å±•çš„ä»£ç åº“æ•™ç¨‹ç”Ÿæˆ Agentï¼Œä¸ºä¸åŒç”¨æˆ·è§’è‰²æä¾›æœ‰ä»·å€¼çš„å­¦ä¹ èµ„æºã€‚
